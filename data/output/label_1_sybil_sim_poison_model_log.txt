
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         1.         0.78595925 0.
 0.         1.         1.         1.         0.14574432 0.56702109
 0.         1.         0.         1.         0.         0.20615755
 0.         0.37803358 0.84401916 0.         0.7116359  0.5391498
 0.2183555  0.40029442]
wv_ed shape (26,)
[0.         1.         0.         1.         0.99672519 0.
 0.         1.         1.         1.         0.591532   0.80294856
 0.         1.         0.         1.         0.         0.02844075
 0.         0.52562051 0.77162671 0.         0.72051761 1.
 0.37929619 0.20612275]
wv_lg shape (26, 1)
[[0.29600018]
 [0.22671743]
 [0.22699693]
 [0.22672144]
 [0.22697287]
 [0.22727059]
 [0.22752179]
 [0.2271522 ]
 [0.22673069]
 [0.22681305]
 [0.22676196]
 [0.2275134 ]
 [0.22687179]
 [0.22748119]
 [0.22694942]
 [0.22701963]
 [0.22696895]
 [0.226674  ]
 [0.22625665]
 [0.22701514]
 [0.22666542]
 [0.22670745]
 [0.22689874]
 [0.22694542]
 [0.2265137 ]
 [0.22695944]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_std shape (26,)
[0.         1.         0.         1.         1.         0.76955919
 1.         0.73195206 0.89473842 0.         0.         1.
 1.         1.         0.396844   1.         0.         1.
 0.         1.         1.         0.3083606  0.26114539 1.
 0.32029559 1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.29600018 1.
  0.         0.         0.        ]
 [1.         0.         1.         1.         0.22671743 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22699693 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.22672144 1.
  1.         1.         1.        ]
 [1.         0.         0.78595925 0.99672519 0.22697287 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22727059 1.
  1.         0.76955919 1.        ]
 [1.         0.         0.         0.         0.22752179 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.2271522  1.
  1.         0.73195206 1.        ]
 [1.         0.         1.         1.         0.22673069 1.
  1.         0.89473842 1.        ]
 [1.         0.         1.         1.         0.22681305 1.
  1.         0.         1.        ]
 [1.         0.         0.14574432 0.591532   0.22676196 1.
  1.         0.         1.        ]
 [1.         0.         0.56702109 0.80294856 0.2275134  1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22687179 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.22748119 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22694942 1.
  1.         0.396844   1.        ]
 [1.         0.         1.         1.         0.22701963 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22696895 1.
  1.         0.         1.        ]
 [1.         0.         0.20615755 0.02844075 0.226674   1.
  1.         1.         1.        ]
 [0.         0.         0.         0.         0.22625665 1.
  1.         0.         1.        ]
 [1.         0.         0.37803358 0.52562051 0.22701514 1.
  1.         1.         1.        ]
 [1.         0.         0.84401916 0.77162671 0.22666542 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.         0.22670745 1.
  1.         0.3083606  1.        ]
 [1.         0.         0.7116359  0.72051761 0.22689874 1.
  1.         0.26114539 1.        ]
 [1.         0.         0.5391498  1.         0.22694542 1.
  1.         1.         1.        ]
 [1.         0.         0.2183555  0.37929619 0.2265137  1.
  1.         0.32029559 1.        ]
 [1.         0.         0.40029442 0.20612275 0.22695944 1.
  1.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 0 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         1.         0.79338965 1.
 1.         1.         1.         0.08429429 1.         1.
 1.         0.54055543 1.         0.19883074 1.         1.
 0.45192359 0.         0.68343253 0.         1.         0.
 0.         1.        ]
wv_ed shape (26,)
[0.         1.         0.         1.         0.62453824 0.84256467
 1.         1.         1.         0.24491693 1.         1.
 1.         0.77257976 1.         0.         1.         1.
 0.29428804 0.         1.         0.         1.         0.
 0.         1.        ]
wv_lg shape (26, 1)
[[0.2978217 ]
 [0.24046634]
 [0.24033908]
 [0.24031197]
 [0.24001545]
 [0.24027904]
 [0.24068763]
 [0.24059199]
 [0.24006057]
 [0.24042846]
 [0.24051441]
 [0.24059955]
 [0.24041048]
 [0.24063932]
 [0.24076922]
 [0.23960602]
 [0.23972458]
 [0.24001701]
 [0.24025832]
 [0.24006956]
 [0.24035168]
 [0.24069603]
 [0.24055121]
 [0.2405323 ]
 [0.24045644]
 [0.23972337]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_std shape (26,)
[0.         0.         0.         1.         0.         0.04140293
 1.         1.         0.45845368 0.         1.         1.
 0.         0.         1.         0.         0.06998899 0.61236512
 0.         0.         0.18623593 0.         0.91128875 0.
 0.         0.26664408]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.2978217  1.
  0.         0.         0.        ]
 [1.         0.         1.         1.         0.24046634 1.
  1.         0.         1.        ]
 [1.         0.         0.         0.         0.24033908 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.24031197 1.
  1.         1.         1.        ]
 [1.         0.         0.79338965 0.62453824 0.24001545 1.
  1.         0.         1.        ]
 [1.         0.         1.         0.84256467 0.24027904 1.
  1.         0.04140293 1.        ]
 [1.         0.         1.         1.         0.24068763 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.24059199 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.24006057 1.
  1.         0.45845368 1.        ]
 [1.         0.         0.08429429 0.24491693 0.24042846 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.24051441 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.24059955 1.
  1.         1.         1.        ]
 [1.         0.         1.         1.         0.24041048 1.
  1.         0.         1.        ]
 [1.         0.         0.54055543 0.77257976 0.24063932 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.24076922 1.
  1.         1.         1.        ]
 [1.         0.         0.19883074 0.         0.23960602 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.23972458 1.
  1.         0.06998899 1.        ]
 [1.         0.         1.         1.         0.24001701 1.
  1.         0.61236512 1.        ]
 [1.         0.         0.45192359 0.29428804 0.24025832 1.
  1.         0.         1.        ]
 [0.         0.         0.         0.         0.24006956 1.
  1.         0.         1.        ]
 [1.         0.         0.68343253 1.         0.24035168 1.
  1.         0.18623593 1.        ]
 [1.         0.         0.         0.         0.24069603 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.24055121 1.
  1.         0.91128875 1.        ]
 [1.         0.         0.         0.         0.2405323  1.
  1.         0.         1.        ]
 [1.         0.         0.         0.         0.24045644 1.
  1.         0.         1.        ]
 [1.         0.         1.         1.         0.23972337 1.
  1.         0.26664408 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 0 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.43358721 1.         0.         0.         0.
 0.79382164 0.40440989 0.0594953  0.         0.         0.
 1.         0.11606844 0.02447466 0.         0.         0.62471989
 1.         0.85403945 0.         0.55133738 1.         0.
 0.48108409 0.20496229]
wv_ed shape (26,)
[0.         0.75074409 1.         0.         0.         0.
 1.         0.44142863 0.19905546 0.         0.         0.
 1.         0.65086285 0.06821008 0.         0.         0.93271758
 1.         0.85709709 0.27500746 0.85955454 1.         0.37165678
 0.79025376 0.39698806]
wv_lg shape (26, 1)
[[0.30170988]
 [0.24936815]
 [0.24943669]
 [0.24915344]
 [0.24941504]
 [0.24970541]
 [0.24954329]
 [0.24960789]
 [0.24972527]
 [0.2496092 ]
 [0.24951633]
 [0.24959271]
 [0.24962269]
 [0.24889531]
 [0.24940703]
 [0.24917193]
 [0.24976076]
 [0.24982587]
 [0.24934863]
 [0.2500949 ]
 [0.24988342]
 [0.24926948]
 [0.24996878]
 [0.24972324]
 [0.24880475]
 [0.24954661]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[0.         0.8090721  0.42684257 1.         1.         0.71867986
 0.82745546 0.41391013 1.         1.         0.62243674 0.74567326
 0.26212553 0.4278703  1.         1.         1.         0.87022653
 0.58315053 1.         1.         0.20444692 1.         0.77104255
 0.27391956 1.        ]
wv_std shape (26,)
[0.         0.         0.89961372 0.         0.22062957 0.
 0.03115566 0.         0.         0.         0.         0.35894008
 1.         0.         0.29954802 0.16457015 0.21437893 0.25559777
 1.         1.         0.         0.         1.         0.
 0.         0.85777952]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.30170988 1.
  0.         0.         0.        ]
 [1.         0.         0.43358721 0.75074409 0.24936815 1.
  0.8090721  0.         1.        ]
 [1.         0.         1.         1.         0.24943669 1.
  0.42684257 0.89961372 1.        ]
 [1.         0.         0.         0.         0.24915344 1.
  1.         0.         1.        ]
 [1.         0.         0.         0.         0.24941504 1.
  1.         0.22062957 1.        ]
 [1.         0.         0.         0.         0.24970541 1.
  0.71867986 0.         1.        ]
 [1.         0.         0.79382164 1.         0.24954329 1.
  0.82745546 0.03115566 1.        ]
 [1.         0.         0.40440989 0.44142863 0.24960789 1.
  0.41391013 0.         1.        ]
 [1.         0.         0.0594953  0.19905546 0.24972527 1.
  1.         0.         1.        ]
 [0.         0.         0.         0.         0.2496092  1.
  1.         0.         1.        ]
 [1.         0.         0.         0.         0.24951633 1.
  0.62243674 0.         1.        ]
 [1.         0.         0.         0.         0.24959271 1.
  0.74567326 0.35894008 1.        ]
 [1.         0.         1.         1.         0.24962269 1.
  0.26212553 1.         1.        ]
 [1.         0.         0.11606844 0.65086285 0.24889531 1.
  0.4278703  0.         1.        ]
 [1.         0.         0.02447466 0.06821008 0.24940703 1.
  1.         0.29954802 1.        ]
 [1.         0.         0.         0.         0.24917193 1.
  1.         0.16457015 1.        ]
 [1.         0.         0.         0.         0.24976076 1.
  1.         0.21437893 1.        ]
 [1.         0.         0.62471989 0.93271758 0.24982587 1.
  0.87022653 0.25559777 1.        ]
 [1.         0.         1.         1.         0.24934863 1.
  0.58315053 1.         1.        ]
 [1.         0.         0.85403945 0.85709709 0.2500949  1.
  1.         1.         1.        ]
 [1.         0.         0.         0.27500746 0.24988342 1.
  1.         0.         1.        ]
 [1.         0.         0.55133738 0.85955454 0.24926948 1.
  0.20444692 0.         1.        ]
 [1.         0.         1.         1.         0.24996878 1.
  1.         1.         1.        ]
 [1.         0.         0.         0.37165678 0.24972324 1.
  0.77104255 0.         1.        ]
 [1.         0.         0.48108409 0.79025376 0.24880475 1.
  0.27391956 0.         1.        ]
 [1.         0.         0.20496229 0.39698806 0.24954661 1.
  1.         0.85777952 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 1 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.44985128 0.         1.         1.
 0.56212041 0.23144858 0.         0.         0.93831022 1.
 0.25441979 0.         0.         0.         1.         0.
 0.52582678 0.         0.         0.         0.         0.17914341
 0.         0.14726961]
wv_ed shape (26,)
[0.         0.         0.7272307  0.         1.         1.
 0.48189354 0.16369974 0.         0.02230035 0.7282965  0.93494738
 0.19868429 0.         0.         0.         1.         0.
 0.64128286 0.         0.         0.         0.         0.15608705
 0.         0.        ]
wv_lg shape (26, 1)
[[0.30413064]
 [0.25812013]
 [0.25814675]
 [0.258407  ]
 [0.25863579]
 [0.2575912 ]
 [0.25850853]
 [0.257892  ]
 [0.25816086]
 [0.25812416]
 [0.25780941]
 [0.2583597 ]
 [0.25881996]
 [0.25851681]
 [0.25805143]
 [0.25806652]
 [0.25822144]
 [0.25756168]
 [0.25828816]
 [0.25821493]
 [0.25724632]
 [0.25790868]
 [0.25797601]
 [0.25792247]
 [0.25776093]
 [0.25791773]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.05608639 0.         0.36933372 1.         1.
 1.         0.54715728 0.         0.         0.45292386 1.
 0.59119391 0.49565644 0.63556231 0.         1.         0.
 0.         0.         0.         0.         0.         0.4695275
 0.         0.13321452]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.30413064 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.25812013 1.
  0.         0.05608639 1.        ]
 [1.         0.         0.44985128 0.7272307  0.25814675 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.258407   1.
  0.         0.36933372 1.        ]
 [1.         0.         1.         1.         0.25863579 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.2575912  1.
  0.         1.         1.        ]
 [1.         0.         0.56212041 0.48189354 0.25850853 1.
  0.         1.         1.        ]
 [1.         0.         0.23144858 0.16369974 0.257892   1.
  0.         0.54715728 1.        ]
 [1.         0.         0.         0.         0.25816086 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.02230035 0.25812416 1.
  0.         0.         1.        ]
 [1.         0.         0.93831022 0.7282965  0.25780941 1.
  0.         0.45292386 1.        ]
 [1.         0.         1.         0.93494738 0.2583597  1.
  0.         1.         1.        ]
 [1.         0.         0.25441979 0.19868429 0.25881996 1.
  0.         0.59119391 1.        ]
 [1.         0.         0.         0.         0.25851681 1.
  0.         0.49565644 1.        ]
 [1.         0.         0.         0.         0.25805143 1.
  0.         0.63556231 1.        ]
 [1.         0.         0.         0.         0.25806652 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.25822144 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.25756168 1.
  0.         0.         1.        ]
 [1.         0.         0.52582678 0.64128286 0.25828816 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.25821493 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.25724632 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.25790868 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.25797601 1.
  0.         0.         1.        ]
 [1.         0.         0.17914341 0.15608705 0.25792247 1.
  0.         0.4695275  1.        ]
 [1.         0.         0.         0.         0.25776093 1.
  0.         0.         1.        ]
 [1.         0.         0.14726961 0.         0.25791773 1.
  0.         0.13321452 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 2 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.22368118 0.62800386 0.         0.
 0.         0.07370923 1.         0.         0.46993924 0.
 0.         0.27159018 1.         1.         0.         0.
 1.         0.73834326 0.         0.97264894 0.         1.
 0.33584494 0.        ]
wv_ed shape (26,)
[0.         0.         0.24230644 0.33581201 0.         0.
 0.         0.03834691 1.         0.         0.49735907 0.
 0.         0.171525   1.         1.         0.         0.
 0.73060125 0.40672804 0.         0.66514618 0.         1.
 0.1205473  0.        ]
wv_lg shape (26, 1)
[[0.30636353]
 [0.26510351]
 [0.26485412]
 [0.2651322 ]
 [0.26578522]
 [0.26496944]
 [0.26530984]
 [0.2654526 ]
 [0.26507515]
 [0.26522529]
 [0.26506848]
 [0.26499461]
 [0.26573584]
 [0.26536132]
 [0.26575004]
 [0.26587682]
 [0.26521837]
 [0.26543356]
 [0.26549704]
 [0.26518233]
 [0.26550773]
 [0.26539951]
 [0.26577178]
 [0.26538463]
 [0.26536864]
 [0.26602395]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.54982058 0.18626233 0.27471302 0.
 0.         0.65864359 0.726404   0.58290415 0.16703667 0.
 1.         0.         1.         1.         0.         0.06265807
 1.         0.09789794 0.         0.14707097 1.         0.9980426
 0.12910928 0.99127178]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.30636353 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.26510351 1.
  0.         0.         1.        ]
 [1.         0.         0.22368118 0.24230644 0.26485412 1.
  0.         0.54982058 1.        ]
 [1.         0.         0.62800386 0.33581201 0.2651322  1.
  0.         0.18626233 1.        ]
 [1.         0.         0.         0.         0.26578522 1.
  0.         0.27471302 1.        ]
 [1.         0.         0.         0.         0.26496944 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.26530984 1.
  0.         0.         1.        ]
 [1.         0.         0.07370923 0.03834691 0.2654526  1.
  0.         0.65864359 1.        ]
 [1.         0.         1.         1.         0.26507515 1.
  0.         0.726404   1.        ]
 [1.         0.         0.         0.         0.26522529 1.
  0.         0.58290415 1.        ]
 [1.         0.         0.46993924 0.49735907 0.26506848 1.
  0.         0.16703667 1.        ]
 [1.         0.         0.         0.         0.26499461 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.26573584 1.
  0.         1.         1.        ]
 [1.         0.         0.27159018 0.171525   0.26536132 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.26575004 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.26587682 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.26521837 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.26543356 1.
  0.         0.06265807 1.        ]
 [1.         0.         1.         0.73060125 0.26549704 1.
  0.         1.         1.        ]
 [1.         0.         0.73834326 0.40672804 0.26518233 1.
  0.         0.09789794 1.        ]
 [1.         0.         0.         0.         0.26550773 1.
  0.         0.         1.        ]
 [1.         0.         0.97264894 0.66514618 0.26539951 1.
  0.         0.14707097 1.        ]
 [1.         0.         0.         0.         0.26577178 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.26538463 1.
  0.         0.9980426  1.        ]
 [1.         0.         0.33584494 0.1205473  0.26536864 1.
  0.         0.12910928 1.        ]
 [1.         0.         0.         0.         0.26602395 1.
  0.         0.99127178 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 3 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         0.         0.         1.
 1.         0.         1.         1.         1.         0.42184039
 0.         0.         0.53982341 1.         0.         0.
 1.         1.         0.28995357 1.         1.         0.
 1.         0.07718092]
wv_ed shape (26,)
[0.         1.         0.         0.         0.         1.
 1.         0.         1.         1.         1.         0.36416359
 0.         0.         0.61573855 1.         0.         0.
 1.         1.         0.11958141 1.         1.         0.
 1.         0.        ]
wv_lg shape (26, 1)
[[0.30879201]
 [0.27195673]
 [0.27099173]
 [0.27122033]
 [0.27104896]
 [0.27142203]
 [0.27174138]
 [0.27143784]
 [0.2716095 ]
 [0.27121172]
 [0.2714277 ]
 [0.2712083 ]
 [0.27138554]
 [0.27133451]
 [0.27160326]
 [0.27110696]
 [0.27107949]
 [0.27109566]
 [0.27113038]
 [0.271362  ]
 [0.27101997]
 [0.27146455]
 [0.27152381]
 [0.27097473]
 [0.27134513]
 [0.27145496]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         0.         0.22119249 1.
 1.         0.00355447 1.         1.         1.         0.39655924
 0.         0.29743481 0.         0.06437795 0.         0.
 0.77489591 1.         0.2353946  0.72113957 1.         0.
 0.99005703 0.0984176 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.30879201 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.27195673 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.27099173 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.27122033 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.27104896 1.
  0.         0.22119249 1.        ]
 [1.         0.         1.         1.         0.27142203 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.27174138 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.27143784 1.
  0.         0.00355447 1.        ]
 [1.         0.         1.         1.         0.2716095  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.27121172 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.2714277  1.
  0.         1.         1.        ]
 [1.         0.         0.42184039 0.36416359 0.2712083  1.
  0.         0.39655924 1.        ]
 [1.         0.         0.         0.         0.27138554 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.27133451 1.
  0.         0.29743481 1.        ]
 [1.         0.         0.53982341 0.61573855 0.27160326 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27110696 1.
  0.         0.06437795 1.        ]
 [1.         0.         0.         0.         0.27107949 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.27109566 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27113038 1.
  0.         0.77489591 1.        ]
 [1.         0.         1.         1.         0.271362   1.
  0.         1.         1.        ]
 [1.         0.         0.28995357 0.11958141 0.27101997 1.
  0.         0.2353946  1.        ]
 [1.         0.         1.         1.         0.27146455 1.
  0.         0.72113957 1.        ]
 [1.         0.         1.         1.         0.27152381 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.27097473 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27134513 1.
  0.         0.99005703 1.        ]
 [1.         0.         0.07718092 0.         0.27145496 1.
  0.         0.0984176  1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 4 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.66053357 1.         1.         1.
 0.46642962 1.         1.         0.16171416 0.         0.96170275
 0.         1.         0.         1.         0.         1.
 0.49112157 0.06552238 1.         0.68181432 1.         0.99958615
 0.         0.        ]
wv_ed shape (26,)
[0.         0.         0.59240372 1.         1.         1.
 0.57997665 1.         1.         0.10062303 0.         1.
 0.         1.         0.         1.         0.         1.
 0.50520185 0.08884664 1.         0.66810791 1.         0.99499099
 0.         0.        ]
wv_lg shape (26, 1)
[[0.31032079]
 [0.27699718]
 [0.27743274]
 [0.27729744]
 [0.27775369]
 [0.27768899]
 [0.2776071 ]
 [0.27733941]
 [0.27766132]
 [0.27685445]
 [0.27675516]
 [0.27674997]
 [0.2770999 ]
 [0.27742588]
 [0.27695012]
 [0.27739844]
 [0.27698324]
 [0.27723259]
 [0.27745594]
 [0.27760117]
 [0.27739123]
 [0.2769938 ]
 [0.27722154]
 [0.27746617]
 [0.27736581]
 [0.27695782]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.50493239 1.         1.         1.
 0.28690274 0.90147866 1.         0.715148   0.         0.70152567
 0.         1.         0.         1.         0.         1.
 1.         0.0801865  1.         0.92773074 1.         1.
 0.20262229 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.31032079 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.27699718 1.
  0.         0.         1.        ]
 [1.         0.         0.66053357 0.59240372 0.27743274 1.
  0.         0.50493239 1.        ]
 [1.         0.         1.         1.         0.27729744 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.27775369 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.27768899 1.
  0.         1.         1.        ]
 [1.         0.         0.46642962 0.57997665 0.2776071  1.
  0.         0.28690274 1.        ]
 [1.         0.         1.         1.         0.27733941 1.
  0.         0.90147866 1.        ]
 [1.         0.         1.         1.         0.27766132 1.
  0.         1.         1.        ]
 [1.         0.         0.16171416 0.10062303 0.27685445 1.
  0.         0.715148   1.        ]
 [0.         0.         0.         0.         0.27675516 1.
  0.         0.         1.        ]
 [1.         0.         0.96170275 1.         0.27674997 1.
  0.         0.70152567 1.        ]
 [1.         0.         0.         0.         0.2770999  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27742588 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.27695012 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27739844 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.27698324 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.27723259 1.
  0.         1.         1.        ]
 [1.         0.         0.49112157 0.50520185 0.27745594 1.
  0.         1.         1.        ]
 [1.         0.         0.06552238 0.08884664 0.27760117 1.
  0.         0.0801865  1.        ]
 [1.         0.         1.         1.         0.27739123 1.
  0.         1.         1.        ]
 [1.         0.         0.68181432 0.66810791 0.2769938  1.
  0.         0.92773074 1.        ]
 [1.         0.         1.         1.         0.27722154 1.
  0.         1.         1.        ]
 [1.         0.         0.99958615 0.99499099 0.27746617 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.27736581 1.
  0.         0.20262229 1.        ]
 [1.         0.         0.         0.         0.27695782 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 5 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.29944751 0.         1.         0.
 0.         0.14087761 0.         1.         0.         0.
 0.         0.00931378 0.         0.         0.22623221 0.
 0.         0.         0.         0.         0.         0.
 0.         0.77317091]
wv_ed shape (26,)
[0.         1.         0.36448292 0.         1.         0.
 0.         0.13794808 0.04749535 1.         0.         0.
 0.         0.         0.         0.         0.24827231 0.
 0.         0.         0.         0.         0.         0.
 0.         0.68113642]
wv_lg shape (26, 1)
[[0.3126013 ]
 [0.28201317]
 [0.28162441]
 [0.28116761]
 [0.28197731]
 [0.28136378]
 [0.28112891]
 [0.28166347]
 [0.28195231]
 [0.2817672 ]
 [0.281842  ]
 [0.28162941]
 [0.28172405]
 [0.28149329]
 [0.28179524]
 [0.28118396]
 [0.28194212]
 [0.28126741]
 [0.28173539]
 [0.28159868]
 [0.2816513 ]
 [0.28168434]
 [0.28154002]
 [0.28156686]
 [0.28171516]
 [0.28170136]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.26332199 0.         0.96505344 0.
 0.         0.25482731 0.1561509  1.         0.2665642  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3126013  1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.28201317 1.
  0.         1.         1.        ]
 [1.         0.         0.29944751 0.36448292 0.28162441 1.
  0.         0.26332199 1.        ]
 [1.         0.         0.         0.         0.28116761 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.28197731 1.
  0.         0.96505344 1.        ]
 [1.         0.         0.         0.         0.28136378 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28112891 1.
  0.         0.         1.        ]
 [1.         0.         0.14087761 0.13794808 0.28166347 1.
  0.         0.25482731 1.        ]
 [1.         0.         0.         0.04749535 0.28195231 1.
  0.         0.1561509  1.        ]
 [1.         0.         1.         1.         0.2817672  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.281842   1.
  0.         0.2665642  1.        ]
 [1.         0.         0.         0.         0.28162941 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28172405 1.
  0.         0.         1.        ]
 [1.         0.         0.00931378 0.         0.28149329 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28179524 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28118396 1.
  0.         0.         1.        ]
 [1.         0.         0.22623221 0.24827231 0.28194212 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28126741 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28173539 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28159868 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.2816513  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28168434 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28154002 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28156686 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28171516 1.
  0.         0.         1.        ]
 [1.         0.         0.77317091 0.68113642 0.28170136 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 6 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         0.         1.         1.
 0.32124962 0.88041949 0.         0.         0.         0.
 0.         0.         0.85473727 0.         0.         0.90981254
 1.         1.         0.57710815 0.         0.         0.
 1.         0.34289787]
wv_ed shape (26,)
[0.         0.         1.         0.         1.         1.
 0.32128305 0.8366867  0.         0.         0.         0.
 0.         0.         0.8126084  0.         0.         0.96993744
 0.82957683 1.         0.58670245 0.         0.         0.
 1.         0.4281404 ]
wv_lg shape (26, 1)
[[0.31460946]
 [0.28539302]
 [0.28575599]
 [0.28544517]
 [0.28605384]
 [0.28600847]
 [0.28557337]
 [0.2858281 ]
 [0.28568917]
 [0.28598782]
 [0.28570718]
 [0.2858112 ]
 [0.28588052]
 [0.2859485 ]
 [0.28567257]
 [0.28566644]
 [0.2854501 ]
 [0.28559878]
 [0.2857665 ]
 [0.28580212]
 [0.2860004 ]
 [0.28555705]
 [0.2855724 ]
 [0.28556583]
 [0.28575703]
 [0.28576213]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.68240026 1.         1.
 0.19916961 0.56825339 0.         0.         0.47796089 0.11609309
 0.12345823 0.7656935  1.         0.43663416 0.         1.
 1.         1.         1.         0.04218953 0.         0.
 1.         0.78162764]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.31460946 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.28539302 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.28575599 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.28544517 1.
  0.         0.68240026 1.        ]
 [1.         0.         1.         1.         0.28605384 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.28600847 1.
  0.         1.         1.        ]
 [1.         0.         0.32124962 0.32128305 0.28557337 1.
  0.         0.19916961 1.        ]
 [1.         0.         0.88041949 0.8366867  0.2858281  1.
  0.         0.56825339 1.        ]
 [1.         0.         0.         0.         0.28568917 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28598782 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.28570718 1.
  0.         0.47796089 1.        ]
 [1.         0.         0.         0.         0.2858112  1.
  0.         0.11609309 1.        ]
 [1.         0.         0.         0.         0.28588052 1.
  0.         0.12345823 1.        ]
 [1.         0.         0.         0.         0.2859485  1.
  0.         0.7656935  1.        ]
 [1.         0.         0.85473727 0.8126084  0.28567257 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.28566644 1.
  0.         0.43663416 1.        ]
 [1.         0.         0.         0.         0.2854501  1.
  0.         0.         1.        ]
 [1.         0.         0.90981254 0.96993744 0.28559878 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.82957683 0.2857665  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.28580212 1.
  0.         1.         1.        ]
 [1.         0.         0.57710815 0.58670245 0.2860004  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.28555705 1.
  0.         0.04218953 1.        ]
 [1.         0.         0.         0.         0.2855724  1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.28556583 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.28575703 1.
  0.         1.         1.        ]
 [1.         0.         0.34289787 0.4281404  0.28576213 1.
  0.         0.78162764 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 7 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         0.47391163 1.         0.18219358
 0.38026801 0.         1.         1.         0.87300492 0.
 1.         0.10409523 1.         0.21792338 1.         0.09693343
 1.         1.         0.48036779 0.         0.63758988 1.
 0.2844716  0.        ]
wv_ed shape (26,)
[0.         0.         1.         0.32617806 1.         0.0459545
 0.34660305 0.         1.         1.         0.72452242 0.
 1.         0.         1.         0.         1.         0.15056732
 1.         1.         0.4447341  0.         0.53143084 0.97524034
 0.14041421 0.        ]
wv_lg shape (26, 1)
[[0.31670856]
 [0.28894555]
 [0.2893586 ]
 [0.28917961]
 [0.28945954]
 [0.28914764]
 [0.2891954 ]
 [0.28910257]
 [0.289272  ]
 [0.28952903]
 [0.28946951]
 [0.28915859]
 [0.28930721]
 [0.28951506]
 [0.28936955]
 [0.28912265]
 [0.28914782]
 [0.28938688]
 [0.28954895]
 [0.28942525]
 [0.28898705]
 [0.28920233]
 [0.28916555]
 [0.28925194]
 [0.28949764]
 [0.28926127]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.73310069 1.         0.99169235
 0.31634493 0.         1.         1.         1.         0.
 1.         0.81634338 1.         0.         1.         0.56941215
 1.         1.         0.01844296 0.31894194 0.33715998 0.86377937
 0.54073457 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.31670856 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.28894555 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.2893586  1.
  0.         1.         1.        ]
 [1.         0.         0.47391163 0.32617806 0.28917961 1.
  0.         0.73310069 1.        ]
 [1.         0.         1.         1.         0.28945954 1.
  0.         1.         1.        ]
 [1.         0.         0.18219358 0.0459545  0.28914764 1.
  0.         0.99169235 1.        ]
 [1.         0.         0.38026801 0.34660305 0.2891954  1.
  0.         0.31634493 1.        ]
 [0.         0.         0.         0.         0.28910257 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.289272   1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.28952903 1.
  0.         1.         1.        ]
 [1.         0.         0.87300492 0.72452242 0.28946951 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.28915859 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.28930721 1.
  0.         1.         1.        ]
 [1.         0.         0.10409523 0.         0.28951506 1.
  0.         0.81634338 1.        ]
 [1.         0.         1.         1.         0.28936955 1.
  0.         1.         1.        ]
 [1.         0.         0.21792338 0.         0.28912265 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.28914782 1.
  0.         1.         1.        ]
 [1.         0.         0.09693343 0.15056732 0.28938688 1.
  0.         0.56941215 1.        ]
 [1.         0.         1.         1.         0.28954895 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.28942525 1.
  0.         1.         1.        ]
 [1.         0.         0.48036779 0.4447341  0.28898705 1.
  0.         0.01844296 1.        ]
 [1.         0.         0.         0.         0.28920233 1.
  0.         0.31894194 1.        ]
 [1.         0.         0.63758988 0.53143084 0.28916555 1.
  0.         0.33715998 1.        ]
 [1.         0.         1.         0.97524034 0.28925194 1.
  0.         0.86377937 1.        ]
 [1.         0.         0.2844716  0.14041421 0.28949764 1.
  0.         0.54073457 1.        ]
 [1.         0.         0.         0.         0.28926127 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 8 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         0.3892648  1.         0.94141911
 0.74612347 0.         1.         0.51364949 0.82015212 0.
 0.         0.         1.         0.52387204 0.         0.64777287
 0.         0.43820353 0.44709519 0.84108535 1.         0.03167462
 0.         0.        ]
wv_ed shape (26,)
[0.         0.         1.         0.38574641 1.         1.
 0.72407721 0.         1.         0.6658122  0.93265812 0.0709964
 0.         0.         1.         0.53771162 0.06658434 0.69452092
 0.         0.48203851 0.48531662 0.99093259 1.         0.16468202
 0.         0.        ]
wv_lg shape (26, 1)
[[0.31790421]
 [0.29301941]
 [0.29358052]
 [0.29328586]
 [0.29338404]
 [0.29293197]
 [0.29344977]
 [0.2933034 ]
 [0.29320249]
 [0.2934334 ]
 [0.29305737]
 [0.29308901]
 [0.29338067]
 [0.29344819]
 [0.2930841 ]
 [0.29326019]
 [0.29296415]
 [0.29317775]
 [0.2930088 ]
 [0.29310246]
 [0.29304925]
 [0.29317214]
 [0.29322172]
 [0.2931025 ]
 [0.29327039]
 [0.29324249]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.23448892 1.         0.
 1.         0.         0.         0.70607649 0.37278531 0.
 0.         0.         0.50193603 0.         0.         0.
 0.         0.38632918 0.         0.         0.44571354 0.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.31790421 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.29301941 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29358052 1.
  0.         1.         1.        ]
 [1.         0.         0.3892648  0.38574641 0.29328586 1.
  0.         0.23448892 1.        ]
 [1.         0.         1.         1.         0.29338404 1.
  0.         1.         1.        ]
 [1.         0.         0.94141911 1.         0.29293197 1.
  0.         0.         1.        ]
 [1.         0.         0.74612347 0.72407721 0.29344977 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.2933034  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29320249 1.
  0.         0.         1.        ]
 [1.         0.         0.51364949 0.6658122  0.2934334  1.
  0.         0.70607649 1.        ]
 [1.         0.         0.82015212 0.93265812 0.29305737 1.
  0.         0.37278531 1.        ]
 [1.         0.         0.         0.0709964  0.29308901 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29338067 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29344819 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.2930841  1.
  0.         0.50193603 1.        ]
 [1.         0.         0.52387204 0.53771162 0.29326019 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.06658434 0.29296415 1.
  0.         0.         1.        ]
 [1.         0.         0.64777287 0.69452092 0.29317775 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.2930088  1.
  0.         0.         1.        ]
 [1.         0.         0.43820353 0.48203851 0.29310246 1.
  0.         0.38632918 1.        ]
 [1.         0.         0.44709519 0.48531662 0.29304925 1.
  0.         0.         1.        ]
 [1.         0.         0.84108535 0.99093259 0.29317214 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29322172 1.
  0.         0.44571354 1.        ]
 [1.         0.         0.03167462 0.16468202 0.2931025  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29327039 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29324249 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 9 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.46947694 1.         0.         0.463458   0.87359011
 0.         1.         0.         0.         1.         0.93371924
 0.         1.         1.         1.         1.         0.
 0.64220009 0.         0.99599102 1.         0.68449468 0.91847381
 1.         0.07886481]
wv_ed shape (26,)
[0.         0.58905639 1.         0.         0.7125158  1.
 0.         1.         0.         0.         1.         0.96767385
 0.10283719 1.         1.         1.         1.         0.
 0.76653426 0.         1.         1.         0.61438438 0.97674574
 1.         0.        ]
wv_lg shape (26, 1)
[[0.3196113 ]
 [0.29644821]
 [0.29634753]
 [0.29607635]
 [0.29626556]
 [0.29636563]
 [0.29623247]
 [0.29641274]
 [0.29616863]
 [0.29633932]
 [0.29666088]
 [0.29639434]
 [0.29629593]
 [0.29642396]
 [0.2963337 ]
 [0.29656974]
 [0.29652628]
 [0.29627562]
 [0.29660303]
 [0.29644268]
 [0.29620584]
 [0.29644358]
 [0.29621733]
 [0.29617636]
 [0.29660275]
 [0.29598217]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         1.         0.         0.         0.50149435
 0.         1.         0.         0.         1.         1.
 0.         0.76916381 1.         1.         1.         0.
 1.         0.         0.48010662 1.         0.82809147 0.59929364
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3196113  1.
  1.         0.         0.        ]
 [1.         0.         0.46947694 0.58905639 0.29644821 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.29634753 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.29607635 1.
  0.         0.         1.        ]
 [1.         0.         0.463458   0.7125158  0.29626556 1.
  0.         0.         1.        ]
 [1.         0.         0.87359011 1.         0.29636563 1.
  0.         0.50149435 1.        ]
 [1.         0.         0.         0.         0.29623247 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29641274 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29616863 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29633932 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29666088 1.
  0.         1.         1.        ]
 [1.         0.         0.93371924 0.96767385 0.29639434 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.10283719 0.29629593 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29642396 1.
  0.         0.76916381 1.        ]
 [1.         0.         1.         1.         0.2963337  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.29656974 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.29652628 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29627562 1.
  0.         0.         1.        ]
 [1.         0.         0.64220009 0.76653426 0.29660303 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29644268 1.
  0.         0.         1.        ]
 [1.         0.         0.99599102 1.         0.29620584 1.
  0.         0.48010662 1.        ]
 [1.         0.         1.         1.         0.29644358 1.
  0.         1.         1.        ]
 [1.         0.         0.68449468 0.61438438 0.29621733 1.
  0.         0.82809147 1.        ]
 [1.         0.         0.91847381 0.97674574 0.29617636 1.
  0.         0.59929364 1.        ]
 [1.         0.         1.         1.         0.29660275 1.
  0.         1.         1.        ]
 [1.         0.         0.07886481 0.         0.29598217 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 10 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         1.         0.         0.92792814
 0.46182867 0.67628788 0.         0.         0.         0.08001054
 0.         0.         0.12584773 1.         0.87524346 0.77630249
 0.1519267  0.         1.         0.         0.81513546 0.
 0.52171171 0.        ]
wv_ed shape (26,)
[0.         0.         1.         1.         0.         0.98772765
 0.51655277 0.74939714 0.         0.         0.         0.22290307
 0.         0.         0.19570909 1.         0.89905501 0.72936659
 0.30371634 0.         1.         0.         0.89151787 0.
 0.60904094 0.        ]
wv_lg shape (26, 1)
[[0.32124777]
 [0.29905637]
 [0.29936992]
 [0.29911141]
 [0.29885284]
 [0.29927511]
 [0.29930696]
 [0.29906753]
 [0.29895607]
 [0.29911436]
 [0.29898538]
 [0.29917446]
 [0.29924207]
 [0.29905988]
 [0.29915087]
 [0.29935234]
 [0.29938294]
 [0.29915716]
 [0.29892762]
 [0.29927446]
 [0.29921282]
 [0.29921625]
 [0.29893268]
 [0.2992799 ]
 [0.29900359]
 [0.29894403]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.10610578 1.         1.         0.22569677 1.
 1.         1.         0.         0.         0.71908838 0.5268779
 0.88216641 0.         0.         1.         1.         1.
 0.2850001  0.15059743 1.         0.76237583 1.         0.94933576
 0.         0.31413165]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32124777 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.29905637 1.
  0.         0.10610578 1.        ]
 [1.         0.         1.         1.         0.29936992 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.29911141 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29885284 1.
  0.         0.22569677 1.        ]
 [1.         0.         0.92792814 0.98772765 0.29927511 1.
  0.         1.         1.        ]
 [1.         0.         0.46182867 0.51655277 0.29930696 1.
  0.         1.         1.        ]
 [1.         0.         0.67628788 0.74939714 0.29906753 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29895607 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.29911436 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29898538 1.
  0.         0.71908838 1.        ]
 [1.         0.         0.08001054 0.22290307 0.29917446 1.
  0.         0.5268779  1.        ]
 [1.         0.         0.         0.         0.29924207 1.
  0.         0.88216641 1.        ]
 [1.         0.         0.         0.         0.29905988 1.
  0.         0.         1.        ]
 [1.         0.         0.12584773 0.19570909 0.29915087 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.29935234 1.
  0.         1.         1.        ]
 [1.         0.         0.87524346 0.89905501 0.29938294 1.
  0.         1.         1.        ]
 [1.         0.         0.77630249 0.72936659 0.29915716 1.
  0.         1.         1.        ]
 [1.         0.         0.1519267  0.30371634 0.29892762 1.
  0.         0.2850001  1.        ]
 [1.         0.         0.         0.         0.29927446 1.
  0.         0.15059743 1.        ]
 [1.         0.         1.         1.         0.29921282 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.29921625 1.
  0.         0.76237583 1.        ]
 [1.         0.         0.81513546 0.89151787 0.29893268 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.2992799  1.
  0.         0.94933576 1.        ]
 [1.         0.         0.52171171 0.60904094 0.29900359 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.29894403 1.
  0.         0.31413165 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 11 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.32971442 0.         1.         0.28016861 0.17308994
 0.37912823 1.         0.86626103 0.23202746 1.         0.52715939
 1.         0.87535172 1.         1.         0.         0.37773443
 1.         0.08761833 1.         1.         1.         0.
 0.78640458 0.61321236]
wv_ed shape (26,)
[0.         0.45356815 0.         1.         0.34515984 0.23785826
 0.330746   1.         0.95186674 0.18637477 0.98120585 0.52231323
 1.         0.81147461 1.         1.         0.         0.39383823
 1.         0.03960253 1.         1.         1.         0.
 0.79937044 0.64311436]
wv_lg shape (26, 1)
[[0.32239785]
 [0.30232909]
 [0.30204162]
 [0.3025174 ]
 [0.3019691 ]
 [0.30190646]
 [0.30225088]
 [0.30238997]
 [0.30201306]
 [0.30210753]
 [0.30192519]
 [0.30203169]
 [0.30232341]
 [0.30231063]
 [0.30204258]
 [0.30215701]
 [0.30207264]
 [0.30213733]
 [0.3020493 ]
 [0.30188325]
 [0.30212121]
 [0.30238897]
 [0.30226536]
 [0.30216024]
 [0.30224691]
 [0.30210645]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         1.         0.         0.36437786
 0.38596599 1.         0.         0.0738146  0.89437551 1.
 1.         0.79600663 0.80173275 1.         0.         0.4283188
 0.50479647 0.         0.99535142 1.         0.82951271 0.
 0.67865845 0.39464675]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32239785 1.
  1.         0.         0.        ]
 [1.         0.         0.32971442 0.45356815 0.30232909 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30204162 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3025174  1.
  0.         1.         1.        ]
 [1.         0.         0.28016861 0.34515984 0.3019691  1.
  0.         0.         1.        ]
 [1.         0.         0.17308994 0.23785826 0.30190646 1.
  0.         0.36437786 1.        ]
 [1.         0.         0.37912823 0.330746   0.30225088 1.
  0.         0.38596599 1.        ]
 [1.         0.         1.         1.         0.30238997 1.
  0.         1.         1.        ]
 [1.         0.         0.86626103 0.95186674 0.30201306 1.
  0.         0.         1.        ]
 [1.         0.         0.23202746 0.18637477 0.30210753 1.
  0.         0.0738146  1.        ]
 [1.         0.         1.         0.98120585 0.30192519 1.
  0.         0.89437551 1.        ]
 [1.         0.         0.52715939 0.52231323 0.30203169 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.30232341 1.
  0.         1.         1.        ]
 [1.         0.         0.87535172 0.81147461 0.30231063 1.
  0.         0.79600663 1.        ]
 [1.         0.         1.         1.         0.30204258 1.
  0.         0.80173275 1.        ]
 [1.         0.         1.         1.         0.30215701 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.30207264 1.
  0.         0.         1.        ]
 [1.         0.         0.37773443 0.39383823 0.30213733 1.
  0.         0.4283188  1.        ]
 [1.         0.         1.         1.         0.3020493  1.
  0.         0.50479647 1.        ]
 [1.         0.         0.08761833 0.03960253 0.30188325 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30212121 1.
  0.         0.99535142 1.        ]
 [1.         0.         1.         1.         0.30238897 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.30226536 1.
  0.         0.82951271 1.        ]
 [1.         0.         0.         0.         0.30216024 1.
  0.         0.         1.        ]
 [1.         0.         0.78640458 0.79937044 0.30224691 1.
  0.         0.67865845 1.        ]
 [1.         0.         0.61321236 0.64311436 0.30210645 1.
  0.         0.39464675 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 12 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         0.         0.80747163 0.47381271
 1.         0.         0.         1.         0.61833107 0.
 0.73165788 0.         0.         0.94291423 0.         0.
 0.18980526 0.         0.         1.         0.5496031  0.07508106
 1.         0.79063398]
wv_ed shape (26,)
[0.         0.         0.14493857 0.         0.90066306 0.75284382
 1.         0.         0.         1.         0.71579087 0.
 0.79880017 0.         0.         1.         0.         0.05061667
 0.26566903 0.         0.         1.         0.50858145 0.23239475
 1.         0.99623487]
wv_lg shape (26, 1)
[[0.32388503]
 [0.30438714]
 [0.30453698]
 [0.30445524]
 [0.30455145]
 [0.30435472]
 [0.30478077]
 [0.30430429]
 [0.30454232]
 [0.30457502]
 [0.3046236 ]
 [0.30442727]
 [0.30505426]
 [0.30426328]
 [0.30432715]
 [0.30464027]
 [0.30440795]
 [0.30411559]
 [0.30466578]
 [0.30453856]
 [0.30455457]
 [0.30467717]
 [0.30471538]
 [0.30459855]
 [0.30441   ]
 [0.30450887]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.25280127 0.         1.         0.87255547
 1.         0.         0.         1.         1.         0.
 1.         0.         0.         1.         0.21690277 0.
 0.96654706 0.39243671 0.93580155 1.         1.         0.57531832
 1.         0.55213025]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32388503 1.
  1.         0.         0.        ]
 [0.         0.         0.         0.         0.30438714 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.14493857 0.30453698 1.
  0.         0.25280127 1.        ]
 [1.         0.         0.         0.         0.30445524 1.
  0.         0.         1.        ]
 [1.         0.         0.80747163 0.90066306 0.30455145 1.
  0.         1.         1.        ]
 [1.         0.         0.47381271 0.75284382 0.30435472 1.
  0.         0.87255547 1.        ]
 [1.         0.         1.         1.         0.30478077 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30430429 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30454232 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30457502 1.
  0.         1.         1.        ]
 [1.         0.         0.61833107 0.71579087 0.3046236  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30442727 1.
  0.         0.         1.        ]
 [1.         0.         0.73165788 0.79880017 0.30505426 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30426328 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30432715 1.
  0.         0.         1.        ]
 [1.         0.         0.94291423 1.         0.30464027 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30440795 1.
  0.         0.21690277 1.        ]
 [1.         0.         0.         0.05061667 0.30411559 1.
  0.         0.         1.        ]
 [1.         0.         0.18980526 0.26566903 0.30466578 1.
  0.         0.96654706 1.        ]
 [1.         0.         0.         0.         0.30453856 1.
  0.         0.39243671 1.        ]
 [1.         0.         0.         0.         0.30455457 1.
  0.         0.93580155 1.        ]
 [1.         0.         1.         1.         0.30467717 1.
  0.         1.         1.        ]
 [1.         0.         0.5496031  0.50858145 0.30471538 1.
  0.         1.         1.        ]
 [1.         0.         0.07508106 0.23239475 0.30459855 1.
  0.         0.57531832 1.        ]
 [1.         0.         1.         1.         0.30441    1.
  0.         1.         1.        ]
 [1.         0.         0.79063398 0.99623487 0.30450887 1.
  0.         0.55213025 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 13 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.18426848 1.         0.         0.         0.
 0.         0.16419798 0.         1.         0.         1.
 0.         1.         0.1527061  1.         0.         0.
 0.10337152 0.         0.29159484 0.         1.         0.
 0.81044639 1.        ]
wv_ed shape (26,)
[0.         0.34972836 1.         0.         0.         0.
 0.         0.22592511 0.         1.         0.         1.
 0.         1.         0.20575799 1.         0.         0.1090055
 0.11312447 0.         0.28106798 0.         1.         0.
 0.72079346 1.        ]
wv_lg shape (26, 1)
[[0.32482317]
 [0.30734519]
 [0.3072339 ]
 [0.30718961]
 [0.30715343]
 [0.30705828]
 [0.30733945]
 [0.30731612]
 [0.30713926]
 [0.3074146 ]
 [0.30706692]
 [0.30735352]
 [0.30718914]
 [0.30732419]
 [0.30695012]
 [0.30745475]
 [0.3072355 ]
 [0.3070581 ]
 [0.30713349]
 [0.30705914]
 [0.30733829]
 [0.3070283 ]
 [0.3075946 ]
 [0.30722544]
 [0.30696069]
 [0.30756333]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.96895352 1.         0.         0.         0.23598238
 0.08077766 0.43610891 0.48226816 1.         0.19542984 1.
 0.89743873 1.         0.27853235 1.         0.         0.13949928
 0.59301813 0.17551799 0.59401438 0.         1.         0.
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32482317 1.
  1.         0.         0.        ]
 [1.         0.         0.18426848 0.34972836 0.30734519 1.
  0.         0.96895352 1.        ]
 [1.         0.         1.         1.         0.3072339  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30718961 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30715343 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30705828 1.
  0.         0.23598238 1.        ]
 [1.         0.         0.         0.         0.30733945 1.
  0.         0.08077766 1.        ]
 [1.         0.         0.16419798 0.22592511 0.30731612 1.
  0.         0.43610891 1.        ]
 [1.         0.         0.         0.         0.30713926 1.
  0.         0.48226816 1.        ]
 [1.         0.         1.         1.         0.3074146  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30706692 1.
  0.         0.19542984 1.        ]
 [1.         0.         1.         1.         0.30735352 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30718914 1.
  0.         0.89743873 1.        ]
 [1.         0.         1.         1.         0.30732419 1.
  0.         1.         1.        ]
 [1.         0.         0.1527061  0.20575799 0.30695012 1.
  0.         0.27853235 1.        ]
 [1.         0.         1.         1.         0.30745475 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3072355  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.1090055  0.3070581  1.
  0.         0.13949928 1.        ]
 [1.         0.         0.10337152 0.11312447 0.30713349 1.
  0.         0.59301813 1.        ]
 [1.         0.         0.         0.         0.30705914 1.
  0.         0.17551799 1.        ]
 [1.         0.         0.29159484 0.28106798 0.30733829 1.
  0.         0.59401438 1.        ]
 [0.         0.         0.         0.         0.3070283  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3075946  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30722544 1.
  0.         0.         1.        ]
 [1.         0.         0.81044639 0.72079346 0.30696069 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.30756333 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 14 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 0.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         0.         1.         0.
 1.         0.17884709 0.45392654 1.         0.         1.
 0.26620288 0.         0.79502711 0.         0.         0.49794765
 0.         0.48994873 0.         0.72027689 1.         0.
 0.         0.        ]
wv_ed shape (26,)
[0.         0.02718926 1.         0.         1.         0.
 1.         0.28052955 0.4975792  1.         0.         1.
 0.35340455 0.         0.89457016 0.         0.         0.58792799
 0.         0.43796947 0.         0.75787004 1.         0.
 0.         0.        ]
wv_lg shape (26, 1)
[[0.32639297]
 [0.30900215]
 [0.30954887]
 [0.30885942]
 [0.30882254]
 [0.3089613 ]
 [0.30893908]
 [0.30906592]
 [0.30898328]
 [0.30926575]
 [0.30894605]
 [0.30940809]
 [0.30915078]
 [0.3091738 ]
 [0.30901821]
 [0.30907001]
 [0.30912104]
 [0.30897467]
 [0.30878421]
 [0.30926181]
 [0.30906779]
 [0.30933563]
 [0.30923851]
 [0.30881728]
 [0.30916152]
 [0.30890385]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.         1.         0.
 0.94332533 0.         0.61624642 0.97169175 0.         1.
 0.29103363 0.         0.3138923  0.         0.         0.75954199
 0.17324533 1.         0.         1.         1.         0.
 0.29727472 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32639297 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.02718926 0.30900215 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30954887 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30885942 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30882254 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3089613  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30893908 1.
  0.         0.94332533 1.        ]
 [1.         0.         0.17884709 0.28052955 0.30906592 1.
  0.         0.         1.        ]
 [1.         0.         0.45392654 0.4975792  0.30898328 1.
  0.         0.61624642 1.        ]
 [1.         0.         1.         1.         0.30926575 1.
  0.         0.97169175 1.        ]
 [1.         0.         0.         0.         0.30894605 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.30940809 1.
  0.         1.         1.        ]
 [1.         0.         0.26620288 0.35340455 0.30915078 1.
  0.         0.29103363 1.        ]
 [1.         0.         0.         0.         0.3091738  1.
  0.         0.         1.        ]
 [1.         0.         0.79502711 0.89457016 0.30901821 1.
  0.         0.3138923  1.        ]
 [1.         0.         0.         0.         0.30907001 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30912104 1.
  0.         0.         1.        ]
 [1.         0.         0.49794765 0.58792799 0.30897467 1.
  0.         0.75954199 1.        ]
 [1.         0.         0.         0.         0.30878421 1.
  0.         0.17324533 1.        ]
 [1.         0.         0.48994873 0.43796947 0.30926181 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30906779 1.
  0.         0.         1.        ]
 [1.         0.         0.72027689 0.75787004 0.30933563 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.30923851 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.30881728 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.30916152 1.
  0.         0.29727472 1.        ]
 [0.         0.         0.         0.         0.30890385 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 15 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         1.         0.         0.4427297
 0.37962125 1.         0.         0.         0.47418218 0.
 0.15322528 0.         0.         0.24390106 0.         0.
 0.         0.         0.         0.         1.         0.
 0.         1.        ]
wv_ed shape (26,)
[0.         1.         0.         1.         0.00696678 0.47483713
 0.54410366 1.         0.         0.         0.47239544 0.
 0.         0.         0.         0.36992441 0.         0.14189419
 0.         0.         0.         0.         1.         0.
 0.         1.        ]
wv_lg shape (26, 1)
[[0.32752397]
 [0.31137106]
 [0.31103517]
 [0.31142861]
 [0.31105051]
 [0.31102081]
 [0.31126661]
 [0.31123329]
 [0.31100716]
 [0.31118155]
 [0.311249  ]
 [0.31116288]
 [0.31137727]
 [0.31120775]
 [0.31100561]
 [0.3113011 ]
 [0.31106902]
 [0.31119339]
 [0.31126802]
 [0.31097624]
 [0.31144977]
 [0.31096181]
 [0.31137975]
 [0.31116274]
 [0.31106174]
 [0.31107678]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         1.         0.14760443 1.
 0.         1.         0.         0.         1.         0.
 0.95108842 0.43468159 0.         1.         0.         0.24787416
 0.         0.         1.         0.         0.92476336 0.16396675
 0.         0.53615702]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32752397 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.31137106 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31103517 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31142861 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.00696678 0.31105051 1.
  0.         0.14760443 1.        ]
 [1.         0.         0.4427297  0.47483713 0.31102081 1.
  0.         1.         1.        ]
 [1.         0.         0.37962125 0.54410366 0.31126661 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31123329 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.31100716 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31118155 1.
  0.         0.         1.        ]
 [1.         0.         0.47418218 0.47239544 0.311249   1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31116288 1.
  0.         0.         1.        ]
 [1.         0.         0.15322528 0.         0.31137727 1.
  0.         0.95108842 1.        ]
 [1.         0.         0.         0.         0.31120775 1.
  0.         0.43468159 1.        ]
 [1.         0.         0.         0.         0.31100561 1.
  0.         0.         1.        ]
 [1.         0.         0.24390106 0.36992441 0.3113011  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31106902 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.14189419 0.31119339 1.
  0.         0.24787416 1.        ]
 [1.         0.         0.         0.         0.31126802 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31097624 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31144977 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31096181 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31137975 1.
  0.         0.92476336 1.        ]
 [1.         0.         0.         0.         0.31116274 1.
  0.         0.16396675 1.        ]
 [1.         0.         0.         0.         0.31106174 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31107678 1.
  0.         0.53615702 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 16 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.34187955 0.         0.51401085 1.         1.
 0.         0.         0.28259243 1.         0.89701044 0.
 1.         1.         0.26701694 0.         0.88319782 1.
 0.81710671 0.         1.         0.17527567 0.26457371 1.
 1.         0.56586728]
wv_ed shape (26,)
[0.         0.17406497 0.         0.59962091 1.         1.
 0.         0.         0.35591373 1.         0.93930094 0.
 1.         1.         0.35009166 0.         0.90212514 1.
 0.79235343 0.03160693 1.         0.23428275 0.22965318 1.
 1.         0.6105476 ]
wv_lg shape (26, 1)
[[0.32855688]
 [0.31311932]
 [0.31313101]
 [0.31320043]
 [0.31299431]
 [0.31325808]
 [0.31311231]
 [0.31301916]
 [0.31329113]
 [0.31305234]
 [0.31320747]
 [0.31317085]
 [0.31326928]
 [0.31314481]
 [0.31318181]
 [0.31330762]
 [0.31321167]
 [0.31300436]
 [0.31304151]
 [0.31305598]
 [0.31341326]
 [0.31330323]
 [0.31336722]
 [0.3131153 ]
 [0.31329422]
 [0.3133664 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.86125428 0.         0.12351895 0.22850889 0.59681986
 0.         0.         0.44709297 1.         0.         0.
 1.         1.         0.         0.         0.80285718 0.31049203
 1.         0.         1.         0.24481956 0.33005318 0.52247038
 1.         0.11698403]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32855688 1.
  1.         0.         0.        ]
 [1.         0.         0.34187955 0.17406497 0.31311932 1.
  0.         0.86125428 1.        ]
 [1.         0.         0.         0.         0.31313101 1.
  0.         0.         1.        ]
 [1.         0.         0.51401085 0.59962091 0.31320043 1.
  0.         0.12351895 1.        ]
 [1.         0.         1.         1.         0.31299431 1.
  0.         0.22850889 1.        ]
 [1.         0.         1.         1.         0.31325808 1.
  0.         0.59681986 1.        ]
 [1.         0.         0.         0.         0.31311231 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.31301916 1.
  0.         0.         1.        ]
 [1.         0.         0.28259243 0.35591373 0.31329113 1.
  0.         0.44709297 1.        ]
 [1.         0.         1.         1.         0.31305234 1.
  0.         1.         1.        ]
 [1.         0.         0.89701044 0.93930094 0.31320747 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31317085 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31326928 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31314481 1.
  0.         1.         1.        ]
 [1.         0.         0.26701694 0.35009166 0.31318181 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31330762 1.
  0.         0.         1.        ]
 [1.         0.         0.88319782 0.90212514 0.31321167 1.
  0.         0.80285718 1.        ]
 [1.         0.         1.         1.         0.31300436 1.
  0.         0.31049203 1.        ]
 [1.         0.         0.81710671 0.79235343 0.31304151 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.03160693 0.31305598 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31341326 1.
  0.         1.         1.        ]
 [1.         0.         0.17527567 0.23428275 0.31330323 1.
  0.         0.24481956 1.        ]
 [1.         0.         0.26457371 0.22965318 0.31336722 1.
  0.         0.33005318 1.        ]
 [1.         0.         1.         1.         0.3131153  1.
  0.         0.52247038 1.        ]
 [1.         0.         1.         1.         0.31329422 1.
  0.         1.         1.        ]
 [1.         0.         0.56586728 0.6105476  0.3133664  1.
  0.         0.11698403 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 17 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.74987089 1.         1.         1.
 0.94455826 0.04721552 0.         0.         1.         0.
 0.81431093 1.         1.         0.         0.         0.18949189
 0.         0.81967322 0.35826496 0.83727858 1.         0.
 1.         0.        ]
wv_ed shape (26,)
[0.         1.         0.5984008  1.         0.90471602 1.
 0.91005336 0.         0.         0.         1.         0.
 0.70991955 1.         1.         0.         0.         0.23663977
 0.         0.84101815 0.28189768 0.82723476 1.         0.
 1.         0.        ]
wv_lg shape (26, 1)
[[0.32984311]
 [0.31500223]
 [0.31491923]
 [0.31482132]
 [0.31461102]
 [0.31494495]
 [0.31462997]
 [0.31455075]
 [0.31493169]
 [0.31490397]
 [0.31475953]
 [0.31489125]
 [0.31470014]
 [0.314778  ]
 [0.31449554]
 [0.3147476 ]
 [0.3144128 ]
 [0.3146906 ]
 [0.31471696]
 [0.31479798]
 [0.31503968]
 [0.31494893]
 [0.31466599]
 [0.31467997]
 [0.31473862]
 [0.31469222]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         1.         1.         1.         1.
 0.58538218 0.01148207 0.         0.         1.         0.
 1.         1.         1.         0.         0.         0.
 0.         0.70505193 0.58071935 0.95381016 1.         0.
 1.         0.04883131]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.32984311 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.31500223 1.
  0.         1.         1.        ]
 [1.         0.         0.74987089 0.5984008  0.31491923 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31482132 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.90471602 0.31461102 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31494495 1.
  0.         1.         1.        ]
 [1.         0.         0.94455826 0.91005336 0.31462997 1.
  0.         0.58538218 1.        ]
 [1.         0.         0.04721552 0.         0.31455075 1.
  0.         0.01148207 1.        ]
 [1.         0.         0.         0.         0.31493169 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31490397 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31475953 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31489125 1.
  0.         0.         1.        ]
 [1.         0.         0.81431093 0.70991955 0.31470014 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.314778   1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31449554 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3147476  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3144128  1.
  0.         0.         1.        ]
 [1.         0.         0.18949189 0.23663977 0.3146906  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31471696 1.
  0.         0.         1.        ]
 [1.         0.         0.81967322 0.84101815 0.31479798 1.
  0.         0.70505193 1.        ]
 [1.         0.         0.35826496 0.28189768 0.31503968 1.
  0.         0.58071935 1.        ]
 [1.         0.         0.83727858 0.82723476 0.31494893 1.
  0.         0.95381016 1.        ]
 [1.         0.         1.         1.         0.31466599 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.31467997 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31473862 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31469222 1.
  0.         0.04883131 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 18 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.25906604 1.         0.7088068  0.
 0.         0.         0.46809809 0.         0.         1.
 0.         0.         0.         0.         0.         0.18731453
 0.74949646 0.         0.02883138 0.         0.         0.27081278
 0.57953307 0.91867368]
wv_ed shape (26,)
[0.         0.         0.29300826 1.         0.74073929 0.
 0.         0.         0.44280281 0.         0.         1.
 0.         0.         0.         0.         0.         0.22943355
 0.69803428 0.         0.01113109 0.         0.         0.33004243
 0.68459681 1.        ]
wv_lg shape (26, 1)
[[0.33103277]
 [0.31613757]
 [0.31625858]
 [0.31671789]
 [0.31630524]
 [0.31625603]
 [0.31631799]
 [0.31626997]
 [0.31619285]
 [0.3162859 ]
 [0.31623036]
 [0.3162683 ]
 [0.31626326]
 [0.31648164]
 [0.31604106]
 [0.31646598]
 [0.31632297]
 [0.31640569]
 [0.31631863]
 [0.3162355 ]
 [0.31629274]
 [0.31643834]
 [0.31618876]
 [0.31641772]
 [0.31636733]
 [0.31626788]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         1.         0.84257554 0.
 0.         0.         0.80233643 0.         0.         1.
 0.         0.         0.         0.20961857 0.         0.12099807
 0.75412803 0.         0.         0.         0.         0.4756833
 0.21445169 0.26838209]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33103277 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.31613757 1.
  0.         0.         1.        ]
 [1.         0.         0.25906604 0.29300826 0.31625858 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31671789 1.
  0.         1.         1.        ]
 [1.         0.         0.7088068  0.74073929 0.31630524 1.
  0.         0.84257554 1.        ]
 [1.         0.         0.         0.         0.31625603 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31631799 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31626997 1.
  0.         0.         1.        ]
 [1.         0.         0.46809809 0.44280281 0.31619285 1.
  0.         0.80233643 1.        ]
 [1.         0.         0.         0.         0.3162859  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31623036 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3162683  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31626326 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31648164 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.31604106 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31646598 1.
  0.         0.20961857 1.        ]
 [1.         0.         0.         0.         0.31632297 1.
  0.         0.         1.        ]
 [1.         0.         0.18731453 0.22943355 0.31640569 1.
  0.         0.12099807 1.        ]
 [1.         0.         0.74949646 0.69803428 0.31631863 1.
  0.         0.75412803 1.        ]
 [1.         0.         0.         0.         0.3162355  1.
  0.         0.         1.        ]
 [1.         0.         0.02883138 0.01113109 0.31629274 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31643834 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31618876 1.
  0.         0.         1.        ]
 [1.         0.         0.27081278 0.33004243 0.31641772 1.
  0.         0.4756833  1.        ]
 [1.         0.         0.57953307 0.68459681 0.31636733 1.
  0.         0.21445169 1.        ]
 [1.         0.         0.91867368 1.         0.31626788 1.
  0.         0.26838209 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 19 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.68797226 0.10184491 0.34198793 0.62992077 1.
 0.20492969 0.99658731 0.         0.0851236  0.         0.
 0.50074692 0.         0.         1.         0.71305855 1.
 1.         0.         1.         1.         1.         0.
 0.27395871 0.        ]
wv_ed shape (26,)
[0.         0.69422842 0.         0.45065163 0.74910167 1.
 0.1676228  0.88252295 0.         0.07188785 0.01773637 0.
 0.39645102 0.         0.         1.         0.78007869 1.
 1.         0.         1.         1.         1.         0.
 0.27797367 0.        ]
wv_lg shape (26, 1)
[[0.33182157]
 [0.31802622]
 [0.31811027]
 [0.31805859]
 [0.31804951]
 [0.31835113]
 [0.31800225]
 [0.31828601]
 [0.31808033]
 [0.31818559]
 [0.31805364]
 [0.3180415 ]
 [0.31843235]
 [0.31787927]
 [0.31791532]
 [0.31847688]
 [0.3183352 ]
 [0.31779893]
 [0.31824098]
 [0.31812009]
 [0.31843184]
 [0.31819492]
 [0.31840749]
 [0.31798881]
 [0.31836186]
 [0.31800996]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         0.         0.37395238 1.
 0.13557609 1.         0.         0.         0.         0.
 1.         0.         0.         0.96805806 0.15341957 1.
 0.75507019 0.         1.         1.         1.         0.
 0.35555706 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33182157 1.
  1.         0.         0.        ]
 [1.         0.         0.68797226 0.69422842 0.31802622 1.
  0.         0.         1.        ]
 [1.         0.         0.10184491 0.         0.31811027 1.
  0.         0.         1.        ]
 [1.         0.         0.34198793 0.45065163 0.31805859 1.
  0.         0.         1.        ]
 [1.         0.         0.62992077 0.74910167 0.31804951 1.
  0.         0.37395238 1.        ]
 [1.         0.         1.         1.         0.31835113 1.
  0.         1.         1.        ]
 [1.         0.         0.20492969 0.1676228  0.31800225 1.
  0.         0.13557609 1.        ]
 [1.         0.         0.99658731 0.88252295 0.31828601 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31808033 1.
  0.         0.         1.        ]
 [1.         0.         0.0851236  0.07188785 0.31818559 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.01773637 0.31805364 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3180415  1.
  0.         0.         1.        ]
 [1.         0.         0.50074692 0.39645102 0.31843235 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.31787927 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.31791532 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31847688 1.
  0.         0.96805806 1.        ]
 [1.         0.         0.71305855 0.78007869 0.3183352  1.
  0.         0.15341957 1.        ]
 [1.         0.         1.         1.         0.31779893 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31824098 1.
  0.         0.75507019 1.        ]
 [1.         0.         0.         0.         0.31812009 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31843184 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31819492 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31840749 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31798881 1.
  0.         0.         1.        ]
 [1.         0.         0.27395871 0.27797367 0.31836186 1.
  0.         0.35555706 1.        ]
 [1.         0.         0.         0.         0.31800996 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 20 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         1.         0.         1.
 0.         0.17032741 0.68615479 1.         1.         0.12389009
 0.         1.         0.         1.         1.         0.
 0.91754449 0.         1.         1.         0.27915407 0.89383316
 1.         1.        ]
wv_ed shape (26,)
[0.         0.         1.         1.         0.         1.
 0.         0.         0.58847132 1.         1.         0.
 0.         1.         0.         1.         1.         0.
 0.75298701 0.         1.         1.         0.21408767 0.73782619
 1.         1.        ]
wv_lg shape (26, 1)
[[0.33288311]
 [0.31938753]
 [0.31991409]
 [0.31973159]
 [0.31958866]
 [0.31964008]
 [0.31943938]
 [0.31960099]
 [0.3195602 ]
 [0.31956905]
 [0.31972263]
 [0.31969444]
 [0.31994566]
 [0.3195691 ]
 [0.31967408]
 [0.31962166]
 [0.31966222]
 [0.31957722]
 [0.31962268]
 [0.31958575]
 [0.31983933]
 [0.31977689]
 [0.31944216]
 [0.31963534]
 [0.3196685 ]
 [0.31964002]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         1.         0.         1.
 0.         0.69965214 0.         1.         1.         1.
 0.44665551 1.         0.         1.         1.         0.
 1.         0.         1.         1.         0.         1.
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33288311 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.31938753 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31991409 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31973159 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31958866 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31964008 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31943938 1.
  0.         0.         1.        ]
 [1.         0.         0.17032741 0.         0.31960099 1.
  0.         0.69965214 1.        ]
 [1.         0.         0.68615479 0.58847132 0.3195602  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31956905 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31972263 1.
  0.         1.         1.        ]
 [1.         0.         0.12389009 0.         0.31969444 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31994566 1.
  0.         0.44665551 1.        ]
 [1.         0.         1.         1.         0.3195691  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31967408 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31962166 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31966222 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.31957722 1.
  0.         0.         1.        ]
 [1.         0.         0.91754449 0.75298701 0.31962268 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.31958575 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.31983933 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31977689 1.
  0.         1.         1.        ]
 [1.         0.         0.27915407 0.21408767 0.31944216 1.
  0.         0.         1.        ]
 [1.         0.         0.89383316 0.73782619 0.31963534 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3196685  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.31964002 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 21 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.59305847 0.1916275  0.         0.83458294 0.
 0.56145744 1.         0.         0.17729805 0.         0.27949783
 0.13475064 0.         1.         1.         0.75847238 0.
 1.         0.         0.84615293 0.48943321 0.6761226  1.
 1.         1.        ]
wv_ed shape (26,)
[0.         0.68073768 0.31873412 0.         0.90663782 0.
 0.65339551 1.         0.         0.22623532 0.         0.41088108
 0.26363371 0.         1.         1.         0.92094637 0.
 1.         0.         0.90444165 0.55058523 0.78396    1.
 1.         1.        ]
wv_lg shape (26, 1)
[[0.3339813 ]
 [0.32092633]
 [0.32105474]
 [0.32077071]
 [0.32097726]
 [0.32066164]
 [0.32080102]
 [0.32076636]
 [0.32087312]
 [0.32098426]
 [0.32084451]
 [0.32082816]
 [0.32105546]
 [0.32083294]
 [0.32110851]
 [0.32106874]
 [0.32105117]
 [0.32080665]
 [0.32096076]
 [0.32091386]
 [0.32095653]
 [0.32080823]
 [0.32095203]
 [0.32094278]
 [0.32109661]
 [0.32082066]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.48625054 0.6195285  0.         0.91674583 0.
 0.23694638 1.         0.         0.91087334 0.         0.
 0.         0.         1.         1.         0.66149465 0.
 1.         0.         0.68216131 0.40043514 0.93875098 1.
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3339813  1.
  1.         0.         0.        ]
 [1.         0.         0.59305847 0.68073768 0.32092633 1.
  0.         0.48625054 1.        ]
 [1.         0.         0.1916275  0.31873412 0.32105474 1.
  0.         0.6195285  1.        ]
 [1.         0.         0.         0.         0.32077071 1.
  0.         0.         1.        ]
 [1.         0.         0.83458294 0.90663782 0.32097726 1.
  0.         0.91674583 1.        ]
 [1.         0.         0.         0.         0.32066164 1.
  0.         0.         1.        ]
 [1.         0.         0.56145744 0.65339551 0.32080102 1.
  0.         0.23694638 1.        ]
 [1.         0.         1.         1.         0.32076636 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32087312 1.
  0.         0.         1.        ]
 [1.         0.         0.17729805 0.22623532 0.32098426 1.
  0.         0.91087334 1.        ]
 [1.         0.         0.         0.         0.32084451 1.
  0.         0.         1.        ]
 [1.         0.         0.27949783 0.41088108 0.32082816 1.
  0.         0.         1.        ]
 [1.         0.         0.13475064 0.26363371 0.32105546 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32083294 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32110851 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32106874 1.
  0.         1.         1.        ]
 [1.         0.         0.75847238 0.92094637 0.32105117 1.
  0.         0.66149465 1.        ]
 [0.         0.         0.         0.         0.32080665 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32096076 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32091386 1.
  0.         0.         1.        ]
 [1.         0.         0.84615293 0.90444165 0.32095653 1.
  0.         0.68216131 1.        ]
 [1.         0.         0.48943321 0.55058523 0.32080823 1.
  0.         0.40043514 1.        ]
 [1.         0.         0.6761226  0.78396    0.32095203 1.
  0.         0.93875098 1.        ]
 [1.         0.         1.         1.         0.32094278 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32109661 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32082066 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 22 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         0.56120763 1.         0.
 1.         0.4607881  0.         0.         1.         0.25769036
 0.30931207 0.80086495 0.75348414 1.         1.         0.54794845
 0.         0.53329831 1.         1.         1.         0.37952189
 0.         1.        ]
wv_ed shape (26,)
[0.         1.         0.         0.58755822 1.         0.
 1.         0.37300836 0.         0.         1.         0.11325691
 0.20638861 0.70681074 0.81697004 1.         1.         0.51386576
 0.         0.52024844 1.         1.         1.         0.35982512
 0.         1.        ]
wv_lg shape (26, 1)
[[0.33475588]
 [0.32240156]
 [0.32239818]
 [0.32258625]
 [0.32248649]
 [0.3222349 ]
 [0.32259941]
 [0.32244758]
 [0.32233958]
 [0.32237962]
 [0.32241499]
 [0.32244801]
 [0.32241322]
 [0.32252778]
 [0.32242812]
 [0.32246909]
 [0.32232774]
 [0.32238712]
 [0.32243346]
 [0.32240304]
 [0.32248727]
 [0.32253915]
 [0.32264523]
 [0.32237222]
 [0.32232789]
 [0.32255807]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         1.         1.         0.
 1.         0.9531213  0.         0.251494   1.         1.
 0.78309133 1.         1.         0.89430256 1.         0.92721497
 0.33838027 0.9085301  1.         1.         1.         1.
 0.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33475588 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.32240156 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32239818 1.
  0.         0.         1.        ]
 [1.         0.         0.56120763 0.58755822 0.32258625 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32248649 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.3222349  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32259941 1.
  0.         1.         1.        ]
 [1.         0.         0.4607881  0.37300836 0.32244758 1.
  0.         0.9531213  1.        ]
 [1.         0.         0.         0.         0.32233958 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32237962 1.
  0.         0.251494   1.        ]
 [1.         0.         1.         1.         0.32241499 1.
  0.         1.         1.        ]
 [1.         0.         0.25769036 0.11325691 0.32244801 1.
  0.         1.         1.        ]
 [1.         0.         0.30931207 0.20638861 0.32241322 1.
  0.         0.78309133 1.        ]
 [1.         0.         0.80086495 0.70681074 0.32252778 1.
  0.         1.         1.        ]
 [1.         0.         0.75348414 0.81697004 0.32242812 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32246909 1.
  0.         0.89430256 1.        ]
 [1.         0.         1.         1.         0.32232774 1.
  0.         1.         1.        ]
 [1.         0.         0.54794845 0.51386576 0.32238712 1.
  0.         0.92721497 1.        ]
 [1.         0.         0.         0.         0.32243346 1.
  0.         0.33838027 1.        ]
 [1.         0.         0.53329831 0.52024844 0.32240304 1.
  0.         0.9085301  1.        ]
 [1.         0.         1.         1.         0.32248727 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32253915 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32264523 1.
  0.         1.         1.        ]
 [1.         0.         0.37952189 0.35982512 0.32237222 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32232789 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32255807 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 23 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         1.         0.         0.
 1.         0.         1.         0.         0.         0.06631622
 0.82006623 1.         0.97986659 0.44685308 0.         1.
 0.         1.         0.         0.         0.13742653 0.16213335
 0.02019652 0.28583405]
wv_ed shape (26,)
[0.         0.         1.         1.         0.05860044 0.
 1.         0.         1.         0.         0.         0.16000724
 0.96071034 1.         1.         0.50858308 0.         1.
 0.         1.         0.         0.         0.20080257 0.08561017
 0.23913139 0.2816284 ]
wv_lg shape (26, 1)
[[0.3355306 ]
 [0.32362198]
 [0.32375613]
 [0.32386437]
 [0.32367265]
 [0.3237824 ]
 [0.32400249]
 [0.32379598]
 [0.32386693]
 [0.3237301 ]
 [0.32365253]
 [0.32384742]
 [0.32408691]
 [0.32398869]
 [0.3239979 ]
 [0.32409192]
 [0.32374957]
 [0.32393192]
 [0.32376024]
 [0.32387993]
 [0.3238418 ]
 [0.32383619]
 [0.32390581]
 [0.32383194]
 [0.32386659]
 [0.32391937]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.96447313 0.         0.
 1.         0.         1.         1.         0.         1.
 1.         1.         1.         1.         0.18935562 0.98922476
 0.         1.         0.         0.         0.37545487 0.19334158
 0.4386275  1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3355306  1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.32362198 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32375613 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32386437 1.
  0.         0.96447313 1.        ]
 [1.         0.         0.         0.05860044 0.32367265 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3237824  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32400249 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32379598 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32386693 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3237301  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32365253 1.
  0.         0.         1.        ]
 [1.         0.         0.06631622 0.16000724 0.32384742 1.
  0.         1.         1.        ]
 [1.         0.         0.82006623 0.96071034 0.32408691 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32398869 1.
  0.         1.         1.        ]
 [1.         0.         0.97986659 1.         0.3239979  1.
  0.         1.         1.        ]
 [1.         0.         0.44685308 0.50858308 0.32409192 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32374957 1.
  0.         0.18935562 1.        ]
 [1.         0.         1.         1.         0.32393192 1.
  0.         0.98922476 1.        ]
 [1.         0.         0.         0.         0.32376024 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32387993 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.3238418  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32383619 1.
  0.         0.         1.        ]
 [1.         0.         0.13742653 0.20080257 0.32390581 1.
  0.         0.37545487 1.        ]
 [1.         0.         0.16213335 0.08561017 0.32383194 1.
  0.         0.19334158 1.        ]
 [1.         0.         0.02019652 0.23913139 0.32386659 1.
  0.         0.4386275  1.        ]
 [1.         0.         0.28583405 0.2816284  0.32391937 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 24 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.02068602 0.30834808 0.05431109 0.         0.
 0.         0.41302183 1.         1.         0.         0.
 0.         0.         0.         0.52451956 0.28177311 0.
 0.5465124  0.13297765 0.         0.83706971 0.         0.
 0.         1.        ]
wv_ed shape (26,)
[0.         0.07994946 0.32378217 0.05935668 0.         0.
 0.         0.47255235 1.         1.         0.         0.
 0.         0.         0.         0.53014365 0.33316682 0.
 0.55552907 0.06518635 0.         0.84597023 0.         0.
 0.         1.        ]
wv_lg shape (26, 1)
[[0.33647935]
 [0.32484309]
 [0.3251127 ]
 [0.32500781]
 [0.32492342]
 [0.32497848]
 [0.32502626]
 [0.32516358]
 [0.32501592]
 [0.32501995]
 [0.32488154]
 [0.32486915]
 [0.32479578]
 [0.32508769]
 [0.32509186]
 [0.32504535]
 [0.32495551]
 [0.32498429]
 [0.3251511 ]
 [0.32509099]
 [0.32501146]
 [0.32510123]
 [0.32498303]
 [0.32513825]
 [0.32486554]
 [0.32513464]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.14969295 0.30343892 0.80764594 0.         0.
 0.         1.         1.         1.         0.         0.
 0.         0.23024074 0.02119303 1.         0.24685337 0.
 0.86092156 0.59416413 0.         1.         0.         0.
 0.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33647935 1.
  1.         0.         0.        ]
 [1.         0.         0.02068602 0.07994946 0.32484309 1.
  0.         0.14969295 1.        ]
 [1.         0.         0.30834808 0.32378217 0.3251127  1.
  0.         0.30343892 1.        ]
 [1.         0.         0.05431109 0.05935668 0.32500781 1.
  0.         0.80764594 1.        ]
 [1.         0.         0.         0.         0.32492342 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32497848 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32502626 1.
  0.         0.         1.        ]
 [1.         0.         0.41302183 0.47255235 0.32516358 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32501592 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32501995 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32488154 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32486915 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32479578 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32508769 1.
  0.         0.23024074 1.        ]
 [1.         0.         0.         0.         0.32509186 1.
  0.         0.02119303 1.        ]
 [1.         0.         0.52451956 0.53014365 0.32504535 1.
  0.         1.         1.        ]
 [1.         0.         0.28177311 0.33316682 0.32495551 1.
  0.         0.24685337 1.        ]
 [0.         0.         0.         0.         0.32498429 1.
  0.         0.         1.        ]
 [1.         0.         0.5465124  0.55552907 0.3251511  1.
  0.         0.86092156 1.        ]
 [1.         0.         0.13297765 0.06518635 0.32509099 1.
  0.         0.59416413 1.        ]
 [1.         0.         0.         0.         0.32501146 1.
  0.         0.         1.        ]
 [1.         0.         0.83706971 0.84597023 0.32510123 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32498303 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32513825 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32486554 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32513464 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 25 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.14978587 1.         0.         1.         1.
 1.         0.28512427 0.10475386 0.17827424 0.         1.
 0.58609764 1.         0.62816838 0.50461869 1.         0.64521623
 0.12121497 0.26227154 0.56146871 0.         0.68654848 0.
 0.         0.        ]
wv_ed shape (26,)
[0.         0.07199989 1.         0.         1.         1.
 1.         0.14934304 0.04141558 0.08074192 0.         1.
 0.5751233  1.         0.58800922 0.51246644 1.         0.61307198
 0.01927985 0.19392711 0.42662161 0.         0.65168992 0.
 0.         0.        ]
wv_lg shape (26, 1)
[[0.33745809]
 [0.32596762]
 [0.32612195]
 [0.32587154]
 [0.32625516]
 [0.32599096]
 [0.32618347]
 [0.32605277]
 [0.32607405]
 [0.32612534]
 [0.3260917 ]
 [0.3260626 ]
 [0.32612391]
 [0.32604016]
 [0.32621433]
 [0.32615258]
 [0.32614262]
 [0.32601463]
 [0.325883  ]
 [0.32618376]
 [0.32622801]
 [0.32613856]
 [0.32613115]
 [0.32609111]
 [0.32588393]
 [0.32591011]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.16955622 1.         0.         1.         1.
 1.         0.73515362 0.89225511 0.84889379 0.         1.
 1.         1.         0.80272098 1.         1.         0.54824821
 0.31013911 0.63081084 1.         0.31529545 1.         0.
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33745809 1.
  1.         0.         0.        ]
 [1.         0.         0.14978587 0.07199989 0.32596762 1.
  0.         0.16955622 1.        ]
 [1.         0.         1.         1.         0.32612195 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.32587154 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32625516 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32599096 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32618347 1.
  0.         1.         1.        ]
 [1.         0.         0.28512427 0.14934304 0.32605277 1.
  0.         0.73515362 1.        ]
 [1.         0.         0.10475386 0.04141558 0.32607405 1.
  0.         0.89225511 1.        ]
 [1.         0.         0.17827424 0.08074192 0.32612534 1.
  0.         0.84889379 1.        ]
 [1.         0.         0.         0.         0.3260917  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3260626  1.
  0.         1.         1.        ]
 [1.         0.         0.58609764 0.5751233  0.32612391 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32604016 1.
  0.         1.         1.        ]
 [1.         0.         0.62816838 0.58800922 0.32621433 1.
  0.         0.80272098 1.        ]
 [1.         0.         0.50461869 0.51246644 0.32615258 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32614262 1.
  0.         1.         1.        ]
 [1.         0.         0.64521623 0.61307198 0.32601463 1.
  0.         0.54824821 1.        ]
 [1.         0.         0.12121497 0.01927985 0.325883   1.
  0.         0.31013911 1.        ]
 [1.         0.         0.26227154 0.19392711 0.32618376 1.
  0.         0.63081084 1.        ]
 [1.         0.         0.56146871 0.42662161 0.32622801 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32613856 1.
  0.         0.31529545 1.        ]
 [1.         0.         0.68654848 0.65168992 0.32613115 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32609111 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32588393 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32591011 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 26 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         0.         1.         0.
 1.         1.         1.         1.         0.77840744 1.
 1.         1.         0.         1.         0.         1.
 0.         0.66697211 0.         0.         1.         0.89688118
 0.         1.        ]
wv_ed shape (26,)
[0.         1.         1.         0.         1.         0.
 1.         1.         1.         1.         0.67766126 1.
 1.         1.         0.         1.         0.         1.
 0.         0.68174624 0.         0.         1.         0.82501914
 0.         1.        ]
wv_lg shape (26, 1)
[[0.33815207]
 [0.32753041]
 [0.32727697]
 [0.32725887]
 [0.32728419]
 [0.32720093]
 [0.32750052]
 [0.32750254]
 [0.32737827]
 [0.3272904 ]
 [0.32736331]
 [0.32737436]
 [0.32752241]
 [0.3272769 ]
 [0.32746855]
 [0.32759314]
 [0.32714131]
 [0.3276006 ]
 [0.32728487]
 [0.32715449]
 [0.32731943]
 [0.32724222]
 [0.3274395 ]
 [0.32735266]
 [0.32734261]
 [0.32730723]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.97082082 0.         1.         0.
 1.         1.         1.         1.         0.60209101 1.
 1.         1.         0.         1.         0.         1.
 0.         0.22101634 0.         0.05372076 1.         0.38200661
 0.         0.62147324]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33815207 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.32753041 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32727697 1.
  0.         0.97082082 1.        ]
 [1.         0.         0.         0.         0.32725887 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32728419 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32720093 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32750052 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32750254 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32737827 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3272904  1.
  0.         1.         1.        ]
 [1.         0.         0.77840744 0.67766126 0.32736331 1.
  0.         0.60209101 1.        ]
 [1.         0.         1.         1.         0.32737436 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32752241 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3272769  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32746855 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32759314 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32714131 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3276006  1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.32728487 1.
  0.         0.         1.        ]
 [1.         0.         0.66697211 0.68174624 0.32715449 1.
  0.         0.22101634 1.        ]
 [1.         0.         0.         0.         0.32731943 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32724222 1.
  0.         0.05372076 1.        ]
 [1.         0.         1.         1.         0.3274395  1.
  0.         1.         1.        ]
 [1.         0.         0.89688118 0.82501914 0.32735266 1.
  0.         0.38200661 1.        ]
 [1.         0.         0.         0.         0.32734261 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32730723 1.
  0.         0.62147324 1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 27 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.93670002 0.         1.         0.28329529 1.
 0.85224227 0.         0.02788474 1.         0.         0.44630542
 0.00887325 0.11664338 0.         1.         0.         1.
 0.         0.         0.         1.         1.         0.
 0.         0.        ]
wv_ed shape (26,)
[0.         0.77525442 0.         0.99807747 0.2835051  1.
 0.74870537 0.         0.         1.         0.         0.48752622
 0.         0.03471063 0.         1.         0.         1.
 0.         0.         0.         1.         1.         0.
 0.         0.        ]
wv_lg shape (26, 1)
[[0.33908419]
 [0.32828581]
 [0.32816539]
 [0.32821625]
 [0.32830372]
 [0.32844855]
 [0.3284368 ]
 [0.32821977]
 [0.3282548 ]
 [0.32844734]
 [0.32833319]
 [0.32834347]
 [0.32832539]
 [0.32831781]
 [0.32846443]
 [0.32846119]
 [0.32825746]
 [0.32841088]
 [0.3282854 ]
 [0.32835058]
 [0.32830181]
 [0.32832768]
 [0.32840625]
 [0.32832458]
 [0.32834545]
 [0.32830333]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         1.         0.14783121 1.
 1.         0.         0.         1.         0.02341218 0.
 0.         0.2371965  0.45199254 1.         0.         1.
 0.         0.         0.         0.95361516 1.         0.
 0.26812648 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.33908419 1.
  1.         0.         0.        ]
 [1.         0.         0.93670002 0.77525442 0.32828581 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32816539 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.99807747 0.32821625 1.
  0.         1.         1.        ]
 [1.         0.         0.28329529 0.2835051  0.32830372 1.
  0.         0.14783121 1.        ]
 [1.         0.         1.         1.         0.32844855 1.
  0.         1.         1.        ]
 [1.         0.         0.85224227 0.74870537 0.3284368  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32821977 1.
  0.         0.         1.        ]
 [1.         0.         0.02788474 0.         0.3282548  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32844734 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32833319 1.
  0.         0.02341218 1.        ]
 [1.         0.         0.44630542 0.48752622 0.32834347 1.
  0.         0.         1.        ]
 [1.         0.         0.00887325 0.         0.32832539 1.
  0.         0.         1.        ]
 [1.         0.         0.11664338 0.03471063 0.32831781 1.
  0.         0.2371965  1.        ]
 [1.         0.         0.         0.         0.32846443 1.
  0.         0.45199254 1.        ]
 [1.         0.         1.         1.         0.32846119 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32825746 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32841088 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3282854  1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.32835058 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32830181 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32832768 1.
  0.         0.95361516 1.        ]
 [1.         0.         1.         1.         0.32840625 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32832458 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32834545 1.
  0.         0.26812648 1.        ]
 [1.         0.         0.         0.         0.32830333 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 28 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.02410237 0.         0.         0.54141858 0.
 0.         0.         1.         1.         1.         0.
 0.         1.         1.         0.         0.51637389 0.
 0.2582194  0.         0.55602732 1.         0.00516916 0.73749093
 0.21778864 0.46507175]
wv_ed shape (26,)
[0.         0.03759417 0.         0.         0.71068012 0.
 0.         0.         1.         1.         1.         0.
 0.         1.         1.         0.         0.49340801 0.
 0.34666955 0.         0.62977818 1.         0.09703903 0.94042538
 0.24926774 0.49157424]
wv_lg shape (26, 1)
[[0.3396369 ]
 [0.32951285]
 [0.32957143]
 [0.32968207]
 [0.32954441]
 [0.32954672]
 [0.3294751 ]
 [0.32952892]
 [0.32985938]
 [0.3295272 ]
 [0.32969061]
 [0.32949932]
 [0.32953654]
 [0.32963228]
 [0.32957244]
 [0.32956268]
 [0.32978098]
 [0.32958655]
 [0.32964435]
 [0.32947791]
 [0.32965961]
 [0.32948212]
 [0.32961005]
 [0.32958866]
 [0.32977726]
 [0.32982268]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.37588517 0.         0.         0.         0.
 0.         0.         1.         1.         1.         0.
 0.         1.         1.         0.         1.         0.
 0.32003575 0.16387963 0.92455223 1.         0.12608596 0.42687334
 0.50501354 1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3396369  1.
  1.         0.         0.        ]
 [1.         0.         0.02410237 0.03759417 0.32951285 1.
  0.         0.37588517 1.        ]
 [1.         0.         0.         0.         0.32957143 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32968207 1.
  0.         0.         1.        ]
 [1.         0.         0.54141858 0.71068012 0.32954441 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32954672 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3294751  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32952892 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32985938 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3295272  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32969061 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32949932 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.32953654 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.32963228 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.32957244 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.32956268 1.
  0.         0.         1.        ]
 [1.         0.         0.51637389 0.49340801 0.32978098 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.32958655 1.
  0.         0.         1.        ]
 [1.         0.         0.2582194  0.34666955 0.32964435 1.
  0.         0.32003575 1.        ]
 [1.         0.         0.         0.         0.32947791 1.
  0.         0.16387963 1.        ]
 [1.         0.         0.55602732 0.62977818 0.32965961 1.
  0.         0.92455223 1.        ]
 [1.         0.         1.         1.         0.32948212 1.
  0.         1.         1.        ]
 [1.         0.         0.00516916 0.09703903 0.32961005 1.
  0.         0.12608596 1.        ]
 [1.         0.         0.73749093 0.94042538 0.32958866 1.
  0.         0.42687334 1.        ]
 [1.         0.         0.21778864 0.24926774 0.32977726 1.
  0.         0.50501354 1.        ]
 [1.         0.         0.46507175 0.49157424 0.32982268 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 29 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         1.         0.7943641  1.
 1.         1.         0.85937485 0.         1.         0.
 0.44804646 0.         0.50060454 0.19929143 0.         0.
 1.         1.         0.         0.6306719  0.         0.28032727
 1.         1.        ]
wv_ed shape (26,)
[0.         1.         1.         1.         0.00496668 0.65489265
 0.98193021 1.         0.         0.         0.41158248 0.
 0.         0.         0.         0.         0.         0.
 0.         0.37155032 0.         0.         0.         0.
 1.         0.62099155]
wv_lg shape (26, 1)
[[0.38639993]
 [0.38100434]
 [0.38125389]
 [0.38148533]
 [0.38131993]
 [0.38066021]
 [0.37962979]
 [0.38162466]
 [0.37949414]
 [0.37919633]
 [0.37970896]
 [0.37725735]
 [0.38026462]
 [0.37827411]
 [0.38045189]
 [0.37975362]
 [0.38110122]
 [0.37938575]
 [0.3814589 ]
 [0.3807398 ]
 [0.37879713]
 [0.37849229]
 [0.37720956]
 [0.37934706]
 [0.3807979 ]
 [0.38194338]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1.         0.         0.         0.         0.         0.
 0.         0.09041631 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
wv_std shape (26,)
[0.         1.         1.         1.         0.4176775  1.
 0.74742476 1.         0.7008563  0.         1.         0.
 0.06308694 0.         0.80122254 1.         0.         0.
 0.63077898 1.         0.         0.32803347 0.         0.12606981
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38639993 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.38100434 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38125389 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38148533 1.
  0.         1.         1.        ]
 [1.         0.         0.7943641  0.00496668 0.38131993 1.
  0.         0.4176775  1.        ]
 [1.         0.         1.         0.65489265 0.38066021 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.98193021 0.37962979 1.
  0.         0.74742476 1.        ]
 [1.         0.         1.         1.         0.38162466 1.
  0.09041631 1.         1.        ]
 [1.         0.         0.85937485 0.         0.37949414 1.
  0.         0.7008563  1.        ]
 [1.         0.         0.         0.         0.37919633 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.41158248 0.37970896 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.37725735 1.
  0.         0.         1.        ]
 [1.         0.         0.44804646 0.         0.38026462 1.
  0.         0.06308694 1.        ]
 [1.         0.         0.         0.         0.37827411 1.
  0.         0.         1.        ]
 [1.         0.         0.50060454 0.         0.38045189 1.
  0.         0.80122254 1.        ]
 [1.         0.         0.19929143 0.         0.37975362 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38110122 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.37938575 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.         0.3814589  1.
  0.         0.63077898 1.        ]
 [1.         0.         1.         0.37155032 0.3807398  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.37879713 1.
  0.         0.         1.        ]
 [1.         0.         0.6306719  0.         0.37849229 1.
  0.         0.32803347 1.        ]
 [1.         0.         0.         0.         0.37720956 1.
  0.         0.         1.        ]
 [1.         0.         0.28032727 0.         0.37934706 1.
  0.         0.12606981 1.        ]
 [1.         0.         1.         1.         0.3807979  1.
  0.         1.         1.        ]
 [1.         0.         1.         0.62099155 0.38194338 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 0 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         1.         1.         0.63517877
 0.10784743 0.4857013  0.67334908 0.         1.         1.
 1.         0.87712985 0.53059054 1.         0.41023852 1.
 1.         1.         1.         0.         0.57492478 1.
 0.74731391 0.23020344]
wv_ed shape (26,)
[0.         0.95714675 1.         1.         1.         0.76471432
 0.05817063 0.4013901  0.46982163 0.         1.         1.
 0.76116536 0.64717243 0.43543085 0.94372209 0.25578549 1.
 1.         1.         1.         0.         0.51918866 1.
 0.67198529 0.20965421]
wv_lg shape (26, 1)
[[0.38695156]
 [0.38082956]
 [0.38093793]
 [0.38105675]
 [0.3812714 ]
 [0.380997  ]
 [0.3811019 ]
 [0.38101833]
 [0.38123336]
 [0.38092087]
 [0.38094855]
 [0.3811342 ]
 [0.38063755]
 [0.38075416]
 [0.38123716]
 [0.38082928]
 [0.38110244]
 [0.38054193]
 [0.38107946]
 [0.38074017]
 [0.38161788]
 [0.38073405]
 [0.38094691]
 [0.38074224]
 [0.38075824]
 [0.38058303]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.98711421 1.         0.97413765 1.         0.23409164
 0.         0.47873303 0.78968828 0.24430205 1.         1.
 1.         0.82240312 0.27084036 1.         0.39466701 1.
 1.         1.         1.         0.         0.21246015 1.
 0.30181507 0.2144859 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38695156 1.
  1.         0.         0.        ]
 [1.         0.         1.         0.95714675 0.38082956 1.
  0.         0.98711421 1.        ]
 [1.         0.         1.         1.         0.38093793 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38105675 1.
  0.         0.97413765 1.        ]
 [1.         0.         1.         1.         0.3812714  1.
  0.         1.         1.        ]
 [1.         0.         0.63517877 0.76471432 0.380997   1.
  0.         0.23409164 1.        ]
 [1.         0.         0.10784743 0.05817063 0.3811019  1.
  0.         0.         1.        ]
 [1.         0.         0.4857013  0.4013901  0.38101833 1.
  0.         0.47873303 1.        ]
 [1.         0.         0.67334908 0.46982163 0.38123336 1.
  0.         0.78968828 1.        ]
 [1.         0.         0.         0.         0.38092087 1.
  0.         0.24430205 1.        ]
 [1.         0.         1.         1.         0.38094855 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3811342  1.
  0.         1.         1.        ]
 [1.         0.         1.         0.76116536 0.38063755 1.
  0.         1.         1.        ]
 [1.         0.         0.87712985 0.64717243 0.38075416 1.
  0.         0.82240312 1.        ]
 [1.         0.         0.53059054 0.43543085 0.38123716 1.
  0.         0.27084036 1.        ]
 [1.         0.         1.         0.94372209 0.38082928 1.
  0.         1.         1.        ]
 [1.         0.         0.41023852 0.25578549 0.38110244 1.
  0.         0.39466701 1.        ]
 [1.         0.         1.         1.         0.38054193 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38107946 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38074017 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38161788 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38073405 1.
  0.         0.         1.        ]
 [1.         0.         0.57492478 0.51918866 0.38094691 1.
  0.         0.21246015 1.        ]
 [1.         0.         1.         1.         0.38074224 1.
  0.         1.         1.        ]
 [1.         0.         0.74731391 0.67198529 0.38075824 1.
  0.         0.30181507 1.        ]
 [1.         0.         0.23020344 0.20965421 0.38058303 1.
  0.         0.2144859  1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 1 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.93839786 1.         1.         0.         0.
 0.14798751 0.         1.         0.16722129 0.29924974 1.
 1.         0.73503253 0.60585635 1.         1.         0.48524972
 0.77798418 0.         0.31117618 0.         0.         1.
 0.70756949 0.3674886 ]
wv_ed shape (26,)
[0.         1.         1.         1.         0.         0.
 0.57335479 0.         1.         0.21964514 0.55979452 1.
 1.         0.52606406 0.70356539 1.         1.         0.53149946
 1.         0.         0.71231103 0.         0.         1.
 0.98642539 0.66609577]
wv_lg shape (26, 1)
[[0.38730284]
 [0.38157964]
 [0.38224254]
 [0.38215049]
 [0.38196847]
 [0.38184546]
 [0.38184968]
 [0.38191941]
 [0.38157249]
 [0.381722  ]
 [0.38181942]
 [0.38167178]
 [0.38208104]
 [0.3821273 ]
 [0.38218502]
 [0.38178227]
 [0.38158917]
 [0.38206556]
 [0.38158196]
 [0.38160396]
 [0.38144833]
 [0.38203522]
 [0.38174504]
 [0.38171239]
 [0.382331  ]
 [0.3818666 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.00000000e+00 2.72964793e-01 1.00000000e+00 1.00000000e+00
 0.00000000e+00 1.45382548e-01 2.55492642e-04 0.00000000e+00
 1.00000000e+00 3.62843579e-01 6.10081053e-02 1.00000000e+00
 7.44646473e-01 6.52728733e-01 1.00000000e+00 1.00000000e+00
 8.14592058e-01 5.53137143e-01 4.18358350e-01 0.00000000e+00
 0.00000000e+00 0.00000000e+00 2.47327325e-01 1.00000000e+00
 1.00000000e+00 0.00000000e+00]
xy shape: (26, 9)
[[0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  3.87302840e-01 1.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00]
 [1.00000000e+00 0.00000000e+00 9.38397856e-01 1.00000000e+00
  3.81579638e-01 1.00000000e+00 0.00000000e+00 2.72964793e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.82242536e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.82150492e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.81968466e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.81845459e-01 1.00000000e+00 0.00000000e+00 1.45382548e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.47987506e-01 5.73354793e-01
  3.81849675e-01 1.00000000e+00 0.00000000e+00 2.55492642e-04
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.81919405e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.81572488e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.67221286e-01 2.19645143e-01
  3.81722000e-01 1.00000000e+00 0.00000000e+00 3.62843579e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 2.99249744e-01 5.59794520e-01
  3.81819419e-01 1.00000000e+00 0.00000000e+00 6.10081053e-02
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.81671781e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.82081042e-01 1.00000000e+00 0.00000000e+00 7.44646473e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.35032527e-01 5.26064058e-01
  3.82127295e-01 1.00000000e+00 0.00000000e+00 6.52728733e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 6.05856354e-01 7.03565394e-01
  3.82185015e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.81782270e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.81589171e-01 1.00000000e+00 0.00000000e+00 8.14592058e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 4.85249717e-01 5.31499463e-01
  3.82065558e-01 1.00000000e+00 0.00000000e+00 5.53137143e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.77984184e-01 1.00000000e+00
  3.81581958e-01 1.00000000e+00 0.00000000e+00 4.18358350e-01
  1.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.81603962e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 3.11176177e-01 7.12311032e-01
  3.81448328e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.82035220e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.81745040e-01 1.00000000e+00 0.00000000e+00 2.47327325e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.81712391e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.07569491e-01 9.86425389e-01
  3.82330997e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 3.67488598e-01 6.66095773e-01
  3.81866598e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 2 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         0.         0.         0.
 0.         0.         0.4978818  0.         0.         0.92047789
 0.         1.         1.         0.46319313 0.97617737 0.10639908
 0.75876308 0.         0.13001495 0.         0.79923592 1.
 1.         0.        ]
wv_ed shape (26,)
[0.         0.         0.         0.         0.         0.
 0.         0.         0.21452753 0.         0.         1.
 0.         1.         1.         0.5352114  0.4855825  0.26019247
 0.14217289 0.         0.         0.00559699 0.81410699 1.
 1.         0.        ]
wv_lg shape (26, 1)
[[0.38785493]
 [0.38234788]
 [0.38262504]
 [0.38265728]
 [0.38262601]
 [0.38271187]
 [0.38257206]
 [0.3823148 ]
 [0.38265299]
 [0.38234468]
 [0.38218736]
 [0.38244965]
 [0.3825532 ]
 [0.38188974]
 [0.38239484]
 [0.3823437 ]
 [0.38222061]
 [0.38224163]
 [0.38232143]
 [0.3824953 ]
 [0.38241917]
 [0.38243873]
 [0.38238074]
 [0.38216433]
 [0.38253017]
 [0.38206844]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         0.         0.14616161 0.4130971
 0.         0.         0.55466383 0.         0.         0.27247123
 0.         1.         1.         0.55631331 0.87475809 0.
 1.         0.55314518 0.33294029 0.         0.85151002 1.
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38785493 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.38234788 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38262504 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38265728 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38262601 1.
  0.         0.14616161 1.        ]
 [1.         0.         0.         0.         0.38271187 1.
  0.         0.4130971  1.        ]
 [0.         0.         0.         0.         0.38257206 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3823148  1.
  0.         0.         1.        ]
 [1.         0.         0.4978818  0.21452753 0.38265299 1.
  0.         0.55466383 1.        ]
 [1.         0.         0.         0.         0.38234468 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38218736 1.
  0.         0.         1.        ]
 [1.         0.         0.92047789 1.         0.38244965 1.
  0.         0.27247123 1.        ]
 [1.         0.         0.         0.         0.3825532  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38188974 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38239484 1.
  0.         1.         1.        ]
 [1.         0.         0.46319313 0.5352114  0.3823437  1.
  0.         0.55631331 1.        ]
 [1.         0.         0.97617737 0.4855825  0.38222061 1.
  0.         0.87475809 1.        ]
 [1.         0.         0.10639908 0.26019247 0.38224163 1.
  0.         0.         1.        ]
 [1.         0.         0.75876308 0.14217289 0.38232143 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3824953  1.
  0.         0.55314518 1.        ]
 [1.         0.         0.13001495 0.         0.38241917 1.
  0.         0.33294029 1.        ]
 [1.         0.         0.         0.00559699 0.38243873 1.
  0.         0.         1.        ]
 [1.         0.         0.79923592 0.81410699 0.38238074 1.
  0.         0.85151002 1.        ]
 [1.         0.         1.         1.         0.38216433 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38253017 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38206844 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9166666865348816
#####################         POISON         ###############################################

############################################################################################

comm_round: 3 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8

Accuracy per class:
[[8 0]
 [0 0]]
[ 1. nan]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 0.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.39801695 1.         1.         1.         1.
 0.34819071 0.60292216 0.         0.         0.9580689  0.88730445
 1.         0.         0.37950049 0.91723383 0.68857401 0.88261949
 1.         0.         0.04840957 0.60965339 0.         1.
 0.49520854 0.        ]
wv_ed shape (26,)
[0.         0.27931312 0.91339184 1.         1.         1.
 0.49387468 0.62460609 0.         0.         0.42733947 0.92442467
 1.         0.         0.71846075 0.82164813 0.82010999 1.
 0.8065383  0.         0.01638505 0.47718641 0.20564985 0.80720803
 0.48586667 0.        ]
wv_lg shape (26, 1)
[[0.38876048]
 [0.38246802]
 [0.38285483]
 [0.38237119]
 [0.38275993]
 [0.38295937]
 [0.38258281]
 [0.3830813 ]
 [0.3825738 ]
 [0.38261253]
 [0.38273799]
 [0.38225663]
 [0.38264296]
 [0.38240122]
 [0.38249473]
 [0.38254133]
 [0.38252763]
 [0.38219071]
 [0.38301191]
 [0.38285532]
 [0.38340316]
 [0.38267443]
 [0.38262578]
 [0.38289435]
 [0.38263674]
 [0.38236299]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.59543338 1.         1.         1.         1.
 0.05466621 0.577535   0.04088913 0.         0.85364467 0.62923675
 0.34236334 0.         0.         0.3882816  0.61477506 0.62444151
 1.         0.         0.54374183 0.23550725 0.         1.
 0.04956643 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38876048 1.
  1.         0.         0.        ]
 [1.         0.         0.39801695 0.27931312 0.38246802 1.
  0.         0.59543338 1.        ]
 [1.         0.         1.         0.91339184 0.38285483 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38237119 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38275993 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38295937 1.
  0.         1.         1.        ]
 [1.         0.         0.34819071 0.49387468 0.38258281 1.
  0.         0.05466621 1.        ]
 [1.         0.         0.60292216 0.62460609 0.3830813  1.
  0.         0.577535   1.        ]
 [1.         0.         0.         0.         0.3825738  1.
  0.         0.04088913 1.        ]
 [1.         0.         0.         0.         0.38261253 1.
  0.         0.         1.        ]
 [1.         0.         0.9580689  0.42733947 0.38273799 1.
  0.         0.85364467 1.        ]
 [1.         0.         0.88730445 0.92442467 0.38225663 1.
  0.         0.62923675 1.        ]
 [1.         0.         1.         1.         0.38264296 1.
  0.         0.34236334 1.        ]
 [1.         0.         0.         0.         0.38240122 1.
  0.         0.         1.        ]
 [1.         0.         0.37950049 0.71846075 0.38249473 1.
  0.         0.         1.        ]
 [1.         0.         0.91723383 0.82164813 0.38254133 1.
  0.         0.3882816  1.        ]
 [1.         0.         0.68857401 0.82010999 0.38252763 1.
  0.         0.61477506 1.        ]
 [1.         0.         0.88261949 1.         0.38219071 1.
  0.         0.62444151 1.        ]
 [1.         0.         1.         0.8065383  0.38301191 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38285532 1.
  0.         0.         1.        ]
 [1.         0.         0.04840957 0.01638505 0.38340316 1.
  0.         0.54374183 1.        ]
 [1.         0.         0.60965339 0.47718641 0.38267443 1.
  0.         0.23550725 1.        ]
 [1.         0.         0.         0.20564985 0.38262578 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.80720803 0.38289435 1.
  0.         1.         1.        ]
 [1.         0.         0.49520854 0.48586667 0.38263674 1.
  0.         0.04956643 1.        ]
 [0.         0.         0.         0.         0.38236299 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 4 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.41434979 0.20100962 0.93251925 0.         0.74231845
 0.91463419 0.64339479 0.         0.         0.         0.
 0.         1.         1.         0.13854762 0.46958784 0.
 0.         0.         0.         1.         0.         0.81229624
 0.         0.        ]
wv_ed shape (26,)
[0.         0.22764395 0.11546466 0.86962026 0.         0.8204926
 0.88044168 0.46013491 0.         0.         0.         0.
 0.         0.82870986 1.         0.16920685 0.39241708 0.
 0.         0.         0.         1.         0.         0.7239984
 0.         0.        ]
wv_lg shape (26, 1)
[[0.38871124]
 [0.38375276]
 [0.38407393]
 [0.38412647]
 [0.38401185]
 [0.38399621]
 [0.38348607]
 [0.38390784]
 [0.38402601]
 [0.38377859]
 [0.38388369]
 [0.38393541]
 [0.38374822]
 [0.38427861]
 [0.38404236]
 [0.38354625]
 [0.38349419]
 [0.38394497]
 [0.38374109]
 [0.38377209]
 [0.38390639]
 [0.38379669]
 [0.38423157]
 [0.38434061]
 [0.38389007]
 [0.38345221]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.99458506 0.2184894  1.         0.         1.
 0.82157504 0.8773306  0.         0.         0.         0.
 0.         1.         1.         0.53543826 0.82753729 0.28974916
 0.         0.         0.         1.         0.         1.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38871124 1.
  1.         0.         0.        ]
 [1.         0.         0.41434979 0.22764395 0.38375276 1.
  0.         0.99458506 1.        ]
 [1.         0.         0.20100962 0.11546466 0.38407393 1.
  0.         0.2184894  1.        ]
 [1.         0.         0.93251925 0.86962026 0.38412647 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38401185 1.
  0.         0.         1.        ]
 [1.         0.         0.74231845 0.8204926  0.38399621 1.
  0.         1.         1.        ]
 [1.         0.         0.91463419 0.88044168 0.38348607 1.
  0.         0.82157504 1.        ]
 [1.         0.         0.64339479 0.46013491 0.38390784 1.
  0.         0.8773306  1.        ]
 [1.         0.         0.         0.         0.38402601 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38377859 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38388369 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38393541 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38374822 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.82870986 0.38427861 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38404236 1.
  0.         1.         1.        ]
 [1.         0.         0.13854762 0.16920685 0.38354625 1.
  0.         0.53543826 1.        ]
 [1.         0.         0.46958784 0.39241708 0.38349419 1.
  0.         0.82753729 1.        ]
 [1.         0.         0.         0.         0.38394497 1.
  0.         0.28974916 1.        ]
 [1.         0.         0.         0.         0.38374109 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38377209 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.38390639 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38379669 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38423157 1.
  0.         0.         1.        ]
 [1.         0.         0.81229624 0.7239984  0.38434061 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38389007 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38345221 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 5 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8

Accuracy per class:
[[7 0]
 [1 0]]
[1. 0.]
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients

Best Training Poisoning Accuracy:
0.7056376338005066

Best Training Poisoning Accuracy:
0.7073018550872803

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7060536742210388

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7050135135650635

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7077178955078125

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7031412720680237

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7043894529342651

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7035573124885559

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.706885814666748

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.698356568813324

Best Training Poisoning Accuracy:
0.7041813731193542

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7073018550872803

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.7066777348518372

Best Training Poisoning Accuracy:
0.7075098752975464

Best Training Poisoning Accuracy:
0.9938114285469055

Best Training Poisoning Accuracy:
0.9919912815093994

Best Training Poisoning Accuracy:
0.9945394992828369

Best Training Poisoning Accuracy:
0.991081178188324

Best Training Poisoning Accuracy:
0.9881688952445984

Best Training Poisoning Accuracy:
0.9943574666976929

Best Training Poisoning Accuracy:
0.9903531074523926

Best Training Poisoning Accuracy:
0.9941754937171936

Best Training Poisoning Accuracy:
0.994721531867981

Best Training Poisoning Accuracy:
0.9929013252258301

Best Training Poisoning Accuracy:
0.9956315755844116

Best Training Poisoning Accuracy:
0.9938114285469055

Best Training Poisoning Accuracy:
0.9956315755844116

Best Training Poisoning Accuracy:
0.9958136081695557

Best Training Poisoning Accuracy:
0.9854386448860168

Best Training Poisoning Accuracy:
0.9805241823196411

Best Training Poisoning Accuracy:
0.9737895727157593

Best Training Poisoning Accuracy:
0.9830724596977234

Best Training Poisoning Accuracy:
0.987440824508667

Best Training Poisoning Accuracy:
0.9763378500938416

Best Training Poisoning Accuracy:
0.9632326364517212

Best Training Poisoning Accuracy:
0.9539497494697571

Best Training Poisoning Accuracy:
0.9166363477706909
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         0.61327571 0.94378274
 0.90313306 1.         1.         1.         0.75486793 1.
 0.         0.9776923  1.         0.         1.         0.67270405
 0.51425653 1.        ]
wv_ed shape (26,)
[0.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         0.60705333 0.71284522
 0.78720976 1.         1.         1.         0.66976415 1.
 0.06216346 0.81669948 1.         0.         1.         0.61162358
 0.51864561 1.        ]
wv_lg shape (26, 1)
[[0.38888248]
 [0.38504119]
 [0.38485314]
 [0.38468058]
 [0.38537557]
 [0.38481183]
 [0.38465229]
 [0.38457714]
 [0.38450075]
 [0.38464115]
 [0.38459206]
 [0.38473329]
 [0.38463116]
 [0.38430733]
 [0.38463499]
 [0.38512345]
 [0.3847234 ]
 [0.38468455]
 [0.38457273]
 [0.38469367]
 [0.38472128]
 [0.38454797]
 [0.38457673]
 [0.38423587]
 [0.38441291]
 [0.3847199 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         1.         1.         1.         1.
 0.94338933 1.         1.         1.         0.52372803 0.87832709
 0.83941657 0.77139098 1.         1.         0.82021379 1.
 0.         0.79918697 1.         0.         0.91868983 0.88818269
 0.6293854  1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38888248 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.38504119 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38485314 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38468058 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38537557 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38481183 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38465229 1.
  0.         0.94338933 1.        ]
 [1.         0.         1.         1.         0.38457714 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38450075 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38464115 1.
  0.         1.         1.        ]
 [1.         0.         0.61327571 0.60705333 0.38459206 1.
  0.         0.52372803 1.        ]
 [1.         0.         0.94378274 0.71284522 0.38473329 1.
  0.         0.87832709 1.        ]
 [1.         0.         0.90313306 0.78720976 0.38463116 1.
  0.         0.83941657 1.        ]
 [1.         0.         1.         1.         0.38430733 1.
  0.         0.77139098 1.        ]
 [1.         0.         1.         1.         0.38463499 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38512345 1.
  0.         1.         1.        ]
 [1.         0.         0.75486793 0.66976415 0.3847234  1.
  0.         0.82021379 1.        ]
 [1.         0.         1.         1.         0.38468455 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.06216346 0.38457273 1.
  0.         0.         1.        ]
 [1.         0.         0.9776923  0.81669948 0.38469367 1.
  0.         0.79918697 1.        ]
 [1.         0.         1.         1.         0.38472128 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38454797 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38457673 1.
  0.         0.91868983 1.        ]
 [1.         0.         0.67270405 0.61162358 0.38423587 1.
  0.         0.88818269 1.        ]
 [1.         0.         0.51425653 0.51864561 0.38441291 1.
  0.         0.6293854  1.        ]
 [1.         0.         1.         1.         0.3847199  1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 0 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         0.         1.         1.
 1.         1.         1.         1.         1.         1.
 0.         1.         0.         1.         0.1628794  0.
 0.         0.43301716 1.         0.         0.93442197 1.
 1.         1.        ]
wv_ed shape (26,)
[0.         1.         1.         0.         1.         1.
 1.         1.         1.         1.         1.         1.
 0.         1.         0.         1.         0.27000657 0.
 0.         0.69992599 1.         0.         0.83290817 1.
 1.         1.        ]
wv_lg shape (26, 1)
[[0.38925899]
 [0.38495526]
 [0.38526039]
 [0.3857705 ]
 [0.3855851 ]
 [0.3852391 ]
 [0.38534304]
 [0.38503891]
 [0.38507296]
 [0.38485103]
 [0.38520962]
 [0.38504023]
 [0.38514226]
 [0.3847976 ]
 [0.38515903]
 [0.38497725]
 [0.38561408]
 [0.3851871 ]
 [0.38517979]
 [0.38507028]
 [0.38531242]
 [0.38541675]
 [0.38540474]
 [0.38528178]
 [0.38549027]
 [0.38531332]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.98934502 0.45019558 0.         1.         0.2093383
 1.         0.87489562 1.         1.         1.         1.
 0.         0.2999144  0.         0.         0.         0.
 0.         0.         0.54015446 0.         0.52263907 0.92436738
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38925899 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.38495526 1.
  0.         0.98934502 1.        ]
 [1.         0.         1.         1.         0.38526039 1.
  0.         0.45019558 1.        ]
 [1.         0.         0.         0.         0.3857705  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3855851  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3852391  1.
  0.         0.2093383  1.        ]
 [1.         0.         1.         1.         0.38534304 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38503891 1.
  0.         0.87489562 1.        ]
 [1.         0.         1.         1.         0.38507296 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38485103 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38520962 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38504023 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38514226 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3847976  1.
  0.         0.2999144  1.        ]
 [1.         0.         0.         0.         0.38515903 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38497725 1.
  0.         0.         1.        ]
 [1.         0.         0.1628794  0.27000657 0.38561408 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.3851871  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38517979 1.
  0.         0.         1.        ]
 [1.         0.         0.43301716 0.69992599 0.38507028 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38531242 1.
  0.         0.54015446 1.        ]
 [1.         0.         0.         0.         0.38541675 1.
  0.         0.         1.        ]
 [1.         0.         0.93442197 0.83290817 0.38540474 1.
  0.         0.52263907 1.        ]
 [1.         0.         1.         1.         0.38528178 1.
  0.         0.92436738 1.        ]
 [1.         0.         1.         1.         0.38549027 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38531332 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 1 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         1.         1.         0.
 1.         0.         1.         1.         0.73433643 1.
 0.42735771 0.85028119 1.         0.         1.         0.
 1.         1.         0.         0.         0.         1.
 0.39102896 0.        ]
wv_ed shape (26,)
[0.         0.         0.         1.         1.         0.
 1.         0.         1.         1.         0.86663274 1.
 0.26929515 0.73982885 1.         0.         1.         0.
 1.         1.         0.         0.         0.         1.
 0.40696204 0.        ]
wv_lg shape (26, 1)
[[0.38898445]
 [0.3861586 ]
 [0.385961  ]
 [0.38645938]
 [0.38641053]
 [0.38642659]
 [0.38641455]
 [0.38624047]
 [0.38618039]
 [0.38589359]
 [0.38598152]
 [0.38648111]
 [0.38603517]
 [0.38686566]
 [0.38654315]
 [0.38605609]
 [0.38623089]
 [0.38629537]
 [0.38670352]
 [0.3863703 ]
 [0.38610277]
 [0.38600201]
 [0.38632061]
 [0.38620352]
 [0.38634861]
 [0.38621731]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         1.         1.         0.
 1.         0.         1.         1.         1.         0.92310132
 0.67996774 1.         1.         0.         1.         0.
 1.         0.8651121  0.         0.         0.         1.
 0.21422695 0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38898445 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.3861586  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.385961   1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38645938 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38641053 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38642659 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38641455 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38624047 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38618039 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38589359 1.
  0.         1.         1.        ]
 [1.         0.         0.73433643 0.86663274 0.38598152 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38648111 1.
  0.         0.92310132 1.        ]
 [1.         0.         0.42735771 0.26929515 0.38603517 1.
  0.         0.67996774 1.        ]
 [1.         0.         0.85028119 0.73982885 0.38686566 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38654315 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38605609 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38623089 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38629537 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38670352 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3863703  1.
  0.         0.8651121  1.        ]
 [1.         0.         0.         0.         0.38610277 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38600201 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38632061 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38620352 1.
  0.         1.         1.        ]
 [1.         0.         0.39102896 0.40696204 0.38634861 1.
  0.         0.21422695 1.        ]
 [1.         0.         0.         0.         0.38621731 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 2 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 0.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.32526789 1.         1.         1.         0.
 1.         0.         0.77838674 0.82418661 0.70779766 0.73581552
 0.25700542 0.68897991 0.92640108 0.34832044 1.         0.85089033
 1.         0.         1.         1.         0.         0.5887117
 0.         0.        ]
wv_ed shape (26,)
[0.         0.47009911 0.84765172 1.         1.         0.
 1.         0.         0.7096656  0.65732214 0.77067549 0.4436515
 0.3444767  0.39500993 0.91797433 0.18984895 1.         0.75030882
 1.         0.         1.         1.         0.06651924 0.65190885
 0.         0.        ]
wv_lg shape (26, 1)
[[0.38982052]
 [0.38610596]
 [0.38659858]
 [0.3862532 ]
 [0.38636682]
 [0.38654771]
 [0.38607852]
 [0.38594856]
 [0.3862467 ]
 [0.38619477]
 [0.38612159]
 [0.38600629]
 [0.38619163]
 [0.38640996]
 [0.38571999]
 [0.38634795]
 [0.38626146]
 [0.38605055]
 [0.38635858]
 [0.38629479]
 [0.38615442]
 [0.38609913]
 [0.38619544]
 [0.38609044]
 [0.38607979]
 [0.38605336]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.82056455 1.         0.
 0.96290392 0.         1.         1.         0.9307489  0.93436911
 0.42174681 0.81208986 0.64082293 0.48961875 0.95761793 0.84471051
 1.         0.         0.70271133 1.         0.         1.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.38982052 1.
  1.         0.         0.        ]
 [1.         0.         0.32526789 0.47009911 0.38610596 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.84765172 0.38659858 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3862532  1.
  0.         0.82056455 1.        ]
 [1.         0.         1.         1.         0.38636682 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38654771 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38607852 1.
  0.         0.96290392 1.        ]
 [1.         0.         0.         0.         0.38594856 1.
  0.         0.         1.        ]
 [1.         0.         0.77838674 0.7096656  0.3862467  1.
  0.         1.         1.        ]
 [1.         0.         0.82418661 0.65732214 0.38619477 1.
  0.         1.         1.        ]
 [1.         0.         0.70779766 0.77067549 0.38612159 1.
  0.         0.9307489  1.        ]
 [1.         0.         0.73581552 0.4436515  0.38600629 1.
  0.         0.93436911 1.        ]
 [1.         0.         0.25700542 0.3444767  0.38619163 1.
  0.         0.42174681 1.        ]
 [1.         0.         0.68897991 0.39500993 0.38640996 1.
  0.         0.81208986 1.        ]
 [1.         0.         0.92640108 0.91797433 0.38571999 1.
  0.         0.64082293 1.        ]
 [1.         0.         0.34832044 0.18984895 0.38634795 1.
  0.         0.48961875 1.        ]
 [1.         0.         1.         1.         0.38626146 1.
  0.         0.95761793 1.        ]
 [1.         0.         0.85089033 0.75030882 0.38605055 1.
  0.         0.84471051 1.        ]
 [1.         0.         1.         1.         0.38635858 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38629479 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38615442 1.
  0.         0.70271133 1.        ]
 [1.         0.         1.         1.         0.38609913 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.06651924 0.38619544 1.
  0.         0.         1.        ]
 [1.         0.         0.5887117  0.65190885 0.38609044 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38607979 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.38605336 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 3 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         1.         1.         0.
 0.6223159  0.         1.         0.         0.3425098  0.
 0.35848262 0.23403489 1.         1.         1.         1.
 1.         0.         1.         0.18315176 0.         1.
 1.         0.        ]
wv_ed shape (26,)
[0.         0.         1.         1.         1.         0.
 0.27578759 0.         1.         0.         0.25837917 0.
 0.34456578 0.29374423 1.         1.         1.         0.84846438
 0.71170328 0.         1.         0.24644784 0.         0.9973173
 1.         0.        ]
wv_lg shape (26, 1)
[[0.39042761]
 [0.38635188]
 [0.38643151]
 [0.3860595 ]
 [0.38632771]
 [0.38641591]
 [0.3863051 ]
 [0.38661985]
 [0.38620591]
 [0.38658377]
 [0.38643387]
 [0.38652565]
 [0.38637622]
 [0.38623526]
 [0.38637611]
 [0.38621948]
 [0.38602094]
 [0.38653134]
 [0.38607571]
 [0.38622981]
 [0.38624527]
 [0.38671051]
 [0.38635336]
 [0.38638166]
 [0.38603187]
 [0.38631458]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         1.         1.         0.
 1.         0.         1.         0.         0.89481268 0.
 0.57406069 0.         1.         1.         1.         0.83183445
 0.63548614 0.         1.         0.         0.         0.60988414
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39042761 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.38635188 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38643151 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3860595  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38632771 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38641591 1.
  0.         0.         1.        ]
 [1.         0.         0.6223159  0.27578759 0.3863051  1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38661985 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38620591 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38658377 1.
  0.         0.         1.        ]
 [1.         0.         0.3425098  0.25837917 0.38643387 1.
  0.         0.89481268 1.        ]
 [1.         0.         0.         0.         0.38652565 1.
  0.         0.         1.        ]
 [1.         0.         0.35848262 0.34456578 0.38637622 1.
  0.         0.57406069 1.        ]
 [1.         0.         0.23403489 0.29374423 0.38623526 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38637611 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38621948 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38602094 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.84846438 0.38653134 1.
  0.         0.83183445 1.        ]
 [1.         0.         1.         0.71170328 0.38607571 1.
  0.         0.63548614 1.        ]
 [1.         0.         0.         0.         0.38622981 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38624527 1.
  0.         1.         1.        ]
 [1.         0.         0.18315176 0.24644784 0.38671051 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38635336 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.9973173  0.38638166 1.
  0.         0.60988414 1.        ]
 [1.         0.         1.         1.         0.38603187 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38631458 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 4 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.         0.73664913 0.         0.
 1.         0.         0.77824223 1.         1.         0.33025807
 0.         0.         0.74172492 0.55832789 1.         1.
 1.         1.         0.         0.01478933 0.45305376 0.
 1.         0.06971467]
wv_ed shape (26,)
[0.         1.         0.         0.8314042  0.         0.
 1.         0.         1.         1.         1.         0.22711727
 0.         0.         1.         0.52333475 1.         1.
 1.         1.         0.         0.26605881 0.32447496 0.
 1.         0.30728299]
wv_lg shape (26, 1)
[[0.39023376]
 [0.38772503]
 [0.38706221]
 [0.38722152]
 [0.38728216]
 [0.38779232]
 [0.38696276]
 [0.38688307]
 [0.38718445]
 [0.38720462]
 [0.38710858]
 [0.38723303]
 [0.38713438]
 [0.38724386]
 [0.38758679]
 [0.3870967 ]
 [0.38725279]
 [0.38691552]
 [0.38730188]
 [0.38745454]
 [0.38739932]
 [0.38683794]
 [0.38735419]
 [0.38725717]
 [0.38727678]
 [0.3875586 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         0.33893347 0.         0.
 1.         0.         0.33326609 1.         1.         1.
 0.         0.         0.         0.51627686 1.         0.8785865
 1.         1.         0.         0.         0.72845046 0.
 0.80778631 0.2365915 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39023376 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.38772503 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38706221 1.
  0.         0.         1.        ]
 [1.         0.         0.73664913 0.8314042  0.38722152 1.
  0.         0.33893347 1.        ]
 [1.         0.         0.         0.         0.38728216 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38779232 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38696276 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38688307 1.
  0.         0.         1.        ]
 [1.         0.         0.77824223 1.         0.38718445 1.
  0.         0.33326609 1.        ]
 [1.         0.         1.         1.         0.38720462 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38710858 1.
  0.         1.         1.        ]
 [1.         0.         0.33025807 0.22711727 0.38723303 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38713438 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38724386 1.
  0.         0.         1.        ]
 [1.         0.         0.74172492 1.         0.38758679 1.
  0.         0.         1.        ]
 [1.         0.         0.55832789 0.52333475 0.3870967  1.
  0.         0.51627686 1.        ]
 [1.         0.         1.         1.         0.38725279 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38691552 1.
  0.         0.8785865  1.        ]
 [1.         0.         1.         1.         0.38730188 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38745454 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38739932 1.
  0.         0.         1.        ]
 [1.         0.         0.01478933 0.26605881 0.38683794 1.
  0.         0.         1.        ]
 [1.         0.         0.45305376 0.32447496 0.38735419 1.
  0.         0.72845046 1.        ]
 [1.         0.         0.         0.         0.38725717 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38727678 1.
  0.         0.80778631 1.        ]
 [1.         0.         0.06971467 0.30728299 0.3875586  1.
  0.         0.2365915  1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 5 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         1.         1.         1.
 0.74646146 0.         1.         0.         1.         0.5334039
 1.         0.20491051 1.         0.29823493 0.32249386 1.
 0.         1.         0.3516878  0.83264622 0.38633407 1.
 0.32613895 0.29656066]
wv_ed shape (26,)
[0.         0.         1.         1.         1.         1.
 0.83099637 0.         1.         0.         1.         0.59006032
 1.         0.0234822  1.         0.40456413 0.59932088 1.
 0.         1.         0.28490572 0.87906908 0.33133393 0.93060596
 0.22937935 0.40432474]
wv_lg shape (26, 1)
[[0.39052861]
 [0.38743004]
 [0.38747544]
 [0.38740504]
 [0.38757599]
 [0.38727992]
 [0.38769929]
 [0.38749635]
 [0.38804559]
 [0.38765911]
 [0.3876906 ]
 [0.38771242]
 [0.38754945]
 [0.38749463]
 [0.38758045]
 [0.38747602]
 [0.38765086]
 [0.38795636]
 [0.38748645]
 [0.3876976 ]
 [0.38761484]
 [0.38722151]
 [0.38752992]
 [0.38775064]
 [0.38763246]
 [0.3877069 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.83693365 0.96713963 1.
 1.         0.         1.         0.         1.         0.54112036
 1.         0.         1.         0.87463308 0.18059373 1.
 0.         1.         0.55977652 0.93026638 0.21910472 1.
 1.         0.61648825]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39052861 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.38743004 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38747544 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38740504 1.
  0.         0.83693365 1.        ]
 [1.         0.         1.         1.         0.38757599 1.
  0.         0.96713963 1.        ]
 [1.         0.         1.         1.         0.38727992 1.
  0.         1.         1.        ]
 [1.         0.         0.74646146 0.83099637 0.38769929 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38749635 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38804559 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38765911 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3876906  1.
  0.         1.         1.        ]
 [1.         0.         0.5334039  0.59006032 0.38771242 1.
  0.         0.54112036 1.        ]
 [1.         0.         1.         1.         0.38754945 1.
  0.         1.         1.        ]
 [1.         0.         0.20491051 0.0234822  0.38749463 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38758045 1.
  0.         1.         1.        ]
 [1.         0.         0.29823493 0.40456413 0.38747602 1.
  0.         0.87463308 1.        ]
 [1.         0.         0.32249386 0.59932088 0.38765086 1.
  0.         0.18059373 1.        ]
 [1.         0.         1.         1.         0.38795636 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38748645 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3876976  1.
  0.         1.         1.        ]
 [1.         0.         0.3516878  0.28490572 0.38761484 1.
  0.         0.55977652 1.        ]
 [1.         0.         0.83264622 0.87906908 0.38722151 1.
  0.         0.93026638 1.        ]
 [1.         0.         0.38633407 0.33133393 0.38752992 1.
  0.         0.21910472 1.        ]
 [1.         0.         1.         0.93060596 0.38775064 1.
  0.         1.         1.        ]
 [1.         0.         0.32613895 0.22937935 0.38763246 1.
  0.         1.         1.        ]
 [1.         0.         0.29656066 0.40432474 0.3877069  1.
  0.         0.61648825 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 6 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.06093397 0.80699066 0.7496012  0.73766212 0.44801107
 0.05868719 0.         0.89210893 0.         1.         1.
 0.60240359 0.87405215 0.48533415 0.92683431 0.         1.
 1.         0.         1.         1.         1.         1.
 1.         0.        ]
wv_ed shape (26,)
[0.         0.         0.840642   0.79113082 0.75645456 0.4108671
 0.0350905  0.         0.7739597  0.         1.         1.
 0.6655836  0.86274243 0.32858206 1.         0.         1.
 1.         0.         1.         1.         1.         0.89510299
 1.         0.        ]
wv_lg shape (26, 1)
[[0.39109993]
 [0.38791108]
 [0.38785162]
 [0.3875405 ]
 [0.38787101]
 [0.38759601]
 [0.38810828]
 [0.38743869]
 [0.38772041]
 [0.38790343]
 [0.38786281]
 [0.38773328]
 [0.38768168]
 [0.38772256]
 [0.38777475]
 [0.3875653 ]
 [0.38766425]
 [0.38776831]
 [0.38762243]
 [0.38780072]
 [0.38796496]
 [0.38808727]
 [0.38753702]
 [0.38743549]
 [0.38757378]
 [0.38759796]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.69960315 0.490701   0.         1.         0.91882568
 0.38863258 0.         1.         0.         1.         0.86526129
 0.74806171 0.93192323 0.60775655 0.85622522 0.         1.
 1.         0.         1.         1.         1.         1.
 1.         0.08424046]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39109993 1.
  1.         0.         0.        ]
 [1.         0.         0.06093397 0.         0.38791108 1.
  0.         0.69960315 1.        ]
 [1.         0.         0.80699066 0.840642   0.38785162 1.
  0.         0.490701   1.        ]
 [1.         0.         0.7496012  0.79113082 0.3875405  1.
  0.         0.         1.        ]
 [1.         0.         0.73766212 0.75645456 0.38787101 1.
  0.         1.         1.        ]
 [1.         0.         0.44801107 0.4108671  0.38759601 1.
  0.         0.91882568 1.        ]
 [1.         0.         0.05868719 0.0350905  0.38810828 1.
  0.         0.38863258 1.        ]
 [1.         0.         0.         0.         0.38743869 1.
  0.         0.         1.        ]
 [1.         0.         0.89210893 0.7739597  0.38772041 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38790343 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38786281 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38773328 1.
  0.         0.86526129 1.        ]
 [1.         0.         0.60240359 0.6655836  0.38768168 1.
  0.         0.74806171 1.        ]
 [1.         0.         0.87405215 0.86274243 0.38772256 1.
  0.         0.93192323 1.        ]
 [1.         0.         0.48533415 0.32858206 0.38777475 1.
  0.         0.60775655 1.        ]
 [1.         0.         0.92683431 1.         0.3875653  1.
  0.         0.85622522 1.        ]
 [1.         0.         0.         0.         0.38766425 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38776831 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38762243 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38780072 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38796496 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38808727 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38753702 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.89510299 0.38743549 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38757378 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38759796 1.
  0.         0.08424046 1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 7 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         0.         0.22230091 0.
 0.11783471 0.04408549 0.         1.         1.         0.01418389
 1.         0.         0.         0.6936366  0.         0.78357019
 0.60058222 0.30383422 0.41341928 0.         0.03189149 0.13844413
 1.         0.        ]
wv_ed shape (26,)
[0.         0.         0.         0.         0.         0.
 0.07212849 0.08767373 0.         1.         1.         0.
 1.         0.         0.         1.         0.         0.53943665
 0.54248216 0.01028844 0.40426186 0.         0.21139221 0.25029839
 1.         0.        ]
wv_lg shape (26, 1)
[[0.39182373]
 [0.38775837]
 [0.3877396 ]
 [0.38753085]
 [0.38785573]
 [0.38736605]
 [0.38813305]
 [0.38749752]
 [0.38768761]
 [0.38759842]
 [0.38769818]
 [0.38782596]
 [0.38757386]
 [0.38723732]
 [0.38750703]
 [0.38746226]
 [0.38768579]
 [0.38736549]
 [0.38788608]
 [0.38804466]
 [0.38785591]
 [0.38764902]
 [0.38774564]
 [0.38756921]
 [0.38760172]
 [0.38770976]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         0.         0.45417098 0.
 0.56116182 0.         0.         1.         1.         0.
 1.         0.         0.         0.         0.         0.68688167
 0.30232522 0.43008108 0.         0.         0.         0.
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39182373 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.38775837 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.3877396  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38753085 1.
  0.         0.         1.        ]
 [1.         0.         0.22230091 0.         0.38785573 1.
  0.         0.45417098 1.        ]
 [1.         0.         0.         0.         0.38736605 1.
  0.         0.         1.        ]
 [1.         0.         0.11783471 0.07212849 0.38813305 1.
  0.         0.56116182 1.        ]
 [1.         0.         0.04408549 0.08767373 0.38749752 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38768761 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38759842 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38769818 1.
  0.         1.         1.        ]
 [1.         0.         0.01418389 0.         0.38782596 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38757386 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38723732 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38750703 1.
  0.         0.         1.        ]
 [1.         0.         0.6936366  1.         0.38746226 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38768579 1.
  0.         0.         1.        ]
 [1.         0.         0.78357019 0.53943665 0.38736549 1.
  0.         0.68688167 1.        ]
 [1.         0.         0.60058222 0.54248216 0.38788608 1.
  0.         0.30232522 1.        ]
 [1.         0.         0.30383422 0.01028844 0.38804466 1.
  0.         0.43008108 1.        ]
 [1.         0.         0.41341928 0.40426186 0.38785591 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38764902 1.
  0.         0.         1.        ]
 [1.         0.         0.03189149 0.21139221 0.38774564 1.
  0.         0.         1.        ]
 [1.         0.         0.13844413 0.25029839 0.38756921 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38760172 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38770976 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 8 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         1.         0.         1.         1.
 0.63713417 1.         0.         1.         1.         1.
 1.         0.363338   0.         1.         1.         0.60511415
 0.32643817 0.         0.         1.         1.         1.
 1.         0.89804314]
wv_ed shape (26,)
[0.         0.         1.         0.         1.         1.
 0.65493524 1.         0.         1.         1.         1.
 0.93969344 0.51869561 0.         1.         1.         0.45089441
 0.11783317 0.         0.         1.         1.         1.
 1.         0.30027841]
wv_lg shape (26, 1)
[[0.39133496]
 [0.38892107]
 [0.38879367]
 [0.38867763]
 [0.38897796]
 [0.38878553]
 [0.3890383 ]
 [0.3888876 ]
 [0.38910775]
 [0.38861891]
 [0.38874255]
 [0.3885492 ]
 [0.38880286]
 [0.38870393]
 [0.38916998]
 [0.38843135]
 [0.38830258]
 [0.38862148]
 [0.38884573]
 [0.38882436]
 [0.38879929]
 [0.38882819]
 [0.38889363]
 [0.38870134]
 [0.38854214]
 [0.38864913]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.         1.         0.69786982
 0.8301188  1.         0.         1.         1.         1.
 1.         0.24745403 0.52239018 1.         1.         1.
 0.90408223 0.         0.         1.         1.         1.
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39133496 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.38892107 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38879367 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38867763 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38897796 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38878553 1.
  0.         0.69786982 1.        ]
 [1.         0.         0.63713417 0.65493524 0.3890383  1.
  0.         0.8301188  1.        ]
 [1.         0.         1.         1.         0.3888876  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38910775 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38861891 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38874255 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3885492  1.
  0.         1.         1.        ]
 [1.         0.         1.         0.93969344 0.38880286 1.
  0.         1.         1.        ]
 [1.         0.         0.363338   0.51869561 0.38870393 1.
  0.         0.24745403 1.        ]
 [1.         0.         0.         0.         0.38916998 1.
  0.         0.52239018 1.        ]
 [1.         0.         1.         1.         0.38843135 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38830258 1.
  0.         1.         1.        ]
 [1.         0.         0.60511415 0.45089441 0.38862148 1.
  0.         1.         1.        ]
 [1.         0.         0.32643817 0.11783317 0.38884573 1.
  0.         0.90408223 1.        ]
 [1.         0.         0.         0.         0.38882436 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.38879929 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38882819 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38889363 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38870134 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38854214 1.
  0.         1.         1.        ]
 [1.         0.         0.89804314 0.30027841 0.38864913 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 9 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         0.55456739 1.         0.10529822
 0.78594057 1.         1.         0.         0.39754206 0.
 1.         0.         0.         0.         1.         0.
 1.         0.         0.         0.         1.         0.
 1.         1.        ]
wv_ed shape (26,)
[0.         1.         1.         0.82160106 0.7870895  0.24924717
 1.         1.         1.         0.         0.41796198 0.
 1.         0.         0.         0.         1.         0.
 1.         0.         0.         0.         1.         0.
 1.         1.        ]
wv_lg shape (26, 1)
[[0.39177727]
 [0.38885127]
 [0.38892099]
 [0.38873496]
 [0.38900644]
 [0.38876269]
 [0.38893093]
 [0.3886092 ]
 [0.38858809]
 [0.38887627]
 [0.38885446]
 [0.38880246]
 [0.38894834]
 [0.3888764 ]
 [0.38893132]
 [0.38865784]
 [0.38893261]
 [0.38855901]
 [0.38909516]
 [0.38889471]
 [0.38884144]
 [0.38874864]
 [0.38870885]
 [0.38883123]
 [0.38908961]
 [0.38875553]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.90474101 1.         0.16437855 1.         0.
 0.44855522 0.87158515 0.64963394 0.         0.40082142 0.
 1.         0.         0.         0.         0.65204093 0.
 0.89827287 0.         0.         0.         1.         0.
 1.         0.49090936]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39177727 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.38885127 1.
  0.         0.90474101 1.        ]
 [1.         0.         1.         1.         0.38892099 1.
  0.         1.         1.        ]
 [1.         0.         0.55456739 0.82160106 0.38873496 1.
  0.         0.16437855 1.        ]
 [1.         0.         1.         0.7870895  0.38900644 1.
  0.         1.         1.        ]
 [1.         0.         0.10529822 0.24924717 0.38876269 1.
  0.         0.         1.        ]
 [1.         0.         0.78594057 1.         0.38893093 1.
  0.         0.44855522 1.        ]
 [1.         0.         1.         1.         0.3886092  1.
  0.         0.87158515 1.        ]
 [1.         0.         1.         1.         0.38858809 1.
  0.         0.64963394 1.        ]
 [0.         0.         0.         0.         0.38887627 1.
  0.         0.         1.        ]
 [1.         0.         0.39754206 0.41796198 0.38885446 1.
  0.         0.40082142 1.        ]
 [1.         0.         0.         0.         0.38880246 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38894834 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3888764  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38893132 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38865784 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38893261 1.
  0.         0.65204093 1.        ]
 [1.         0.         0.         0.         0.38855901 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38909516 1.
  0.         0.89827287 1.        ]
 [1.         0.         0.         0.         0.38889471 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38884144 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38874864 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38870885 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38883123 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38908961 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38875553 1.
  0.         0.49090936 1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 10 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.1868928  0.         1.         0.         0.68233464
 0.         0.         1.         1.         1.         0.31612269
 1.         1.         0.98046542 1.         1.         1.
 1.         1.         1.         1.         1.         0.27487312
 0.         0.40483886]
wv_ed shape (26,)
[0.         0.6142832  0.         1.         0.         0.69305653
 0.         0.         1.         1.         1.         0.14842311
 1.         1.         0.90939087 1.         1.         0.85416651
 1.         1.         1.         1.         1.         0.3591726
 0.         0.43893796]
wv_lg shape (26, 1)
[[0.39199488]
 [0.389202  ]
 [0.38911723]
 [0.38893381]
 [0.38922776]
 [0.38935804]
 [0.38934554]
 [0.38908947]
 [0.38900857]
 [0.38914199]
 [0.38897381]
 [0.3892392 ]
 [0.38905708]
 [0.38921867]
 [0.38897999]
 [0.38928517]
 [0.38897557]
 [0.3891559 ]
 [0.38906832]
 [0.3890776 ]
 [0.3892021 ]
 [0.38944059]
 [0.3892076 ]
 [0.38922941]
 [0.38915835]
 [0.38898834]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         1.         0.         0.10059886
 0.0591302  0.         0.64326429 1.         1.         0.
 0.31491977 0.54642052 0.         1.         1.         0.29594559
 0.81727994 1.         0.72435787 1.         0.38188501 0.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39199488 1.
  1.         0.         0.        ]
 [1.         0.         0.1868928  0.6142832  0.389202   1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38911723 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38893381 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38922776 1.
  0.         0.         1.        ]
 [1.         0.         0.68233464 0.69305653 0.38935804 1.
  0.         0.10059886 1.        ]
 [1.         0.         0.         0.         0.38934554 1.
  0.         0.0591302  1.        ]
 [1.         0.         0.         0.         0.38908947 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38900857 1.
  0.         0.64326429 1.        ]
 [1.         0.         1.         1.         0.38914199 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38897381 1.
  0.         1.         1.        ]
 [1.         0.         0.31612269 0.14842311 0.3892392  1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38905708 1.
  0.         0.31491977 1.        ]
 [1.         0.         1.         1.         0.38921867 1.
  0.         0.54642052 1.        ]
 [1.         0.         0.98046542 0.90939087 0.38897999 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38928517 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38897557 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.85416651 0.3891559  1.
  0.         0.29594559 1.        ]
 [1.         0.         1.         1.         0.38906832 1.
  0.         0.81727994 1.        ]
 [1.         0.         1.         1.         0.3890776  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3892021  1.
  0.         0.72435787 1.        ]
 [1.         0.         1.         1.         0.38944059 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3892076  1.
  0.         0.38188501 1.        ]
 [1.         0.         0.27487312 0.3591726  0.38922941 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38915835 1.
  0.         0.         1.        ]
 [1.         0.         0.40483886 0.43893796 0.38898834 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 11 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.36850674 0.15206579 1.         0.97480114 1.
 0.48916731 1.         1.         1.         1.         0.
 1.         0.88005454 1.         0.         1.         0.95718594
 0.         1.         0.         0.92815664 1.         0.54330063
 0.         1.        ]
wv_ed shape (26,)
[0.         0.38883078 0.         1.         0.97018629 0.86194246
 0.59032733 1.         1.         1.         1.         0.
 1.         0.8402962  1.         0.         0.94030933 0.75616301
 0.         1.         0.         0.70423564 1.         0.31585012
 0.         0.89117282]
wv_lg shape (26, 1)
[[0.39211943]
 [0.3892595 ]
 [0.38957399]
 [0.38934276]
 [0.38976897]
 [0.38961166]
 [0.38938858]
 [0.38964266]
 [0.38934653]
 [0.38962817]
 [0.3896843 ]
 [0.38961304]
 [0.38944403]
 [0.38983461]
 [0.38962842]
 [0.38956619]
 [0.3897487 ]
 [0.3893097 ]
 [0.38962302]
 [0.38960908]
 [0.38963812]
 [0.38958387]
 [0.38949658]
 [0.38970273]
 [0.38968205]
 [0.38986781]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.08446685 0.66755128 1.         1.         1.
 0.7232328  1.         1.         1.         1.         0.
 1.         1.         1.         0.         1.         1.
 0.         0.80329096 0.         1.         1.         1.
 0.19554169 1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39211943 1.
  1.         0.         0.        ]
 [1.         0.         0.36850674 0.38883078 0.3892595  1.
  0.         0.08446685 1.        ]
 [1.         0.         0.15206579 0.         0.38957399 1.
  0.         0.66755128 1.        ]
 [1.         0.         1.         1.         0.38934276 1.
  0.         1.         1.        ]
 [1.         0.         0.97480114 0.97018629 0.38976897 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.86194246 0.38961166 1.
  0.         1.         1.        ]
 [1.         0.         0.48916731 0.59032733 0.38938858 1.
  0.         0.7232328  1.        ]
 [1.         0.         1.         1.         0.38964266 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38934653 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38962817 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3896843  1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38961304 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38944403 1.
  0.         1.         1.        ]
 [1.         0.         0.88005454 0.8402962  0.38983461 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38962842 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38956619 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.94030933 0.3897487  1.
  0.         1.         1.        ]
 [1.         0.         0.95718594 0.75616301 0.3893097  1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.38962302 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38960908 1.
  0.         0.80329096 1.        ]
 [1.         0.         0.         0.         0.38963812 1.
  0.         0.         1.        ]
 [1.         0.         0.92815664 0.70423564 0.38958387 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38949658 1.
  0.         1.         1.        ]
 [1.         0.         0.54330063 0.31585012 0.38970273 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38968205 1.
  0.         0.19554169 1.        ]
 [1.         0.         1.         0.89117282 0.38986781 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 12 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.93757712 0.         0.         0.         0.
 1.         1.         0.79138908 0.91959243 0.         0.
 0.         0.3411855  0.67767502 1.         0.78648721 0.
 0.         1.         0.78580866 0.27085102 1.         1.
 1.         0.0311828 ]
wv_ed shape (26,)
[0.         0.63610517 0.         0.         0.         0.
 1.         1.         0.66253216 0.78983456 0.         0.
 0.12460396 0.         0.88437742 1.         0.81155003 0.
 0.         1.         0.62494402 0.         1.         0.84732253
 1.         0.        ]
wv_lg shape (26, 1)
[[0.3925159 ]
 [0.38968271]
 [0.38957914]
 [0.38971004]
 [0.38960949]
 [0.38958022]
 [0.38982027]
 [0.38942492]
 [0.38961422]
 [0.38991702]
 [0.38953679]
 [0.38963112]
 [0.3895413 ]
 [0.38998003]
 [0.389537  ]
 [0.38959759]
 [0.38983588]
 [0.3898649 ]
 [0.38966765]
 [0.38964581]
 [0.38964735]
 [0.38961929]
 [0.38986795]
 [0.38962242]
 [0.38986438]
 [0.38970517]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         0.26982244 0.         0.
 0.99495755 1.         1.         1.         0.         0.20278147
 0.24381904 1.         0.2454148  1.         0.87536945 0.07016913
 0.31871639 1.         1.         0.55365479 1.         1.
 1.         0.1534047 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3925159  1.
  1.         0.         0.        ]
 [1.         0.         0.93757712 0.63610517 0.38968271 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38957914 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38971004 1.
  0.         0.26982244 1.        ]
 [0.         0.         0.         0.         0.38960949 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38958022 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38982027 1.
  0.         0.99495755 1.        ]
 [1.         0.         1.         1.         0.38942492 1.
  0.         1.         1.        ]
 [1.         0.         0.79138908 0.66253216 0.38961422 1.
  0.         1.         1.        ]
 [1.         0.         0.91959243 0.78983456 0.38991702 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.38953679 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.38963112 1.
  0.         0.20278147 1.        ]
 [1.         0.         0.         0.12460396 0.3895413  1.
  0.         0.24381904 1.        ]
 [1.         0.         0.3411855  0.         0.38998003 1.
  0.         1.         1.        ]
 [1.         0.         0.67767502 0.88437742 0.389537   1.
  0.         0.2454148  1.        ]
 [1.         0.         1.         1.         0.38959759 1.
  0.         1.         1.        ]
 [1.         0.         0.78648721 0.81155003 0.38983588 1.
  0.         0.87536945 1.        ]
 [1.         0.         0.         0.         0.3898649  1.
  0.         0.07016913 1.        ]
 [1.         0.         0.         0.         0.38966765 1.
  0.         0.31871639 1.        ]
 [1.         0.         1.         1.         0.38964581 1.
  0.         1.         1.        ]
 [1.         0.         0.78580866 0.62494402 0.38964735 1.
  0.         1.         1.        ]
 [1.         0.         0.27085102 0.         0.38961929 1.
  0.         0.55365479 1.        ]
 [1.         0.         1.         1.         0.38986795 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.84732253 0.38962242 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.38986438 1.
  0.         1.         1.        ]
 [1.         0.         0.0311828  0.         0.38970517 1.
  0.         0.1534047  1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 13 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.72432474 1.         0.53097099 1.         0.
 0.         1.         0.         1.         0.14724527 1.
 0.         1.         0.         1.         0.         1.
 0.60211869 0.42159614 1.         0.         0.82289868 0.
 0.         0.        ]
wv_ed shape (26,)
[0.         0.44530316 1.         0.57702727 1.         0.01910305
 0.         1.         0.         1.         0.15841805 1.
 0.00585527 1.         0.31881811 1.         0.         1.
 0.8531931  0.74850476 1.         0.         0.90259122 0.
 0.22839211 0.08754214]
wv_lg shape (26, 1)
[[0.39264612]
 [0.39008705]
 [0.39032066]
 [0.38999522]
 [0.39002411]
 [0.39009422]
 [0.38995002]
 [0.38972787]
 [0.38992591]
 [0.39031455]
 [0.39009115]
 [0.39005075]
 [0.38968528]
 [0.39000282]
 [0.39013902]
 [0.38996673]
 [0.39006174]
 [0.39005303]
 [0.38985976]
 [0.38981328]
 [0.38978988]
 [0.39018576]
 [0.3897498 ]
 [0.39010176]
 [0.38999593]
 [0.39005846]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.75346169 0.23559342 1.         0.15565351
 0.         0.81543007 0.         1.         0.27649535 1.
 0.25068354 1.         0.         1.         0.         1.
 0.02925299 0.         1.         0.         0.60144599 0.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39264612 1.
  1.         0.         0.        ]
 [1.         0.         0.72432474 0.44530316 0.39008705 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39032066 1.
  0.         0.75346169 1.        ]
 [1.         0.         0.53097099 0.57702727 0.38999522 1.
  0.         0.23559342 1.        ]
 [1.         0.         1.         1.         0.39002411 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.01910305 0.39009422 1.
  0.         0.15565351 1.        ]
 [1.         0.         0.         0.         0.38995002 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38972787 1.
  0.         0.81543007 1.        ]
 [1.         0.         0.         0.         0.38992591 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39031455 1.
  0.         1.         1.        ]
 [1.         0.         0.14724527 0.15841805 0.39009115 1.
  0.         0.27649535 1.        ]
 [1.         0.         1.         1.         0.39005075 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.00585527 0.38968528 1.
  0.         0.25068354 1.        ]
 [1.         0.         1.         1.         0.39000282 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.31881811 0.39013902 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38996673 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39006174 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39005303 1.
  0.         1.         1.        ]
 [1.         0.         0.60211869 0.8531931  0.38985976 1.
  0.         0.02925299 1.        ]
 [1.         0.         0.42159614 0.74850476 0.38981328 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.38978988 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.39018576 1.
  0.         0.         1.        ]
 [1.         0.         0.82289868 0.90259122 0.3897498  1.
  0.         0.60144599 1.        ]
 [1.         0.         0.         0.         0.39010176 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.22839211 0.38999593 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.08754214 0.39005846 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 14 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         0.         0.         0.
 0.69465377 0.         0.         0.         1.         0.03957785
 0.44723145 0.         0.         0.         0.         1.
 1.         0.26078437 1.         1.         0.         0.
 0.         0.86288527]
wv_ed shape (26,)
[0.         0.         0.         0.         0.         0.
 0.45953185 0.         0.         0.10301956 1.         0.
 0.54977472 0.11345617 0.10620568 0.         0.         1.
 1.         0.43607411 1.         1.         0.         0.
 0.         1.        ]
wv_lg shape (26, 1)
[[0.39247592]
 [0.39057832]
 [0.39065541]
 [0.39062381]
 [0.39092203]
 [0.39032589]
 [0.39065941]
 [0.39048648]
 [0.3910636 ]
 [0.3904249 ]
 [0.39047801]
 [0.39061835]
 [0.39044398]
 [0.3906918 ]
 [0.39094012]
 [0.39087662]
 [0.39059052]
 [0.39073933]
 [0.39087587]
 [0.39047192]
 [0.3906941 ]
 [0.39063827]
 [0.39069509]
 [0.39044205]
 [0.39046412]
 [0.3907247 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         0.         0.         0.
 0.92691968 0.         0.         0.14803227 1.         0.79069744
 0.01049849 0.         0.11526449 0.         0.         1.
 1.         0.32902244 1.         1.         0.         0.
 0.         0.7428258 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39247592 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.39057832 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39065541 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39062381 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39092203 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39032589 1.
  0.         0.         1.        ]
 [1.         0.         0.69465377 0.45953185 0.39065941 1.
  0.         0.92691968 1.        ]
 [1.         0.         0.         0.         0.39048648 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3910636  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.10301956 0.3904249  1.
  0.         0.14803227 1.        ]
 [1.         0.         1.         1.         0.39047801 1.
  0.         1.         1.        ]
 [1.         0.         0.03957785 0.         0.39061835 1.
  0.         0.79069744 1.        ]
 [1.         0.         0.44723145 0.54977472 0.39044398 1.
  0.         0.01049849 1.        ]
 [1.         0.         0.         0.11345617 0.3906918  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.10620568 0.39094012 1.
  0.         0.11526449 1.        ]
 [1.         0.         0.         0.         0.39087662 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.39059052 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39073933 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39087587 1.
  0.         1.         1.        ]
 [1.         0.         0.26078437 0.43607411 0.39047192 1.
  0.         0.32902244 1.        ]
 [1.         0.         1.         1.         0.3906941  1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39063827 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39069509 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39044205 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39046412 1.
  0.         0.         1.        ]
 [1.         0.         0.86288527 1.         0.3907247  1.
  0.         0.7428258  1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 15 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.2294526  0.2054902  0.         0.
 0.         1.         0.06229545 0.34599182 1.         0.
 0.83482317 0.         0.22263101 0.25992418 1.         0.
 1.         0.62914547 1.         0.         0.         0.6647802
 0.         0.17851034]
wv_ed shape (26,)
[0.         1.         0.19707241 0.06879385 0.         0.
 0.         1.         0.         0.3436116  1.         0.
 0.82916877 0.         0.51243955 0.30753078 1.         0.
 1.         0.46727188 1.         0.         0.         0.63978236
 0.         0.        ]
wv_lg shape (26, 1)
[[0.39283841]
 [0.39054196]
 [0.39067387]
 [0.39060813]
 [0.39065006]
 [0.39081187]
 [0.39048615]
 [0.39083378]
 [0.39075846]
 [0.39089616]
 [0.39084565]
 [0.39075435]
 [0.39036674]
 [0.39063843]
 [0.39066162]
 [0.39062421]
 [0.39105903]
 [0.39073036]
 [0.39034145]
 [0.39058366]
 [0.39052948]
 [0.39091384]
 [0.39081619]
 [0.39085809]
 [0.3906342 ]
 [0.39071937]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.56052843 0.66854803 0.         0.02169184
 0.         1.         0.70685378 0.         1.         0.
 0.6582639  0.         0.         0.75956    1.         0.
 1.         1.         1.         0.         0.         0.56711318
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39283841 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.39054196 1.
  0.         1.         1.        ]
 [1.         0.         0.2294526  0.19707241 0.39067387 1.
  0.         0.56052843 1.        ]
 [1.         0.         0.2054902  0.06879385 0.39060813 1.
  0.         0.66854803 1.        ]
 [1.         0.         0.         0.         0.39065006 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39081187 1.
  0.         0.02169184 1.        ]
 [1.         0.         0.         0.         0.39048615 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39083378 1.
  0.         1.         1.        ]
 [1.         0.         0.06229545 0.         0.39075846 1.
  0.         0.70685378 1.        ]
 [1.         0.         0.34599182 0.3436116  0.39089616 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39084565 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39075435 1.
  0.         0.         1.        ]
 [1.         0.         0.83482317 0.82916877 0.39036674 1.
  0.         0.6582639  1.        ]
 [1.         0.         0.         0.         0.39063843 1.
  0.         0.         1.        ]
 [1.         0.         0.22263101 0.51243955 0.39066162 1.
  0.         0.         1.        ]
 [1.         0.         0.25992418 0.30753078 0.39062421 1.
  0.         0.75956    1.        ]
 [1.         0.         1.         1.         0.39105903 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39073036 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39034145 1.
  0.         1.         1.        ]
 [1.         0.         0.62914547 0.46727188 0.39058366 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39052948 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39091384 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.39081619 1.
  0.         0.         1.        ]
 [1.         0.         0.6647802  0.63978236 0.39085809 1.
  0.         0.56711318 1.        ]
 [1.         0.         0.         0.         0.3906342  1.
  0.         0.         1.        ]
 [1.         0.         0.17851034 0.         0.39071937 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 16 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         1.         1.         0.         1.
 1.         0.4858289  1.         1.         0.         0.8298702
 0.         0.75151559 0.36868905 0.96829541 0.40075051 1.
 1.         0.         0.75336478 0.50681003 1.         0.51653146
 0.         1.        ]
wv_ed shape (26,)
[0.         1.         1.         1.         0.         1.
 1.         0.         1.         1.         0.         0.80225703
 0.         0.73523525 0.34978762 1.         0.32489489 1.
 1.         0.         0.91642787 0.57717667 1.         0.7536853
 0.         1.        ]
wv_lg shape (26, 1)
[[0.39292373]
 [0.39110422]
 [0.39085303]
 [0.39109471]
 [0.39092045]
 [0.39110691]
 [0.39100902]
 [0.39090257]
 [0.39103136]
 [0.39075212]
 [0.39120984]
 [0.39094215]
 [0.39103943]
 [0.3908989 ]
 [0.39113913]
 [0.3908848 ]
 [0.39088019]
 [0.39117821]
 [0.390759  ]
 [0.39076904]
 [0.39109109]
 [0.39100518]
 [0.39108822]
 [0.39107434]
 [0.39094482]
 [0.3907354 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         1.         1.         0.         1.
 1.         1.         1.         1.         0.         1.
 0.35385185 1.         0.90316647 1.         0.9325216  1.
 1.         0.73382648 1.         1.         1.         0.89576094
 0.43019549 1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39292373 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.39110422 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39085303 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39109471 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.39092045 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39110691 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39100902 1.
  0.         1.         1.        ]
 [1.         0.         0.4858289  0.         0.39090257 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39103136 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39075212 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39120984 1.
  0.         0.         1.        ]
 [1.         0.         0.8298702  0.80225703 0.39094215 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39103943 1.
  0.         0.35385185 1.        ]
 [1.         0.         0.75151559 0.73523525 0.3908989  1.
  0.         1.         1.        ]
 [1.         0.         0.36868905 0.34978762 0.39113913 1.
  0.         0.90316647 1.        ]
 [1.         0.         0.96829541 1.         0.3908848  1.
  0.         1.         1.        ]
 [1.         0.         0.40075051 0.32489489 0.39088019 1.
  0.         0.9325216  1.        ]
 [1.         0.         1.         1.         0.39117821 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.390759   1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39076904 1.
  0.         0.73382648 1.        ]
 [1.         0.         0.75336478 0.91642787 0.39109109 1.
  0.         1.         1.        ]
 [1.         0.         0.50681003 0.57717667 0.39100518 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39108822 1.
  0.         1.         1.        ]
 [1.         0.         0.51653146 0.7536853  0.39107434 1.
  0.         0.89576094 1.        ]
 [1.         0.         0.         0.         0.39094482 1.
  0.         0.43019549 1.        ]
 [1.         0.         1.         1.         0.3907354  1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 17 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.21026272 0.         0.         1.         1.
 0.         0.         1.         0.24039042 1.         1.
 0.41471089 0.23223513 0.         0.         1.         0.
 0.68588031 0.25688169 1.         0.63199245 0.03373935 0.5846423
 0.83445712 1.        ]
wv_ed shape (26,)
[0.         0.         0.         0.         1.         1.
 0.         0.         1.         0.09937565 1.         1.
 0.81574642 0.14506263 0.         0.         1.         0.01867343
 0.74771125 0.38964197 1.         0.94744357 0.1194743  0.56913351
 1.         1.        ]
wv_lg shape (26, 1)
[[0.39326442]
 [0.39086722]
 [0.39113665]
 [0.39096856]
 [0.39079008]
 [0.39094504]
 [0.39110757]
 [0.39100049]
 [0.39093148]
 [0.39080405]
 [0.39099825]
 [0.39117412]
 [0.39089902]
 [0.39103647]
 [0.39089003]
 [0.39092333]
 [0.39105109]
 [0.39092602]
 [0.39110793]
 [0.39114011]
 [0.3912161 ]
 [0.39103877]
 [0.39122651]
 [0.39109133]
 [0.39113694]
 [0.39118093]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.18877178 0.         0.         1.         0.81693664
 0.         0.         1.         0.50866857 1.         1.
 0.         0.30440278 0.         0.         0.69571532 0.
 0.73820528 0.0058659  1.         0.         0.11848283 0.74630152
 0.5138715  0.77924898]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39326442 1.
  1.         0.         0.        ]
 [1.         0.         0.21026272 0.         0.39086722 1.
  0.         0.18877178 1.        ]
 [0.         0.         0.         0.         0.39113665 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39096856 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39079008 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39094504 1.
  0.         0.81693664 1.        ]
 [1.         0.         0.         0.         0.39110757 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39100049 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39093148 1.
  0.         1.         1.        ]
 [1.         0.         0.24039042 0.09937565 0.39080405 1.
  0.         0.50866857 1.        ]
 [1.         0.         1.         1.         0.39099825 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39117412 1.
  0.         1.         1.        ]
 [1.         0.         0.41471089 0.81574642 0.39089902 1.
  0.         0.         1.        ]
 [1.         0.         0.23223513 0.14506263 0.39103647 1.
  0.         0.30440278 1.        ]
 [1.         0.         0.         0.         0.39089003 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39092333 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39105109 1.
  0.         0.69571532 1.        ]
 [1.         0.         0.         0.01867343 0.39092602 1.
  0.         0.         1.        ]
 [1.         0.         0.68588031 0.74771125 0.39110793 1.
  0.         0.73820528 1.        ]
 [1.         0.         0.25688169 0.38964197 0.39114011 1.
  0.         0.0058659  1.        ]
 [1.         0.         1.         1.         0.3912161  1.
  0.         1.         1.        ]
 [1.         0.         0.63199245 0.94744357 0.39103877 1.
  0.         0.         1.        ]
 [1.         0.         0.03373935 0.1194743  0.39122651 1.
  0.         0.11848283 1.        ]
 [1.         0.         0.5846423  0.56913351 0.39109133 1.
  0.         0.74630152 1.        ]
 [1.         0.         0.83445712 1.         0.39113694 1.
  0.         0.5138715  1.        ]
 [1.         0.         1.         1.         0.39118093 1.
  0.         0.77924898 1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 18 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 0.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.22270481 0.         0.53565821 0.
 1.         0.         0.73325054 0.         0.         0.76231564
 1.         1.         0.87842013 0.         1.         0.
 0.23251853 0.02788879 1.         0.20128351 1.         1.
 0.         0.        ]
wv_ed shape (26,)
[0.         1.         0.20181842 0.21002127 0.50608416 0.
 1.         0.         0.50898109 0.         0.         0.67296509
 0.94717326 1.         1.         0.         1.         0.
 0.25460974 0.         1.         0.19491725 0.99398885 1.
 0.         0.        ]
wv_lg shape (26, 1)
[[0.39355665]
 [0.39126346]
 [0.39124898]
 [0.39105233]
 [0.3910858 ]
 [0.39092154]
 [0.39114487]
 [0.39098014]
 [0.39132507]
 [0.39101066]
 [0.39089011]
 [0.39106091]
 [0.39098229]
 [0.39098274]
 [0.39100017]
 [0.39110488]
 [0.39122397]
 [0.39093781]
 [0.39113333]
 [0.39131157]
 [0.39106666]
 [0.39106005]
 [0.39096468]
 [0.39093239]
 [0.39110565]
 [0.39108078]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.27697601 0.         0.72847695 0.
 1.         0.         1.         0.         0.         1.
 1.         1.         1.         0.         1.         0.37730916
 0.22728095 0.60866552 1.         0.54841458 1.         1.
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39355665 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.39126346 1.
  0.         1.         1.        ]
 [1.         0.         0.22270481 0.20181842 0.39124898 1.
  0.         0.27697601 1.        ]
 [1.         0.         0.         0.21002127 0.39105233 1.
  0.         0.         1.        ]
 [1.         0.         0.53565821 0.50608416 0.3910858  1.
  0.         0.72847695 1.        ]
 [1.         0.         0.         0.         0.39092154 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39114487 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39098014 1.
  0.         0.         1.        ]
 [1.         0.         0.73325054 0.50898109 0.39132507 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39101066 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39089011 1.
  0.         0.         1.        ]
 [1.         0.         0.76231564 0.67296509 0.39106091 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.94717326 0.39098229 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39098274 1.
  0.         1.         1.        ]
 [1.         0.         0.87842013 1.         0.39100017 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39110488 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39122397 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39093781 1.
  0.         0.37730916 1.        ]
 [1.         0.         0.23251853 0.25460974 0.39113333 1.
  0.         0.22728095 1.        ]
 [1.         0.         0.02788879 0.         0.39131157 1.
  0.         0.60866552 1.        ]
 [1.         0.         1.         1.         0.39106666 1.
  0.         1.         1.        ]
 [1.         0.         0.20128351 0.19491725 0.39106005 1.
  0.         0.54841458 1.        ]
 [1.         0.         1.         0.99398885 0.39096468 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39093239 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39110565 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.39108078 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 19 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 0. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.79977609 0.59400888 0.         1.         1.
 0.         0.24580197 0.73656558 0.28829884 0.50683139 0.71786027
 1.         0.01933599 0.         0.72285475 0.         0.
 0.         0.         0.         0.36682263 0.08985061 1.
 0.         0.        ]
wv_ed shape (26,)
[0.         0.23225322 0.20043163 0.         0.8445642  1.
 0.         0.         0.57741414 0.01840666 0.         0.42358948
 1.         0.         0.         0.73546916 0.         0.
 0.         0.         0.         0.         0.         1.
 0.         0.        ]
wv_lg shape (26, 1)
[[0.3935232 ]
 [0.39163333]
 [0.3913116 ]
 [0.39156849]
 [0.39139075]
 [0.39167425]
 [0.39147542]
 [0.39194094]
 [0.391507  ]
 [0.39141393]
 [0.39135055]
 [0.39178723]
 [0.39132496]
 [0.39157572]
 [0.39163459]
 [0.39171145]
 [0.39137446]
 [0.39141008]
 [0.39134723]
 [0.39143998]
 [0.39134735]
 [0.39136368]
 [0.3914164 ]
 [0.39138887]
 [0.39163847]
 [0.39136472]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         0.         0.71099045 0.91094572
 0.         0.48886968 0.         0.12597262 0.         0.68136142
 1.         0.         0.         0.06346457 0.         0.
 0.         0.         0.         0.         0.04708315 0.32034837
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.3935232  1.
  1.         0.         0.        ]
 [1.         0.         0.79977609 0.23225322 0.39163333 1.
  0.         1.         1.        ]
 [1.         0.         0.59400888 0.20043163 0.3913116  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39156849 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.8445642  0.39139075 1.
  0.         0.71099045 1.        ]
 [1.         0.         1.         1.         0.39167425 1.
  0.         0.91094572 1.        ]
 [1.         0.         0.         0.         0.39147542 1.
  0.         0.         1.        ]
 [1.         0.         0.24580197 0.         0.39194094 1.
  0.         0.48886968 1.        ]
 [1.         0.         0.73656558 0.57741414 0.391507   1.
  0.         0.         1.        ]
 [1.         0.         0.28829884 0.01840666 0.39141393 1.
  0.         0.12597262 1.        ]
 [1.         0.         0.50683139 0.         0.39135055 1.
  0.         0.         1.        ]
 [1.         0.         0.71786027 0.42358948 0.39178723 1.
  0.         0.68136142 1.        ]
 [1.         0.         1.         1.         0.39132496 1.
  0.         1.         1.        ]
 [1.         0.         0.01933599 0.         0.39157572 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39163459 1.
  0.         0.         1.        ]
 [1.         0.         0.72285475 0.73546916 0.39171145 1.
  0.         0.06346457 1.        ]
 [1.         0.         0.         0.         0.39137446 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39141008 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39134723 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39143998 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39134735 1.
  0.         0.         1.        ]
 [1.         0.         0.36682263 0.         0.39136368 1.
  0.         0.         1.        ]
 [1.         0.         0.08985061 0.         0.3914164  1.
  0.         0.04708315 1.        ]
 [1.         0.         1.         1.         0.39138887 1.
  0.         0.32034837 1.        ]
 [0.         0.         0.         0.         0.39163847 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39136472 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 20 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.23390872 1.         0.         0.         0.48444607
 0.24013252 0.82234396 0.33329293 0.49134222 0.         0.
 0.43945014 0.         1.         0.         0.         1.
 0.         0.         0.8860421  0.53174697 0.         0.
 1.         0.05980981]
wv_ed shape (26,)
[0.         0.63817024 1.         0.         0.         0.37883352
 0.32808259 1.         0.39988543 0.46985886 0.         0.
 0.12941229 0.         1.         0.         0.0696267  0.86455465
 0.         0.         0.81337053 0.30977049 0.         0.
 1.         0.19910023]
wv_lg shape (26, 1)
[[0.39358342]
 [0.39156306]
 [0.39176682]
 [0.39173128]
 [0.39186302]
 [0.39176833]
 [0.39188867]
 [0.39177711]
 [0.39162484]
 [0.39178689]
 [0.39168028]
 [0.39188686]
 [0.39182597]
 [0.39184227]
 [0.39151668]
 [0.39204149]
 [0.39193234]
 [0.39179525]
 [0.3918843 ]
 [0.39178152]
 [0.39163996]
 [0.39183156]
 [0.39200604]
 [0.3920411 ]
 [0.39183865]
 [0.39171801]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         1.         0.         0.         0.88147414
 0.         0.24794166 0.37835951 0.19125989 0.         0.
 1.         0.         1.         0.         0.         1.
 0.         0.         1.         0.66035252 0.         0.76299606
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39358342 1.
  1.         0.         0.        ]
 [1.         0.         0.23390872 0.63817024 0.39156306 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39176682 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39173128 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39186302 1.
  0.         0.         1.        ]
 [1.         0.         0.48444607 0.37883352 0.39176833 1.
  0.         0.88147414 1.        ]
 [1.         0.         0.24013252 0.32808259 0.39188867 1.
  0.         0.         1.        ]
 [1.         0.         0.82234396 1.         0.39177711 1.
  0.         0.24794166 1.        ]
 [1.         0.         0.33329293 0.39988543 0.39162484 1.
  0.         0.37835951 1.        ]
 [1.         0.         0.49134222 0.46985886 0.39178689 1.
  0.         0.19125989 1.        ]
 [1.         0.         0.         0.         0.39168028 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39188686 1.
  0.         0.         1.        ]
 [1.         0.         0.43945014 0.12941229 0.39182597 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39184227 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39151668 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39204149 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.0696267  0.39193234 1.
  0.         0.         1.        ]
 [1.         0.         1.         0.86455465 0.39179525 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.3918843  1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.39178152 1.
  0.         0.         1.        ]
 [1.         0.         0.8860421  0.81337053 0.39163996 1.
  0.         1.         1.        ]
 [1.         0.         0.53174697 0.30977049 0.39183156 1.
  0.         0.66035252 1.        ]
 [1.         0.         0.         0.         0.39200604 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3920411  1.
  0.         0.76299606 1.        ]
 [1.         0.         1.         1.         0.39183865 1.
  0.         1.         1.        ]
 [1.         0.         0.05980981 0.19910023 0.39171801 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 21 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.         0.         0.64756535 1.
 0.         0.54934213 0.         0.50363652 0.         1.
 0.31754257 0.         1.         1.         0.16207221 0.
 1.         0.30770371 1.         0.38569604 0.81377005 0.50759799
 0.         0.60572727]
wv_ed shape (26,)
[0.         0.         0.         0.         0.50417028 1.
 0.         0.37380635 0.         0.60230733 0.         1.
 0.24485449 0.         1.         1.         0.         0.
 1.         0.15732579 1.         0.26170383 0.63452045 0.33353483
 0.         0.84141141]
wv_lg shape (26, 1)
[[0.39404506]
 [0.39171004]
 [0.39159053]
 [0.39155458]
 [0.39159238]
 [0.39162187]
 [0.39162452]
 [0.39154079]
 [0.39155596]
 [0.39198062]
 [0.39158892]
 [0.39171725]
 [0.39159013]
 [0.39152729]
 [0.39192721]
 [0.39180926]
 [0.39175833]
 [0.39165048]
 [0.39171355]
 [0.39156772]
 [0.39174873]
 [0.39165564]
 [0.39166976]
 [0.39165141]
 [0.39176457]
 [0.3915885 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.         0.         0.61634831 1.
 0.         0.57829028 0.         0.40144963 0.         1.
 0.30241838 0.         1.         1.         0.14090793 0.
 1.         0.60750527 1.         0.73226481 1.         0.51690999
 0.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39404506 1.
  1.         0.         0.        ]
 [0.         0.         0.         0.         0.39171004 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39159053 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39155458 1.
  0.         0.         1.        ]
 [1.         0.         0.64756535 0.50417028 0.39159238 1.
  0.         0.61634831 1.        ]
 [1.         0.         1.         1.         0.39162187 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39162452 1.
  0.         0.         1.        ]
 [1.         0.         0.54934213 0.37380635 0.39154079 1.
  0.         0.57829028 1.        ]
 [1.         0.         0.         0.         0.39155596 1.
  0.         0.         1.        ]
 [1.         0.         0.50363652 0.60230733 0.39198062 1.
  0.         0.40144963 1.        ]
 [1.         0.         0.         0.         0.39158892 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39171725 1.
  0.         1.         1.        ]
 [1.         0.         0.31754257 0.24485449 0.39159013 1.
  0.         0.30241838 1.        ]
 [1.         0.         0.         0.         0.39152729 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39192721 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39180926 1.
  0.         1.         1.        ]
 [1.         0.         0.16207221 0.         0.39175833 1.
  0.         0.14090793 1.        ]
 [1.         0.         0.         0.         0.39165048 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39171355 1.
  0.         1.         1.        ]
 [1.         0.         0.30770371 0.15732579 0.39156772 1.
  0.         0.60750527 1.        ]
 [1.         0.         1.         1.         0.39174873 1.
  0.         1.         1.        ]
 [1.         0.         0.38569604 0.26170383 0.39165564 1.
  0.         0.73226481 1.        ]
 [1.         0.         0.81377005 0.63452045 0.39166976 1.
  0.         1.         1.        ]
 [1.         0.         0.50759799 0.33353483 0.39165141 1.
  0.         0.51690999 1.        ]
 [1.         0.         0.         0.         0.39176457 1.
  0.         0.         1.        ]
 [1.         0.         0.60572727 0.84141141 0.3915885  1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 22 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.87067796 0.34197193 0.24935576 1.         0.80512239
 1.         1.         0.         1.         0.93421849 1.
 1.         0.9881637  0.         1.         0.33860428 1.
 0.10743496 0.75417533 0.05470901 1.         1.         1.
 1.         0.44485458]
wv_ed shape (26,)
[0.         0.80172075 0.21325498 0.06504451 1.         0.73970465
 1.         1.         0.         1.         0.56093772 1.
 1.         1.         0.         1.         0.09861656 0.98529668
 0.06735948 0.62509123 0.06364358 1.         1.         1.
 1.         0.40869839]
wv_lg shape (26, 1)
[[0.39423126]
 [0.39178384]
 [0.39190389]
 [0.39177124]
 [0.39160241]
 [0.39194124]
 [0.39165074]
 [0.39193242]
 [0.39197251]
 [0.39225783]
 [0.39175213]
 [0.39155851]
 [0.39171594]
 [0.39186267]
 [0.39193554]
 [0.39168546]
 [0.39186337]
 [0.39167472]
 [0.39176359]
 [0.39164996]
 [0.39180893]
 [0.39209173]
 [0.39176645]
 [0.39183069]
 [0.3918926 ]
 [0.39188947]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.58651712 0.35834072 0.69379226 1.         0.90017611
 1.         1.         0.         1.         1.         0.89913398
 0.80685644 0.1292698  0.26236746 1.         0.24127907 1.
 0.         0.70893379 0.29039967 1.         1.         1.
 0.76452824 0.28167598]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39423126 1.
  1.         0.         0.        ]
 [1.         0.         0.87067796 0.80172075 0.39178384 1.
  0.         0.58651712 1.        ]
 [1.         0.         0.34197193 0.21325498 0.39190389 1.
  0.         0.35834072 1.        ]
 [1.         0.         0.24935576 0.06504451 0.39177124 1.
  0.         0.69379226 1.        ]
 [1.         0.         1.         1.         0.39160241 1.
  0.         1.         1.        ]
 [1.         0.         0.80512239 0.73970465 0.39194124 1.
  0.         0.90017611 1.        ]
 [1.         0.         1.         1.         0.39165074 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39193242 1.
  0.         1.         1.        ]
 [0.         0.         0.         0.         0.39197251 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39225783 1.
  0.         1.         1.        ]
 [1.         0.         0.93421849 0.56093772 0.39175213 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39155851 1.
  0.         0.89913398 1.        ]
 [1.         0.         1.         1.         0.39171594 1.
  0.         0.80685644 1.        ]
 [1.         0.         0.9881637  1.         0.39186267 1.
  0.         0.1292698  1.        ]
 [1.         0.         0.         0.         0.39193554 1.
  0.         0.26236746 1.        ]
 [1.         0.         1.         1.         0.39168546 1.
  0.         1.         1.        ]
 [1.         0.         0.33860428 0.09861656 0.39186337 1.
  0.         0.24127907 1.        ]
 [1.         0.         1.         0.98529668 0.39167472 1.
  0.         1.         1.        ]
 [1.         0.         0.10743496 0.06735948 0.39176359 1.
  0.         0.         1.        ]
 [1.         0.         0.75417533 0.62509123 0.39164996 1.
  0.         0.70893379 1.        ]
 [1.         0.         0.05470901 0.06364358 0.39180893 1.
  0.         0.29039967 1.        ]
 [1.         0.         1.         1.         0.39209173 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39176645 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39183069 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.3918926  1.
  0.         0.76452824 1.        ]
 [1.         0.         0.44485458 0.40869839 0.39188947 1.
  0.         0.28167598 1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 23 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.         0.10569182 0.         1.         0.
 1.         0.         0.         0.         1.         1.
 0.         0.65417782 0.         0.43930599 0.27923251 0.20688637
 0.         0.66273058 1.         0.34488948 0.         1.
 1.         0.        ]
wv_ed shape (26,)
[0.         0.         0.04810787 0.         1.         0.
 1.         0.         0.17476596 0.         1.         0.99526731
 0.         0.57469746 0.         0.27235801 0.06139977 0.33322719
 0.         0.6889022  0.87380396 0.26360971 0.         1.
 1.         0.        ]
wv_lg shape (26, 1)
[[0.39404081]
 [0.39246705]
 [0.39233183]
 [0.39248424]
 [0.39240007]
 [0.39261788]
 [0.39210898]
 [0.39241611]
 [0.3923606 ]
 [0.39238295]
 [0.3924877 ]
 [0.39231818]
 [0.39232322]
 [0.39239555]
 [0.39236409]
 [0.39245183]
 [0.39237516]
 [0.39238662]
 [0.39238262]
 [0.39247674]
 [0.39241824]
 [0.39232751]
 [0.39203501]
 [0.39241274]
 [0.39219774]
 [0.39215846]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.         0.9009825  0.         1.         1.
 1.         0.         0.18393284 0.         1.         1.
 0.         1.         0.         1.         0.82957087 0.48454655
 0.         1.         1.         1.         0.         1.
 1.         0.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39404081 1.
  1.         0.         0.        ]
 [1.         0.         0.         0.         0.39246705 1.
  0.         0.         1.        ]
 [1.         0.         0.10569182 0.04810787 0.39233183 1.
  0.         0.9009825  1.        ]
 [1.         0.         0.         0.         0.39248424 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39240007 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39261788 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39210898 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39241611 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.17476596 0.3923606  1.
  0.         0.18393284 1.        ]
 [0.         0.         0.         0.         0.39238295 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3924877  1.
  0.         1.         1.        ]
 [1.         0.         1.         0.99526731 0.39231818 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39232322 1.
  0.         0.         1.        ]
 [1.         0.         0.65417782 0.57469746 0.39239555 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39236409 1.
  0.         0.         1.        ]
 [1.         0.         0.43930599 0.27235801 0.39245183 1.
  0.         1.         1.        ]
 [1.         0.         0.27923251 0.06139977 0.39237516 1.
  0.         0.82957087 1.        ]
 [1.         0.         0.20688637 0.33322719 0.39238662 1.
  0.         0.48454655 1.        ]
 [1.         0.         0.         0.         0.39238262 1.
  0.         0.         1.        ]
 [1.         0.         0.66273058 0.6889022  0.39247674 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.87380396 0.39241824 1.
  0.         1.         1.        ]
 [1.         0.         0.34488948 0.26360971 0.39232751 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39203501 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39241274 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39219774 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39215846 1.
  0.         0.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 24 | global_test_acc: 87.500% | global_f1: 0.9333333333333333 | global_precision: 0.875
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         1
           1       0.88      1.00      0.93         7

    accuracy                           0.88         8
   macro avg       0.44      0.50      0.47         8
weighted avg       0.77      0.88      0.82         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.17434134 0.96051594 0.         0.         0.
 0.         0.         0.98036873 0.         0.         0.88009128
 1.         0.         0.         0.         1.         0.
 0.24848171 0.         0.37957961 0.         0.         0.
 0.73088717 0.03271693]
wv_ed shape (26,)
[0.         0.10989384 0.69976251 0.         0.         0.
 0.         0.         0.69825274 0.         0.         1.
 0.87847929 0.         0.         0.         1.         0.
 0.46409603 0.         0.05211296 0.         0.         0.
 0.36957318 0.        ]
wv_lg shape (26, 1)
[[0.39389142]
 [0.39308927]
 [0.39286364]
 [0.39271224]
 [0.39299104]
 [0.39276678]
 [0.39267764]
 [0.3926897 ]
 [0.39256963]
 [0.39254321]
 [0.39302498]
 [0.39280559]
 [0.39277668]
 [0.39272931]
 [0.3928621 ]
 [0.39289056]
 [0.39293224]
 [0.39291448]
 [0.39289675]
 [0.3927889 ]
 [0.39285071]
 [0.39306408]
 [0.39288301]
 [0.39298742]
 [0.39275251]
 [0.39286468]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.82811813 1.         0.         0.         0.66284461
 0.         0.69537087 1.         0.         0.         0.43198867
 1.         0.         0.         0.         1.         0.13336007
 0.54825926 0.05414865 1.         0.         0.         0.53855052
 1.         1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39389142 1.
  1.         0.         0.        ]
 [1.         0.         0.17434134 0.10989384 0.39308927 1.
  0.         0.82811813 1.        ]
 [1.         0.         0.96051594 0.69976251 0.39286364 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39271224 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.39299104 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39276678 1.
  0.         0.66284461 1.        ]
 [1.         0.         0.         0.         0.39267764 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3926897  1.
  0.         0.69537087 1.        ]
 [1.         0.         0.98036873 0.69825274 0.39256963 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39254321 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39302498 1.
  0.         0.         1.        ]
 [1.         0.         0.88009128 1.         0.39280559 1.
  0.         0.43198867 1.        ]
 [1.         0.         1.         0.87847929 0.39277668 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39272931 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3928621  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39289056 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39293224 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39291448 1.
  0.         0.13336007 1.        ]
 [1.         0.         0.24848171 0.46409603 0.39289675 1.
  0.         0.54825926 1.        ]
 [1.         0.         0.         0.         0.3927889  1.
  0.         0.05414865 1.        ]
 [1.         0.         0.37957961 0.05211296 0.39285071 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39306408 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39288301 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39298742 1.
  0.         0.53855052 1.        ]
 [1.         0.         0.73088717 0.36957318 0.39275251 1.
  0.         1.         1.        ]
 [1.         0.         0.03271693 0.         0.39286468 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
1.0
#####################         POISON         ###############################################

############################################################################################

comm_round: 25 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.10480996 0.24531589 0.3909274  0.         1.
 0.95090866 1.         1.         1.         0.40961586 0.34896066
 0.         0.20692399 0.81049109 0.         1.         1.
 1.         0.12696719 0.         1.         1.         1.
 0.57586855 0.35941285]
wv_ed shape (26,)
[0.         0.         0.20677122 0.48460754 0.         1.
 0.67589527 1.         1.         1.         0.25882047 0.
 0.         0.         0.76097837 0.         1.         1.
 0.90536136 0.08342241 0.         1.         1.         0.94423217
 0.47273379 0.09158407]
wv_lg shape (26, 1)
[[0.39407546]
 [0.39290343]
 [0.39286621]
 [0.39297053]
 [0.39300251]
 [0.39288006]
 [0.39304257]
 [0.39273675]
 [0.39298399]
 [0.39296572]
 [0.3928232 ]
 [0.39272445]
 [0.39286242]
 [0.392969  ]
 [0.39300931]
 [0.39286484]
 [0.39294757]
 [0.39310449]
 [0.39293598]
 [0.39315873]
 [0.39282879]
 [0.39308079]
 [0.39293299]
 [0.3929088 ]
 [0.39304978]
 [0.39295203]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.15902047 0.22801802 0.27440555 0.48927995 1.
 1.         1.         1.         1.         0.65814066 0.71089151
 0.         0.91111384 1.         0.         1.         0.94525032
 1.         0.57599464 0.         1.         1.         1.
 0.42901705 1.        ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39407546 1.
  1.         0.         0.        ]
 [1.         0.         0.10480996 0.         0.39290343 1.
  0.         0.15902047 1.        ]
 [1.         0.         0.24531589 0.20677122 0.39286621 1.
  0.         0.22801802 1.        ]
 [1.         0.         0.3909274  0.48460754 0.39297053 1.
  0.         0.27440555 1.        ]
 [1.         0.         0.         0.         0.39300251 1.
  0.         0.48927995 1.        ]
 [1.         0.         1.         1.         0.39288006 1.
  0.         1.         1.        ]
 [1.         0.         0.95090866 0.67589527 0.39304257 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39273675 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39298399 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39296572 1.
  0.         1.         1.        ]
 [1.         0.         0.40961586 0.25882047 0.3928232  1.
  0.         0.65814066 1.        ]
 [1.         0.         0.34896066 0.         0.39272445 1.
  0.         0.71089151 1.        ]
 [0.         0.         0.         0.         0.39286242 1.
  0.         0.         1.        ]
 [1.         0.         0.20692399 0.         0.392969   1.
  0.         0.91111384 1.        ]
 [1.         0.         0.81049109 0.76097837 0.39300931 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39286484 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39294757 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39310449 1.
  0.         0.94525032 1.        ]
 [1.         0.         1.         0.90536136 0.39293598 1.
  0.         1.         1.        ]
 [1.         0.         0.12696719 0.08342241 0.39315873 1.
  0.         0.57599464 1.        ]
 [1.         0.         0.         0.         0.39282879 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39308079 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39293299 1.
  0.         1.         1.        ]
 [1.         0.         1.         0.94423217 0.3929088  1.
  0.         1.         1.        ]
 [1.         0.         0.57586855 0.47273379 0.39304978 1.
  0.         0.42901705 1.        ]
 [1.         0.         0.35941285 0.09158407 0.39295203 1.
  0.         1.         1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 26 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 0. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         1.         0.05271026 1.         0.86987558 0.
 0.         0.57117373 0.5851716  0.         0.         0.
 0.         0.44440429 0.17307235 0.         0.         0.
 0.23123251 0.54088286 0.         0.54622091 0.86839601 0.5688373
 0.         0.3678262 ]
wv_ed shape (26,)
[0.         1.         0.         1.         0.73396174 0.
 0.         0.46273139 0.58076807 0.         0.         0.
 0.         0.66553725 0.14674137 0.         0.         0.
 0.00844225 0.34795459 0.         0.33000724 0.66982183 0.33951769
 0.         0.47512602]
wv_lg shape (26, 1)
[[0.39412658]
 [0.39339021]
 [0.39326934]
 [0.39312016]
 [0.39312523]
 [0.3931724 ]
 [0.39345497]
 [0.3931181 ]
 [0.39327639]
 [0.39310937]
 [0.39311797]
 [0.39315628]
 [0.39299811]
 [0.39302223]
 [0.39311127]
 [0.3931365 ]
 [0.3933441 ]
 [0.39320864]
 [0.39291907]
 [0.39320253]
 [0.3931044 ]
 [0.39298094]
 [0.39317058]
 [0.39313601]
 [0.39298388]
 [0.39327893]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.         1.         0.93129963 0.
 0.         0.9362299  0.29185674 0.         0.         0.
 0.         0.         0.26358338 0.         0.         0.
 0.62727549 0.77811249 0.29454673 0.7124773  1.         0.6252279
 0.         0.25408699]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39412658 1.
  1.         0.         0.        ]
 [1.         0.         1.         1.         0.39339021 1.
  0.         1.         1.        ]
 [1.         0.         0.05271026 0.         0.39326934 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39312016 1.
  0.         1.         1.        ]
 [1.         0.         0.86987558 0.73396174 0.39312523 1.
  0.         0.93129963 1.        ]
 [1.         0.         0.         0.         0.3931724  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39345497 1.
  0.         0.         1.        ]
 [1.         0.         0.57117373 0.46273139 0.3931181  1.
  0.         0.9362299  1.        ]
 [1.         0.         0.5851716  0.58076807 0.39327639 1.
  0.         0.29185674 1.        ]
 [1.         0.         0.         0.         0.39310937 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39311797 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39315628 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39299811 1.
  0.         0.         1.        ]
 [1.         0.         0.44440429 0.66553725 0.39302223 1.
  0.         0.         1.        ]
 [1.         0.         0.17307235 0.14674137 0.39311127 1.
  0.         0.26358338 1.        ]
 [1.         0.         0.         0.         0.3931365  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.3933441  1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39320864 1.
  0.         0.         1.        ]
 [1.         0.         0.23123251 0.00844225 0.39291907 1.
  0.         0.62727549 1.        ]
 [1.         0.         0.54088286 0.34795459 0.39320253 1.
  0.         0.77811249 1.        ]
 [1.         0.         0.         0.         0.3931044  1.
  0.         0.29454673 1.        ]
 [1.         0.         0.54622091 0.33000724 0.39298094 1.
  0.         0.7124773  1.        ]
 [1.         0.         0.86839601 0.66982183 0.39317058 1.
  0.         1.         1.        ]
 [1.         0.         0.5688373  0.33951769 0.39313601 1.
  0.         0.6252279  1.        ]
 [0.         0.         0.         0.         0.39298388 1.
  0.         0.         1.        ]
 [1.         0.         0.3678262  0.47512602 0.39327893 1.
  0.         0.25408699 1.        ]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 27 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.         0.46566037 1.         0.8133146  0.         0.
 0.14395117 1.         1.         1.         0.         0.
 0.         0.52746124 0.26327338 0.6310468  1.         0.
 1.         0.         1.         0.69949452 0.0106341  0.
 0.         0.61382681]
wv_ed shape (26,)
[0.         0.57085879 1.         0.95426914 0.         0.
 0.43870809 1.         1.         1.         0.         0.
 0.         0.75127271 0.63160151 0.90095567 1.         0.
 1.         0.         1.         1.         0.         0.
 0.1084234  0.77754643]
wv_lg shape (26, 1)
[[0.39463738]
 [0.39280679]
 [0.39296934]
 [0.39279333]
 [0.39304102]
 [0.3927686 ]
 [0.3928481 ]
 [0.39317743]
 [0.39272803]
 [0.39290967]
 [0.39290821]
 [0.39302369]
 [0.39293727]
 [0.39268283]
 [0.39282684]
 [0.39296826]
 [0.39305297]
 [0.392918  ]
 [0.39298705]
 [0.39306633]
 [0.3930478 ]
 [0.39290678]
 [0.39302973]
 [0.39290691]
 [0.39281072]
 [0.39292891]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         0.87656836 1.         0.97010332 0.         0.
 0.12611587 1.         1.         0.83127953 0.         0.
 0.         0.26486467 0.08890901 0.30303733 1.         0.
 1.         0.         1.         0.52653106 0.23025558 0.
 0.25179395 0.0025035 ]
xy shape: (26, 9)
[[0.         1.         0.         0.         0.39463738 1.
  1.         0.         0.        ]
 [1.         0.         0.46566037 0.57085879 0.39280679 1.
  0.         0.87656836 1.        ]
 [1.         0.         1.         1.         0.39296934 1.
  0.         1.         1.        ]
 [1.         0.         0.8133146  0.95426914 0.39279333 1.
  0.         0.97010332 1.        ]
 [1.         0.         0.         0.         0.39304102 1.
  0.         0.         1.        ]
 [0.         0.         0.         0.         0.3927686  1.
  0.         0.         1.        ]
 [1.         0.         0.14395117 0.43870809 0.3928481  1.
  0.         0.12611587 1.        ]
 [1.         0.         1.         1.         0.39317743 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39272803 1.
  0.         1.         1.        ]
 [1.         0.         1.         1.         0.39290967 1.
  0.         0.83127953 1.        ]
 [1.         0.         0.         0.         0.39290821 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39302369 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.         0.39293727 1.
  0.         0.         1.        ]
 [1.         0.         0.52746124 0.75127271 0.39268283 1.
  0.         0.26486467 1.        ]
 [1.         0.         0.26327338 0.63160151 0.39282684 1.
  0.         0.08890901 1.        ]
 [1.         0.         0.6310468  0.90095567 0.39296826 1.
  0.         0.30303733 1.        ]
 [1.         0.         1.         1.         0.39305297 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.392918   1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.39298705 1.
  0.         1.         1.        ]
 [1.         0.         0.         0.         0.39306633 1.
  0.         0.         1.        ]
 [1.         0.         1.         1.         0.3930478  1.
  0.         1.         1.        ]
 [1.         0.         0.69949452 1.         0.39290678 1.
  0.         0.52653106 1.        ]
 [1.         0.         0.0106341  0.         0.39302973 1.
  0.         0.23025558 1.        ]
 [1.         0.         0.         0.         0.39290691 1.
  0.         0.         1.        ]
 [1.         0.         0.         0.1084234  0.39281072 1.
  0.         0.25179395 1.        ]
 [1.         0.         0.61382681 0.77754643 0.39292891 1.
  0.         0.0025035  1.        ]]

Best Training Poisoning Accuracy:
0.7857142686843872
#####################         POISON         ###############################################

############################################################################################

comm_round: 28 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients
y shape (26,)
[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
wv_asf shape (26,)
[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_fg shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_mn shape (26,)
[0.00000000e+00 1.00000000e+00 1.83148406e-02 7.45472418e-01
 6.87535015e-02 1.00000000e+00 1.00000000e+00 1.00000000e+00
 1.00000000e+00 1.00000000e+00 0.00000000e+00 4.18555962e-04
 0.00000000e+00 1.00000000e+00 7.47334173e-01 2.85142388e-01
 1.00000000e+00 4.91510695e-01 0.00000000e+00 1.00000000e+00
 7.35650939e-01 1.58347325e-01 9.56731034e-01 1.00000000e+00
 0.00000000e+00 4.18527632e-01]
wv_ed shape (26,)
[0.         0.97659333 0.         0.78373875 0.         1.
 0.73016741 0.72777399 1.         0.71034929 0.         0.
 0.         1.         0.57820219 0.         1.         0.27812184
 0.         1.         0.51056069 0.09402204 0.71999411 0.727921
 0.         0.1688681 ]
wv_lg shape (26, 1)
[[0.39465655]
 [0.39325148]
 [0.39337114]
 [0.39312412]
 [0.39308159]
 [0.3930865 ]
 [0.39312362]
 [0.39315638]
 [0.39305671]
 [0.39315885]
 [0.39316565]
 [0.39315878]
 [0.39316112]
 [0.3931091 ]
 [0.39317975]
 [0.39335517]
 [0.39334425]
 [0.39327517]
 [0.39333551]
 [0.39304469]
 [0.39308068]
 [0.39334064]
 [0.39321018]
 [0.39328265]
 [0.39321119]
 [0.3931632 ]]
wv_jc shape (26,)
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1.]
wv_ndT shape (26,)
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
wv_std shape (26,)
[0.         1.         0.42739858 0.70604719 0.78442125 1.
 1.         1.         1.         1.         0.67364957 0.26690119
 0.         1.         1.         1.         1.         1.
 0.         0.80005461 1.         0.         1.         1.
 0.         0.65113811]
xy shape: (26, 9)
[[0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
  3.94656549e-01 1.00000000e+00 1.00000000e+00 0.00000000e+00
  0.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 9.76593330e-01
  3.93251483e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.83148406e-02 0.00000000e+00
  3.93371141e-01 1.00000000e+00 0.00000000e+00 4.27398579e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.45472418e-01 7.83738748e-01
  3.93124124e-01 1.00000000e+00 0.00000000e+00 7.06047188e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 6.87535015e-02 0.00000000e+00
  3.93081595e-01 1.00000000e+00 0.00000000e+00 7.84421248e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.93086501e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 7.30167408e-01
  3.93123616e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 7.27773995e-01
  3.93156384e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.93056713e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 7.10349285e-01
  3.93158848e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.93165649e-01 1.00000000e+00 0.00000000e+00 6.73649571e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 4.18555962e-04 0.00000000e+00
  3.93158780e-01 1.00000000e+00 0.00000000e+00 2.66901192e-01
  1.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.93161117e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.93109101e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.47334173e-01 5.78202191e-01
  3.93179747e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 2.85142388e-01 0.00000000e+00
  3.93355166e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.93344250e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 4.91510695e-01 2.78121836e-01
  3.93275172e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.93335505e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00
  3.93044693e-01 1.00000000e+00 0.00000000e+00 8.00054612e-01
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 7.35650939e-01 5.10560691e-01
  3.93080676e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.58347325e-01 9.40220361e-02
  3.93340643e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 9.56731034e-01 7.19994107e-01
  3.93210179e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 1.00000000e+00 7.27921001e-01
  3.93282655e-01 1.00000000e+00 0.00000000e+00 1.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.93211188e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00
  1.00000000e+00]
 [1.00000000e+00 0.00000000e+00 4.18527632e-01 1.68868104e-01
  3.93163200e-01 1.00000000e+00 0.00000000e+00 6.51138111e-01
  1.00000000e+00]]

Best Training Poisoning Accuracy:
0.9285714030265808
#####################         POISON         ###############################################

############################################################################################

comm_round: 29 | global_test_acc: 100.000% | global_f1: 1.0 | global_precision: 1.0
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         8

    accuracy                           1.00         8
   macro avg       1.00      1.00      1.00         8
weighted avg       1.00      1.00      1.00         8
poison scaling shape: (26, 1)
[[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]scaled_weight_list: Rows 26 cols 21Adding node: 0 value: [1] to honest_clientsAdding node: 1 value: [1] to honest_clientsAdding node: 2 value: [1] to honest_clientsAdding node: 3 value: [1] to honest_clientsAdding node: 4 value: [1] to honest_clientsAdding node: 5 value: [1] to honest_clientsAdding node: 6 value: [1] to honest_clientsAdding node: 7 value: [1] to honest_clientsAdding node: 8 value: [1] to honest_clientsAdding node: 9 value: [1] to honest_clientsAdding node: 10 value: [1] to honest_clientsAdding node: 11 value: [1] to honest_clientsAdding node: 12 value: [1] to honest_clientsAdding node: 13 value: [1] to honest_clientsAdding node: 14 value: [1] to honest_clientsAdding node: 15 value: [1] to honest_clientsAdding node: 16 value: [1] to honest_clientsAdding node: 17 value: [1] to honest_clientsAdding node: 18 value: [1] to honest_clientsAdding node: 19 value: [1] to honest_clientsAdding node: 20 value: [1] to honest_clientsAdding node: 21 value: [1] to honest_clientsAdding node: 22 value: [1] to honest_clientsAdding node: 23 value: [1] to honest_clientsAdding node: 24 value: [1] to honest_clientsAdding node: 25 value: [1] to honest_clients

Best Training Poisoning Accuracy:
0.8295723795890808

Best Training Poisoning Accuracy:
0.8295723795890808

Best Training Poisoning Accuracy:
0.8295723795890808

Best Training Poisoning Accuracy:
0.8480353355407715

Best Training Poisoning Accuracy:
0.8302035927772522

Best Training Poisoning Accuracy:
0.8308348059654236

Best Training Poisoning Accuracy:
0.8770711421966553

Best Training Poisoning Accuracy:
0.8813318610191345

Best Training Poisoning Accuracy:
0.8980590105056763

Best Training Poisoning Accuracy:
0.9130503535270691

Best Training Poisoning Accuracy:
0.9084740281105042

Best Training Poisoning Accuracy:
0.9008994698524475

Best Training Poisoning Accuracy:
0.8655515313148499

Best Training Poisoning Accuracy:
0.859239399433136

Best Training Poisoning Accuracy:
0.8295723795890808

Best Training Poisoning Accuracy:
0.903897762298584

Best Training Poisoning Accuracy:
0.9422439932823181